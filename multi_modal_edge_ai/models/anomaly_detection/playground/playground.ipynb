{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multi_modal_edge_ai.models.anomaly_detection.data_access.parser import parse_file_with_idle\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.local_outlier_factor import LOF\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.isolation_forest import IForest\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.one_class_svm import OCSVM\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.autoencoder import Autoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.lstm_autoencoder import LSTMAutoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.model_validator import model_train_eval\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.hyperparameter_config import HyperparameterConfig as Hparams\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "data = parse_file_with_idle(\"../../public_datasets/Aruba_Idle_Squashed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5905411163187048\n",
      "[[1376   82]\n",
      " [ 879   10]]\n",
      "Accuracy:  0.5905411163187048\n",
      "Precision:  0.943758573388203\n",
      "Recall:  0.61019955654102\n",
      "F1:  0.7411796391058443\n"
     ]
    }
   ],
   "source": [
    " # Basic LOF model with changed hyperparameters\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 300,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 40,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.1,\n",
    "             window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "0.5069867431028305\n",
      "[[ 111 1347]\n",
      " [  29 1304]]\n",
      "Accuracy:  0.5069867431028305\n",
      "Precision:  0.07613168724279835\n",
      "Recall:  0.7928571428571428\n",
      "F1:  0.13892365456821026\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 16\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 4000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 400,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": 0.001,\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5206019347903977\n",
      "[[ 166 1292]\n",
      " [  46 1287]]\n",
      "Accuracy:  0.5206019347903977\n",
      "Precision:  0.11385459533607682\n",
      "Recall:  0.7830188679245284\n",
      "F1:  0.19880239520958085\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 64\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 2000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 300,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iforest = IForest()\n",
    "\n",
    "iforestParams = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_samples\": \"auto\",\n",
    "    \"contamination\": 0.1,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "hp = Hparams(i_forest_hparams=iforestParams, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(iforest, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[1 0 1 ... 1 1 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "0.7200546946216956\n",
      "[[6917 1831]\n",
      " [1240  982]]\n",
      "Accuracy: 0.7200546946216956\n",
      "Precision:  0.790695016003658\n",
      "Recall:  0.8479833272036288\n",
      "F1:  0.818337769890565\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.01,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2400,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.25,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05389717221260071\n",
      "Epoch 2 training loss: 0.030980540439486504\n",
      "Epoch 3 training loss: 0.023487167432904243\n",
      "Epoch 4 training loss: 0.020996732637286186\n",
      "Epoch 5 training loss: 0.019222674891352654\n",
      "Epoch 6 training loss: 0.017535774037241936\n",
      "Epoch 7 training loss: 0.016274292021989822\n",
      "Epoch 8 training loss: 0.01566317304968834\n",
      "Epoch 9 training loss: 0.015262589789927006\n",
      "Epoch 10 training loss: 0.014593065716326237\n",
      "0.2847235417062512\n",
      "[[1318 7430]\n",
      " [  99 1679]]\n",
      "Accuracy: 0.2847235417062512\n",
      "Precision:  0.1843231968382918\n",
      "Recall:  0.9443194600674916\n",
      "F1:  0.30844126021860935\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([96,64,32,16], [16,32,64,96],nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=16,  n_epochs=10, anomaly_generation_ratio = 11, clean_test_data_ratio=0.2, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0.4505562422744129\n",
      "[[1458    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.4505562422744129\n",
      "Precision:  1.0\n",
      "Recall:  0.4505562422744129\n",
      "F1:  0.6212185769066894\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=32, n_epochs=16, clean_test_data_ratio=0.2,window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07760713093037244\n",
      "Epoch 2 training loss: 0.07725755977669586\n",
      "Epoch 3 training loss: 0.0772386808702628\n",
      "Epoch 4 training loss: 0.07725333552849474\n",
      "Epoch 5 training loss: 0.07720437902104449\n",
      "Epoch 6 training loss: 0.07721456329896627\n",
      "Epoch 7 training loss: 0.0772136117869106\n",
      "Epoch 8 training loss: 0.07721375107463486\n",
      "Epoch 9 training loss: 0.07718426470631153\n",
      "Epoch 10 training loss: 0.07720113484895001\n",
      "Epoch 11 training loss: 0.07719907346490616\n",
      "Epoch 12 training loss: 0.07718318898107174\n",
      "Epoch 13 training loss: 0.07717363352660002\n",
      "Epoch 14 training loss: 0.07718654360969973\n",
      "Epoch 15 training loss: 0.07717847603837319\n",
      "Epoch 16 training loss: 0.07719744948766648\n",
      "Epoch 17 training loss: 0.07721296846458041\n",
      "Epoch 18 training loss: 0.07720602415680164\n",
      "Epoch 19 training loss: 0.07716414992486968\n",
      "Epoch 20 training loss: 0.07716112782344792\n",
      "Epoch 21 training loss: 0.07718445156324819\n",
      "Epoch 22 training loss: 0.07716832268737986\n",
      "Epoch 23 training loss: 0.07718266583130569\n",
      "Epoch 24 training loss: 0.0771673936056365\n",
      "Epoch 25 training loss: 0.07717815300615251\n",
      "Epoch 26 training loss: 0.07719496329638717\n",
      "Epoch 27 training loss: 0.0771747040143289\n",
      "Epoch 28 training loss: 0.07716852902509147\n",
      "Epoch 29 training loss: 0.07714804287327305\n",
      "Epoch 30 training loss: 0.07716223987655377\n",
      "Epoch 31 training loss: 0.07717982528331041\n",
      "Epoch 32 training loss: 0.07717942436989418\n",
      "0.6146477132262052\n",
      "[[ 340 1118]\n",
      " [ 129 1649]]\n",
      "Accuracy: 0.6146477132262052\n",
      "Precision:  0.5959522949042284\n",
      "Recall:  0.9274465691788526\n",
      "F1:  0.7256325632563256\n"
     ]
    }
   ],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=16, n_epochs=32, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.7262534184138559\n",
      "[[6972 1776]\n",
      " [1227  995]]\n",
      "Accuracy:  0.7262534184138559\n",
      "Precision:  0.3590761457957416\n",
      "Recall:  0.44779477947794777\n",
      "F1:  0.3985579811736431\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  1e-05\n",
      "0.697538742023701\n",
      "[[6620 2128]\n",
      " [1190 1032]]\n",
      "Accuracy:  0.697538742023701\n",
      "Precision:  0.3265822784810127\n",
      "Recall:  0.46444644464446444\n",
      "F1:  0.3835005574136009\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.7302643573381951\n",
      "[[6966 1782]\n",
      " [1177 1045]]\n",
      "Accuracy:  0.7302643573381951\n",
      "Precision:  0.36964980544747084\n",
      "Recall:  0.47029702970297027\n",
      "F1:  0.4139433551198257\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  1e-05\n",
      "0.703281677301732\n",
      "[[6672 2076]\n",
      " [1179 1043]]\n",
      "Accuracy:  0.703281677301732\n",
      "Precision:  0.3344020519397243\n",
      "Recall:  0.4693969396939694\n",
      "F1:  0.3905635648754915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.7003646308113035\n",
      "[[6583 2165]\n",
      " [1122 1100]]\n",
      "Accuracy:  0.7003646308113035\n",
      "Precision:  0.33690658499234305\n",
      "Recall:  0.49504950495049505\n",
      "F1:  0.40094769455075635\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  1e-05\n",
      "0.7062898814949863\n",
      "[[6674 2074]\n",
      " [1148 1074]]\n",
      "Accuracy:  0.7062898814949863\n",
      "Precision:  0.3411689961880559\n",
      "Recall:  0.48334833483348333\n",
      "F1:  0.4\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.692616226071103\n",
      "[[6469 2279]\n",
      " [1093 1129]]\n",
      "Accuracy:  0.692616226071103\n",
      "Precision:  0.3312793427230047\n",
      "Recall:  0.508100810081008\n",
      "F1:  0.4010657193605684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  1e-05\n",
      "0.7061987237921604\n",
      "[[6658 2090]\n",
      " [1133 1089]]\n",
      "Accuracy:  0.7061987237921604\n",
      "Precision:  0.34256055363321797\n",
      "Recall:  0.4900990099009901\n",
      "F1:  0.40325865580448067\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.46298997265268915\n",
      "[[3161 5587]\n",
      " [ 304 1918]]\n",
      "Accuracy:  0.46298997265268915\n",
      "Precision:  0.25556295802798135\n",
      "Recall:  0.8631863186318632\n",
      "F1:  0.39436619718309857\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  1e-05\n",
      "0.47265268915223335\n",
      "[[3271 5477]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47265268915223335\n",
      "Precision:  0.2589636043837099\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39821075626755437\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.4610756608933455\n",
      "[[3173 5575]\n",
      " [ 337 1885]]\n",
      "Accuracy:  0.4610756608933455\n",
      "Precision:  0.2526809651474531\n",
      "Recall:  0.8483348334833484\n",
      "F1:  0.38938235901673207\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  1e-05\n",
      "0.4740200546946217\n",
      "[[3323 5425]\n",
      " [ 345 1877]]\n",
      "Accuracy:  0.4740200546946217\n",
      "Precision:  0.25705286222952617\n",
      "Recall:  0.8447344734473448\n",
      "F1:  0.3941621167576648\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.46362807657247035\n",
      "[[3200 5548]\n",
      " [ 336 1886]]\n",
      "Accuracy:  0.46362807657247035\n",
      "Precision:  0.2536992198009147\n",
      "Recall:  0.8487848784878488\n",
      "F1:  0.39063794531897267\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  1e-05\n",
      "0.4690975387420237\n",
      "[[3250 5498]\n",
      " [ 326 1896]]\n",
      "Accuracy:  0.4690975387420237\n",
      "Precision:  0.2564241276710847\n",
      "Recall:  0.8532853285328533\n",
      "F1:  0.39434276206322794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.47201458523245216\n",
      "[[3264 5484]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47201458523245216\n",
      "Precision:  0.2587185725871857\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39792099792099794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  1e-05\n",
      "0.4545123062898815\n",
      "[[3079 5669]\n",
      " [ 315 1907]]\n",
      "Accuracy:  0.4545123062898815\n",
      "Precision:  0.25171594508975714\n",
      "Recall:  0.8582358235823583\n",
      "F1:  0.3892631149214125\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for OCSVM\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "kernels = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.0001, 0.00001]\n",
    "\n",
    "for k in kernels:\n",
    "    for d in degrees:\n",
    "        for g in gammas:\n",
    "            for c in coef0s:\n",
    "                for t in tols:\n",
    "                    ovscmParams = {\n",
    "                        \"kernel\": k,\n",
    "                        \"degree\": d if k == \"poly\" else 3,\n",
    "                        \"gamma\": g,\n",
    "                        \"coef0\": c if k == \"poly\" or k == \"sigmoid\" else 0.0,\n",
    "                        \"tol\": t,\n",
    "                        \"nu\": 0.001,\n",
    "                        \"shrinking\": True,\n",
    "                        \"cache_size\": 3200,\n",
    "                        \"verbose\": False,\n",
    "                        \"max_iter\": -1,\n",
    "                    }\n",
    "\n",
    "                    hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=11,\n",
    "                                 clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                                 one_hot=False)\n",
    "                    (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                    print(\"for kernel: \", k, \" degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                    print(avg)\n",
    "                    print(cm)\n",
    "                    (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                    print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                    print(\"Precision: \", tp / (tp + fp))\n",
    "                    print(\"Recall: \", tp / (tp + fn))\n",
    "                    print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                    print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6910205869469995\n",
      "[[6581 2167]\n",
      " [1360 1307]]\n",
      "Accuracy:  0.6910205869469995\n",
      "Precision:  0.37622337363270003\n",
      "Recall:  0.49006374203224595\n",
      "F1:  0.425663572708028\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.0001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  26 1432]\n",
      " [   7 2215]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6073485056210584\n",
      "Recall:  0.9968496849684968\n",
      "F1:  0.7548134264781053\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  23 1435]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074945295404814\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7556992174208914\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  25 1433]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078270388615217\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7559564329475834\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6168478260869565\n",
      "[[  54 1404]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6168478260869565\n",
      "Precision:  0.6121546961325967\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7586442998972954\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6103260869565217\n",
      "[[  24 1434]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6077680525164114\n",
      "Recall:  1.0\n",
      "F1:  0.7560394692072133\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  17 1441]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064991807755324\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7549286199864038\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6078804347826087\n",
      "[[  16 1442]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6078804347826087\n",
      "Precision:  0.6063336063336063\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7548003398470688\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  19 1439]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069379950833106\n",
      "Recall:  1.0\n",
      "F1:  0.7553969063403025\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6108695652173913\n",
      "[[  26 1432]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6081007115489874\n",
      "Recall:  1.0\n",
      "F1:  0.7562968005445881\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.647554347826087\n",
      "[[ 310 1148]\n",
      " [ 149 2073]]\n",
      "Accuracy:  0.647554347826087\n",
      "Precision:  0.6435889475318224\n",
      "Recall:  0.9329432943294329\n",
      "F1:  0.7617122910159838\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.61875\n",
      "[[  75 1383]\n",
      " [  20 2202]]\n",
      "Accuracy:  0.61875\n",
      "Precision:  0.6142259414225941\n",
      "Recall:  0.990999099909991\n",
      "F1:  0.7583950404684002\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6108695652173913\n",
      "[[  32 1426]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6084568918176826\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7557980900409277\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6105978260869566\n",
      "[[  25 1433]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6105978260869566\n",
      "Precision:  0.6079343365253078\n",
      "Recall:  1.0\n",
      "F1:  0.7561681129828144\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6111413043478261\n",
      "[[  31 1427]\n",
      " [   4 2218]]\n",
      "Accuracy:  0.6111413043478261\n",
      "Precision:  0.6085048010973937\n",
      "Recall:  0.9981998199819982\n",
      "F1:  0.7560934037838759\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  26 1432]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078860898138007\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7558733401430031\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6144021739130435\n",
      "[[  41 1417]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6144021739130435\n",
      "Precision:  0.6103931811932912\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7578084997439836\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6059782608695652\n",
      "[[   8 1450]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6059782608695652\n",
      "Precision:  0.605119825708061\n",
      "Recall:  1.0\n",
      "F1:  0.7539871055310485\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6293478260869565\n",
      "[[ 732  726]\n",
      " [ 638 1584]]\n",
      "Accuracy:  0.6293478260869565\n",
      "Precision:  0.6857142857142857\n",
      "Recall:  0.7128712871287128\n",
      "F1:  0.6990291262135923\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6157608695652174\n",
      "[[  49 1409]\n",
      " [   5 2217]]\n",
      "Accuracy:  0.6157608695652174\n",
      "Precision:  0.6114175399889685\n",
      "Recall:  0.9977497749774977\n",
      "F1:  0.7582079343365253\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6141304347826086\n",
      "[[  40 1418]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6141304347826086\n",
      "Precision:  0.6102253985706432\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.757679180887372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  20 1438]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069964471166985\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7553137221560959\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 10% of the data\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6518787878787878\n",
      "[[  23 1435]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6518787878787878\n",
      "Precision:  0.6500853450377957\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7878250591016549\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"poly\",\n",
    "    \"degree\": 5,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.3,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  200\n",
      "0.6681212121212121\n",
      "[[ 217 1241]\n",
      " [ 128 2539]]\n",
      "Accuracy:  0.6681212121212121\n",
      "Precision:  0.6716931216931217\n",
      "Recall:  0.9520059992500938\n",
      "F1:  0.7876531720179929\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  250\n",
      "0.6678787878787878\n",
      "[[ 213 1245]\n",
      " [ 125 2542]]\n",
      "Accuracy:  0.6678787878787878\n",
      "Precision:  0.6712437285450225\n",
      "Recall:  0.9531308586426697\n",
      "F1:  0.7877285404400372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  300\n",
      "0.6693333333333333\n",
      "[[ 215 1243]\n",
      " [ 121 2546]]\n",
      "Accuracy:  0.6693333333333333\n",
      "Precision:  0.6719451042491422\n",
      "Recall:  0.9546306711661042\n",
      "F1:  0.788723667905824\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  350\n",
      "0.6676363636363636\n",
      "[[ 210 1248]\n",
      " [ 123 2544]]\n",
      "Accuracy:  0.6676363636363636\n",
      "Precision:  0.6708860759493671\n",
      "Recall:  0.953880764904387\n",
      "F1:  0.7877380399442638\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  200\n",
      "0.6557575757575758\n",
      "[[  78 1380]\n",
      " [  40 2627]]\n",
      "Accuracy:  0.6557575757575758\n",
      "Precision:  0.6556026952832543\n",
      "Recall:  0.9850018747656543\n",
      "F1:  0.7872340425531915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  250\n",
      "0.6564848484848484\n",
      "[[  85 1373]\n",
      " [  44 2623]]\n",
      "Accuracy:  0.6564848484848484\n",
      "Precision:  0.6564064064064065\n",
      "Recall:  0.9835020622422197\n",
      "F1:  0.7873330331682425\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  300\n",
      "0.6591515151515152\n",
      "[[  83 1375]\n",
      " [  31 2636]]\n",
      "Accuracy:  0.6591515151515152\n",
      "Precision:  0.6571927200199451\n",
      "Recall:  0.9883764529433821\n",
      "F1:  0.7894579215333932\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  350\n",
      "0.6574545454545454\n",
      "[[  88 1370]\n",
      " [  43 2624]]\n",
      "Accuracy:  0.6574545454545454\n",
      "Precision:  0.656985478217326\n",
      "Recall:  0.9838770153730784\n",
      "F1:  0.7878696892358504\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  200\n",
      "0.6484848484848484\n",
      "[[  20 1438]\n",
      " [  12 2655]]\n",
      "Accuracy:  0.6484848484848484\n",
      "Precision:  0.6486684583435133\n",
      "Recall:  0.9955005624296963\n",
      "F1:  0.7855029585798816\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  250\n",
      "0.6477575757575758\n",
      "[[  20 1438]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6477575757575758\n",
      "Precision:  0.6484107579462103\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.784963741305313\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  300\n",
      "0.6494545454545455\n",
      "[[  25 1433]\n",
      " [  13 2654]]\n",
      "Accuracy:  0.6494545454545455\n",
      "Precision:  0.6493760704673355\n",
      "Recall:  0.9951256092988376\n",
      "F1:  0.7859046490968316\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  350\n",
      "0.6487272727272727\n",
      "[[  24 1434]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6487272727272727\n",
      "Precision:  0.6490455212922174\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.7854286983562861\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  200\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  250\n",
      "0.6472727272727272\n",
      "[[   4 1454]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6472727272727272\n",
      "Precision:  0.6470873786407767\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7856195668189185\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  300\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  350\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for LOF\n",
    "\n",
    "lof = LOF()\n",
    "\n",
    "n_neighbors = [1500,2000,2500,3000]\n",
    "leaf_sizes = [200,250,300,350]\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for l in leaf_sizes:\n",
    "        lofParams = {\n",
    "            \"n_neighbors\": n,\n",
    "            \"algorithm\": \"auto\",\n",
    "            \"leaf_size\": l,\n",
    "            \"metric\": \"minkowski\",\n",
    "            \"p\": 2,\n",
    "            \"metric_params\": None,\n",
    "            \"contamination\": \"auto\",\n",
    "            \"novelty\": True,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "\n",
    "        hp = Hparams(lof_hparams=lofParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "        (avg, cm) = model_train_eval(lof, data, hp)\n",
    "        print(\"for n_neighbors: \", n, \" leaf_size: \", l)\n",
    "        print(avg)\n",
    "        print(cm)\n",
    "        (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "        print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "        print(\"Precision: \", tp / (tp + fp))\n",
    "        print(\"Recall: \", tp / (tp + fn))\n",
    "        print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "        print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.22790743091494972\n",
      "[[ 112 7907]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21937012538256492\n",
      "Recall:  1.0\n",
      "F1:  0.3598089223544652\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.22820037105751392\n",
      "[[ 116 7903]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22820037105751392\n",
      "Precision:  0.21937969182141445\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597926453912198\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.22800507762913777\n",
      "[[ 113 7906]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22800507762913777\n",
      "Precision:  0.21939178515007898\n",
      "Recall:  1.0\n",
      "F1:  0.35983805668016194\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.22868860462845425\n",
      "[[ 120 7899]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22868860462845425\n",
      "Precision:  0.2195435233672562\n",
      "Recall:  1.0\n",
      "F1:  0.3600421291420238\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.22595449663118836\n",
      "[[  92 7927]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22595449663118836\n",
      "Precision:  0.21893782638683615\n",
      "Recall:  1.0\n",
      "F1:  0.35922722496160375\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.22663802363050484\n",
      "[[  99 7920]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22663802363050484\n",
      "Precision:  0.21908893709327548\n",
      "Recall:  1.0\n",
      "F1:  0.3594306049822064\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.0001\n",
      "0.22790743091494972\n",
      "[[ 113 7906]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21931470326849017\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597052392906308\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.22986036519871106\n",
      "[[ 132 7887]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22986036519871106\n",
      "Precision:  0.21980413492927095\n",
      "Recall:  1.0\n",
      "F1:  0.360392506690455\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.22810272434332585\n",
      "[[ 114 7905]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22810272434332585\n",
      "Precision:  0.21941344919522068\n",
      "Recall:  1.0\n",
      "F1:  0.35986719572435016\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.22546626306024803\n",
      "[[  88 7931]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22546626306024803\n",
      "Precision:  0.2187746256895193\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3589785033133991\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.22585684991700028\n",
      "[[  91 7928]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22585684991700028\n",
      "Precision:  0.21891625615763546\n",
      "Recall:  1.0\n",
      "F1:  0.35919818946007115\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.22634508348794063\n",
      "[[  96 7923]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22634508348794063\n",
      "Precision:  0.21902414982750124\n",
      "Recall:  1.0\n",
      "F1:  0.359343413924153\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.22605214334537643\n",
      "[[  93 7926]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22605214334537643\n",
      "Precision:  0.21895940086716595\n",
      "Recall:  1.0\n",
      "F1:  0.3592562651576395\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.2261497900595645\n",
      "[[  95 7924]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.2261497900595645\n",
      "Precision:  0.21892557910300642\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3591816932158163\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.22761449077238552\n",
      "[[ 110 7909]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22761449077238552\n",
      "Precision:  0.2192497532082922\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3596178756476684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 30\u001B[0m\n\u001B[1;32m     14\u001B[0m ovscmParams \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkernel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdegree\u001B[39m\u001B[38;5;124m\"\u001B[39m: d,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     25\u001B[0m }\n\u001B[1;32m     27\u001B[0m hp \u001B[38;5;241m=\u001B[39m Hparams(ocsvm_hparams\u001B[38;5;241m=\u001B[39movscmParams, anomaly_generation_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     28\u001B[0m              clean_test_data_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m, window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, window_slide\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     29\u001B[0m              one_hot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m (avg, cm) \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_train_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mocsvm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor degree: \u001B[39m\u001B[38;5;124m\"\u001B[39m, d, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m gamma: \u001B[39m\u001B[38;5;124m\"\u001B[39m, g, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m coef0: \u001B[39m\u001B[38;5;124m\"\u001B[39m, c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m tol: \u001B[39m\u001B[38;5;124m\"\u001B[39m, t)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(avg)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/model_validator.py:36\u001B[0m, in \u001B[0;36mmodel_train_eval\u001B[0;34m(model, data, hparams)\u001B[0m\n\u001B[1;32m     33\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m clean_df[:split_index]\n\u001B[1;32m     34\u001B[0m training_df \u001B[38;5;241m=\u001B[39m clean_df[split_index:]\n\u001B[0;32m---> 36\u001B[0m generated_anomalies_df \u001B[38;5;241m=\u001B[39m \u001B[43msynthetic_anomaly_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manomaly_generation_ratio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m anomalous_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([anomalous_df, generated_anomalies_df])\n\u001B[1;32m     39\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m testing_df\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:28\u001B[0m, in \u001B[0;36msynthetic_anomaly_generator\u001B[0;34m(anomalous_windows, anomaly_generation_ratio)\u001B[0m\n\u001B[1;32m     25\u001B[0m anomalous_windows \u001B[38;5;241m=\u001B[39m anomalous_windows\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index_window, window \u001B[38;5;129;01min\u001B[39;00m anomalous_windows\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# Get the activity, reason and type of anomaly window\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     activity, reason, type_anomaly, window \u001B[38;5;241m=\u001B[39m \u001B[43mextract_anomaly_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     new_activity: List[pd\u001B[38;5;241m.\u001B[39mDataFrame] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     30\u001B[0m     new_duration \u001B[38;5;241m=\u001B[39m timedelta(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:63\u001B[0m, in \u001B[0;36mextract_anomaly_details\u001B[0;34m(anomalous_windows, index_window)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_anomaly_details\u001B[39m(anomalous_windows: pd\u001B[38;5;241m.\u001B[39mDataFrame, index_window: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \\\n\u001B[1;32m     55\u001B[0m         Tuple[pd\u001B[38;5;241m.\u001B[39mDataFrame, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m, pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m     56\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    This function will process the window and return the activity, reason and type of anomaly\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m    :param anomalous_windows: the windows to be processed\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    :param index_window: the index of the window to be processed\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    :return: the activity, reason, type of anomaly and the window\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m     window \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     reason \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     65\u001B[0m     type_anomaly \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/frame.py:790\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    781\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m    782\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m nested_data_to_arrays(\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;00m\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    788\u001B[0m         dtype,\n\u001B[1;32m    789\u001B[0m     )\n\u001B[0;32m--> 790\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    798\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m ndarray_to_mgr(\n\u001B[1;32m    799\u001B[0m         data,\n\u001B[1;32m    800\u001B[0m         index,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    804\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[1;32m    805\u001B[0m     )\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:153\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    150\u001B[0m axes \u001B[38;5;241m=\u001B[39m [columns, index]\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_block_manager_from_column_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefs\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2137\u001B[0m, in \u001B[0;36mcreate_block_manager_from_column_arrays\u001B[0;34m(arrays, axes, consolidate, refs)\u001B[0m\n\u001B[1;32m   2119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_block_manager_from_column_arrays\u001B[39m(\n\u001B[1;32m   2120\u001B[0m     arrays: \u001B[38;5;28mlist\u001B[39m[ArrayLike],\n\u001B[1;32m   2121\u001B[0m     axes: \u001B[38;5;28mlist\u001B[39m[Index],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2133\u001B[0m     \u001B[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001B[39;00m\n\u001B[1;32m   2134\u001B[0m     \u001B[38;5;66;03m#  verify_integrity=False below.\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2137\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m \u001B[43m_form_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2138\u001B[0m         mgr \u001B[38;5;241m=\u001B[39m BlockManager(blocks, axes, verify_integrity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2203\u001B[0m, in \u001B[0;36m_form_blocks\u001B[0;34m(arrays, consolidate, refs)\u001B[0m\n\u001B[1;32m   2196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nbs\n\u001B[1;32m   2198\u001B[0m \u001B[38;5;66;03m# when consolidating, we can ignore refs (either stacking always copies,\u001B[39;00m\n\u001B[1;32m   2199\u001B[0m \u001B[38;5;66;03m# or the EA is already copied in the calling dict_to_mgr)\u001B[39;00m\n\u001B[1;32m   2200\u001B[0m \u001B[38;5;66;03m# TODO(CoW) check if this is also valid for rec_array_to_mgr\u001B[39;00m\n\u001B[1;32m   2201\u001B[0m \n\u001B[1;32m   2202\u001B[0m \u001B[38;5;66;03m# group by dtype\u001B[39;00m\n\u001B[0;32m-> 2203\u001B[0m grouper \u001B[38;5;241m=\u001B[39m \u001B[43mitertools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtuples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_grouping_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2205\u001B[0m nbs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   2206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (_, _, dtype), tup_block \u001B[38;5;129;01min\u001B[39;00m grouper:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [2,3,4]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.03566889837384224\n",
      "Epoch 2 training loss: 0.01830665022134781\n",
      "Epoch 3 training loss: 0.013418372720479965\n",
      "Epoch 4 training loss: 0.011778959073126316\n",
      "Epoch 5 training loss: 0.01130879856646061\n",
      "Epoch 6 training loss: 0.011056452058255672\n",
      "Epoch 7 training loss: 0.01079351082444191\n",
      "Epoch 8 training loss: 0.010838769376277924\n",
      "Epoch 9 training loss: 0.010137386620044708\n",
      "Epoch 10 training loss: 0.010338276624679565\n",
      "Epoch 11 training loss: 0.010535934008657932\n",
      "Epoch 12 training loss: 0.009792396798729897\n",
      "Epoch 13 training loss: 0.009921427816152573\n",
      "Epoch 14 training loss: 0.009429894387722015\n",
      "Epoch 15 training loss: 0.008734602481126785\n",
      "Epoch 16 training loss: 0.009123115800321102\n",
      "Epoch 17 training loss: 0.009666664525866508\n",
      "Epoch 18 training loss: 0.008979364298284054\n",
      "Epoch 19 training loss: 0.009243450127542019\n",
      "Epoch 20 training loss: 0.008844000287353992\n",
      "Epoch 21 training loss: 0.00910976342856884\n",
      "Epoch 22 training loss: 0.008840230293571949\n",
      "Epoch 23 training loss: 0.008172215893864632\n",
      "Epoch 24 training loss: 0.009498702362179756\n",
      "Epoch 25 training loss: 0.00898265652358532\n",
      "Epoch 26 training loss: 0.008497258648276329\n",
      "Epoch 27 training loss: 0.008709250018000603\n",
      "Epoch 28 training loss: 0.008624088950455189\n",
      "Epoch 29 training loss: 0.00815357081592083\n",
      "Epoch 30 training loss: 0.009594975039362907\n",
      "Epoch 31 training loss: 0.009273276664316654\n",
      "Epoch 32 training loss: 0.008483963087201118\n",
      "0.8185158722057773\n",
      "[[8019    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.8185158722057773\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/868071132.py:10: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=10, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  100  bootstrap:  True\n",
      "0.4315217391304348\n",
      "[[1457    1]\n",
      " [2091  131]]\n",
      "Accuracy:  0.4315217391304348\n",
      "Precision:  0.9924242424242424\n",
      "Recall:  0.05895589558955896\n",
      "F1:  0.11129991503823279\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  100  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  100  bootstrap:  False\n",
      "0.5076086956521739\n",
      "[[1426   32]\n",
      " [1780  442]]\n",
      "Accuracy:  0.5076086956521739\n",
      "Precision:  0.9324894514767933\n",
      "Recall:  0.19891989198919893\n",
      "F1:  0.327893175074184\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  100  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  200  bootstrap:  True\n",
      "0.40054347826086956\n",
      "[[1458    0]\n",
      " [2206   16]]\n",
      "Accuracy:  0.40054347826086956\n",
      "Precision:  1.0\n",
      "Recall:  0.0072007200720072\n",
      "F1:  0.014298480786416443\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  200  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  200  bootstrap:  False\n",
      "0.41440217391304346\n",
      "[[1454    4]\n",
      " [2151   71]]\n",
      "Accuracy:  0.41440217391304346\n",
      "Precision:  0.9466666666666667\n",
      "Recall:  0.03195319531953195\n",
      "F1:  0.06181976491075316\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  200  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  400  bootstrap:  True\n",
      "0.39918478260869567\n",
      "[[1458    0]\n",
      " [2211   11]]\n",
      "Accuracy:  0.39918478260869567\n",
      "Precision:  1.0\n",
      "Recall:  0.0049504950495049506\n",
      "F1:  0.009852216748768473\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  400  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  400  bootstrap:  False\n",
      "0.4008152173913043\n",
      "[[1458    0]\n",
      " [2205   17]]\n",
      "Accuracy:  0.4008152173913043\n",
      "Precision:  1.0\n",
      "Recall:  0.00765076507650765\n",
      "F1:  0.015185350602947744\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  400  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  800  bootstrap:  True\n",
      "0.4016304347826087\n",
      "[[1458    0]\n",
      " [2202   20]]\n",
      "Accuracy:  0.4016304347826087\n",
      "Precision:  1.0\n",
      "Recall:  0.009000900090009001\n",
      "F1:  0.01784121320249777\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  800  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  800  bootstrap:  False\n",
      "0.3967391304347826\n",
      "[[1458    0]\n",
      " [2220    2]]\n",
      "Accuracy:  0.3967391304347826\n",
      "Precision:  1.0\n",
      "Recall:  0.0009000900090009\n",
      "F1:  0.0017985611510791368\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [100, 200, 400, 800]\n",
    "bootstraps = [True, False]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "    for b in bootstraps:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"bootstrap\": b,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.04122960567474365\n",
      "Epoch 2 training loss: 0.032301533967256546\n",
      "Epoch 3 training loss: 0.029458846896886826\n",
      "Epoch 4 training loss: 0.029231302440166473\n",
      "Epoch 5 training loss: 0.026410769671201706\n",
      "Epoch 6 training loss: 0.024050837382674217\n",
      "Epoch 7 training loss: 0.02383429929614067\n",
      "Epoch 8 training loss: 0.02304689958691597\n",
      "Epoch 9 training loss: 0.023099087178707123\n",
      "Epoch 10 training loss: 0.022938374429941177\n",
      "Epoch 11 training loss: 0.021551033481955528\n",
      "Epoch 12 training loss: 0.020283041521906853\n",
      "Epoch 13 training loss: 0.019526956602931023\n",
      "Epoch 14 training loss: 0.018245210871100426\n",
      "Epoch 15 training loss: 0.01643492467701435\n",
      "Epoch 16 training loss: 0.01508433185517788\n",
      "Epoch 17 training loss: 0.014789544977247715\n",
      "Epoch 18 training loss: 0.014925693161785603\n",
      "Epoch 19 training loss: 0.013959605246782303\n",
      "Epoch 20 training loss: 0.014511987566947937\n",
      "Epoch 21 training loss: 0.014036488719284534\n",
      "Epoch 22 training loss: 0.01356128416955471\n",
      "Epoch 23 training loss: 0.013709161430597305\n",
      "Epoch 24 training loss: 0.014896780252456665\n",
      "Epoch 25 training loss: 0.013893967494368553\n",
      "Epoch 26 training loss: 0.013346045278012753\n",
      "Epoch 27 training loss: 0.013873269781470299\n",
      "Epoch 28 training loss: 0.013923469930887222\n",
      "Epoch 29 training loss: 0.013328543864190578\n",
      "Epoch 30 training loss: 0.013698993250727654\n",
      "Epoch 31 training loss: 0.013894200325012207\n",
      "Epoch 32 training loss: 0.01420495193451643\n",
      "0.7647831800262812\n",
      "[[8730   18]\n",
      " [2667    0]]\n",
      "Accuracy: 0.7647831800262812\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 24, 16, 8], [8, 16, 24, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=11, clean_test_data_ratio=0.3, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.058898258954286575\n",
      "Epoch 2 training loss: 0.05155443772673607\n",
      "Epoch 3 training loss: 0.05057140812277794\n",
      "Epoch 4 training loss: 0.04858234152197838\n",
      "Epoch 5 training loss: 0.04724820703268051\n",
      "Epoch 6 training loss: 0.04657506197690964\n",
      "Epoch 7 training loss: 0.045772116631269455\n",
      "Epoch 8 training loss: 0.045489318668842316\n",
      "Epoch 9 training loss: 0.045221779495477676\n",
      "Epoch 10 training loss: 0.04497319832444191\n",
      "Epoch 11 training loss: 0.043941523879766464\n",
      "Epoch 12 training loss: 0.04381180927157402\n",
      "Epoch 13 training loss: 0.04358973726630211\n",
      "Epoch 14 training loss: 0.043723657727241516\n",
      "Epoch 15 training loss: 0.04434793069958687\n",
      "Epoch 16 training loss: 0.04259477183222771\n",
      "Epoch 17 training loss: 0.04313371703028679\n",
      "Epoch 18 training loss: 0.04417560249567032\n",
      "Epoch 19 training loss: 0.04458608478307724\n",
      "Epoch 20 training loss: 0.04394195228815079\n",
      "Epoch 21 training loss: 0.04413234442472458\n",
      "Epoch 22 training loss: 0.04372331127524376\n",
      "Epoch 23 training loss: 0.04293721169233322\n",
      "Epoch 24 training loss: 0.04323875531554222\n",
      "Epoch 25 training loss: 0.042636580765247345\n",
      "Epoch 26 training loss: 0.042571090161800385\n",
      "Epoch 27 training loss: 0.042420923709869385\n",
      "Epoch 28 training loss: 0.042121902108192444\n",
      "Epoch 29 training loss: 0.04230155050754547\n",
      "Epoch 30 training loss: 0.04195545241236687\n",
      "Epoch 31 training loss: 0.04092766344547272\n",
      "Epoch 32 training loss: 0.04151569679379463\n",
      "Epoch 33 training loss: 0.040453843772411346\n",
      "Epoch 34 training loss: 0.04354794695973396\n",
      "Epoch 35 training loss: 0.040512025356292725\n",
      "Epoch 36 training loss: 0.040154166519641876\n",
      "Epoch 37 training loss: 0.04132810980081558\n",
      "Epoch 38 training loss: 0.040305864065885544\n",
      "Epoch 39 training loss: 0.04092114418745041\n",
      "Epoch 40 training loss: 0.04208670184016228\n",
      "Epoch 41 training loss: 0.04183347523212433\n",
      "Epoch 42 training loss: 0.04109683260321617\n",
      "Epoch 43 training loss: 0.04025791957974434\n",
      "Epoch 44 training loss: 0.03995497524738312\n",
      "Epoch 45 training loss: 0.04097607359290123\n",
      "Epoch 46 training loss: 0.05953291058540344\n",
      "Epoch 47 training loss: 0.07895916700363159\n",
      "Epoch 48 training loss: 0.06983033567667007\n",
      "Epoch 49 training loss: 0.06246565654873848\n",
      "Epoch 50 training loss: 0.04813380911946297\n",
      "Epoch 51 training loss: 0.0453314334154129\n",
      "Epoch 52 training loss: 0.04376808926463127\n",
      "Epoch 53 training loss: 0.042266298085451126\n",
      "Epoch 54 training loss: 0.053543899208307266\n",
      "Epoch 55 training loss: 0.06179076060652733\n",
      "Epoch 56 training loss: 0.05984741076827049\n",
      "Epoch 57 training loss: 0.061102330684661865\n",
      "Epoch 58 training loss: 0.05008704960346222\n",
      "Epoch 59 training loss: 0.04680133983492851\n",
      "Epoch 60 training loss: 0.045489221811294556\n",
      "Epoch 61 training loss: 0.045494046062231064\n",
      "Epoch 62 training loss: 0.045764144510030746\n",
      "Epoch 63 training loss: 0.0467325896024704\n",
      "Epoch 64 training loss: 0.04569430276751518\n",
      "0.47950589556428974\n",
      "[[2888 5131]\n",
      " [ 431 2236]]\n",
      "Accuracy: 0.47950589556428974\n",
      "Precision:  0.30351567802361884\n",
      "Recall:  0.838395200599925\n",
      "F1:  0.44568467211480967\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05829282104969025\n",
      "Epoch 2 training loss: 0.05097535252571106\n",
      "Epoch 3 training loss: 0.0498952753841877\n",
      "Epoch 4 training loss: 0.04790065810084343\n",
      "Epoch 5 training loss: 0.04763051122426987\n",
      "Epoch 6 training loss: 0.0501713901758194\n",
      "Epoch 7 training loss: 0.05156087875366211\n",
      "Epoch 8 training loss: 0.04860074445605278\n",
      "Epoch 9 training loss: 0.04710253328084946\n",
      "Epoch 10 training loss: 0.04843513295054436\n",
      "Epoch 11 training loss: 0.04762199521064758\n",
      "Epoch 12 training loss: 0.0470193512737751\n",
      "Epoch 13 training loss: 0.04710486903786659\n",
      "Epoch 14 training loss: 0.04916980862617493\n",
      "Epoch 15 training loss: 0.047472093254327774\n",
      "Epoch 16 training loss: 0.045982830226421356\n",
      "Epoch 17 training loss: 0.047718703746795654\n",
      "Epoch 18 training loss: 0.046799905598163605\n",
      "Epoch 19 training loss: 0.0502435639500618\n",
      "Epoch 20 training loss: 0.05604420229792595\n",
      "Epoch 21 training loss: 0.05407804250717163\n",
      "Epoch 22 training loss: 0.05319788679480553\n",
      "Epoch 23 training loss: 0.05286208540201187\n",
      "Epoch 24 training loss: 0.05242634192109108\n",
      "Epoch 25 training loss: 0.051848530769348145\n",
      "Epoch 26 training loss: 0.05164097994565964\n",
      "Epoch 27 training loss: 0.05050581693649292\n",
      "Epoch 28 training loss: 0.05016383156180382\n",
      "Epoch 29 training loss: 0.05007558315992355\n",
      "Epoch 30 training loss: 0.051285270601511\n",
      "Epoch 31 training loss: 0.07751145958900452\n",
      "Epoch 32 training loss: 0.0772283524274826\n",
      "Epoch 33 training loss: 0.07716844975948334\n",
      "Epoch 34 training loss: 0.07719416171312332\n",
      "Epoch 35 training loss: 0.07715099304914474\n",
      "Epoch 36 training loss: 0.07715228199958801\n",
      "Epoch 37 training loss: 0.07712981849908829\n",
      "Epoch 38 training loss: 0.0771409124135971\n",
      "Epoch 39 training loss: 0.07716117054224014\n",
      "Epoch 40 training loss: 0.07712095975875854\n",
      "Epoch 41 training loss: 0.07708992063999176\n",
      "Epoch 42 training loss: 0.07712382078170776\n",
      "Epoch 43 training loss: 0.07712587714195251\n",
      "Epoch 44 training loss: 0.07708938419818878\n",
      "Epoch 45 training loss: 0.07708832621574402\n",
      "Epoch 46 training loss: 0.07708758115768433\n",
      "Epoch 47 training loss: 0.07712376862764359\n",
      "Epoch 48 training loss: 0.07710060477256775\n",
      "Epoch 49 training loss: 0.07710567861795425\n",
      "Epoch 50 training loss: 0.07708684355020523\n",
      "Epoch 51 training loss: 0.07705388963222504\n",
      "Epoch 52 training loss: 0.07705193758010864\n",
      "Epoch 53 training loss: 0.07709895074367523\n",
      "Epoch 54 training loss: 0.07709212601184845\n",
      "Epoch 55 training loss: 0.07709264755249023\n",
      "Epoch 56 training loss: 0.07708374410867691\n",
      "Epoch 57 training loss: 0.07709158211946487\n",
      "Epoch 58 training loss: 0.07708431780338287\n",
      "Epoch 59 training loss: 0.07708100229501724\n",
      "Epoch 60 training loss: 0.0770869180560112\n",
      "Epoch 61 training loss: 0.07706017047166824\n",
      "Epoch 62 training loss: 0.07704704254865646\n",
      "Epoch 63 training loss: 0.07706429064273834\n",
      "Epoch 64 training loss: 0.07704580575227737\n",
      "Epoch 65 training loss: 0.07707887887954712\n",
      "Epoch 66 training loss: 0.07708815485239029\n",
      "Epoch 67 training loss: 0.07705943286418915\n",
      "Epoch 68 training loss: 0.07705625891685486\n",
      "Epoch 69 training loss: 0.07703600078821182\n",
      "Epoch 70 training loss: 0.07705473899841309\n",
      "Epoch 71 training loss: 0.07705537974834442\n",
      "Epoch 72 training loss: 0.07704281806945801\n",
      "Epoch 73 training loss: 0.07704969495534897\n",
      "Epoch 74 training loss: 0.07704239338636398\n",
      "Epoch 75 training loss: 0.07703730463981628\n",
      "Epoch 76 training loss: 0.07702906429767609\n",
      "Epoch 77 training loss: 0.07700634002685547\n",
      "Epoch 78 training loss: 0.07702942937612534\n",
      "Epoch 79 training loss: 0.07702913880348206\n",
      "Epoch 80 training loss: 0.07703971117734909\n",
      "Epoch 81 training loss: 0.0770302414894104\n",
      "Epoch 82 training loss: 0.07702948153018951\n",
      "Epoch 83 training loss: 0.07702755928039551\n",
      "Epoch 84 training loss: 0.07703575491905212\n",
      "Epoch 85 training loss: 0.07703883945941925\n",
      "Epoch 86 training loss: 0.07702811062335968\n",
      "Epoch 87 training loss: 0.07703310996294022\n",
      "Epoch 88 training loss: 0.07703188061714172\n",
      "Epoch 89 training loss: 0.0770219936966896\n",
      "Epoch 90 training loss: 0.07701282203197479\n",
      "Epoch 91 training loss: 0.07702331244945526\n",
      "Epoch 92 training loss: 0.07702778279781342\n",
      "Epoch 93 training loss: 0.07703300565481186\n",
      "Epoch 94 training loss: 0.07703817635774612\n",
      "Epoch 95 training loss: 0.07702020555734634\n",
      "Epoch 96 training loss: 0.07703542709350586\n",
      "Epoch 97 training loss: 0.07701947540044785\n",
      "Epoch 98 training loss: 0.07702722400426865\n",
      "Epoch 99 training loss: 0.07701949775218964\n",
      "Epoch 100 training loss: 0.07704021781682968\n",
      "Epoch 101 training loss: 0.07701585441827774\n",
      "Epoch 102 training loss: 0.07704055309295654\n",
      "Epoch 103 training loss: 0.07703545689582825\n",
      "Epoch 104 training loss: 0.07702822238206863\n",
      "Epoch 105 training loss: 0.07702390849590302\n",
      "Epoch 106 training loss: 0.07703152298927307\n",
      "Epoch 107 training loss: 0.07702185958623886\n",
      "Epoch 108 training loss: 0.07702432572841644\n",
      "Epoch 109 training loss: 0.07702384889125824\n",
      "Epoch 110 training loss: 0.07701688259840012\n",
      "Epoch 111 training loss: 0.07702643424272537\n",
      "Epoch 112 training loss: 0.07702019810676575\n",
      "Epoch 113 training loss: 0.07704506069421768\n",
      "Epoch 114 training loss: 0.07703159004449844\n",
      "Epoch 115 training loss: 0.07703684270381927\n",
      "Epoch 116 training loss: 0.07701458781957626\n",
      "Epoch 117 training loss: 0.07704203575849533\n",
      "Epoch 118 training loss: 0.07702761888504028\n",
      "Epoch 119 training loss: 0.07701722532510757\n",
      "Epoch 120 training loss: 0.07703814655542374\n",
      "Epoch 121 training loss: 0.07704107463359833\n",
      "Epoch 122 training loss: 0.0770290419459343\n",
      "Epoch 123 training loss: 0.07702631503343582\n",
      "Epoch 124 training loss: 0.07701708376407623\n",
      "Epoch 125 training loss: 0.07703052461147308\n",
      "Epoch 126 training loss: 0.07702711969614029\n",
      "Epoch 127 training loss: 0.07702083885669708\n",
      "Epoch 128 training loss: 0.07702898234128952\n",
      "Epoch 129 training loss: 0.07703835517168045\n",
      "Epoch 130 training loss: 0.07700995355844498\n",
      "Epoch 131 training loss: 0.07702820748090744\n",
      "Epoch 132 training loss: 0.07702609151601791\n",
      "Epoch 133 training loss: 0.07702130079269409\n",
      "Epoch 134 training loss: 0.07702707499265671\n",
      "Epoch 135 training loss: 0.07702576369047165\n",
      "Epoch 136 training loss: 0.07702488452196121\n",
      "Epoch 137 training loss: 0.07699967920780182\n",
      "Epoch 138 training loss: 0.07703471183776855\n",
      "Epoch 139 training loss: 0.07701798528432846\n",
      "Epoch 140 training loss: 0.07703015208244324\n",
      "Epoch 141 training loss: 0.07700847834348679\n",
      "Epoch 142 training loss: 0.07703274488449097\n",
      "Epoch 143 training loss: 0.0770258828997612\n",
      "Epoch 144 training loss: 0.07701438665390015\n",
      "Epoch 145 training loss: 0.07703520357608795\n",
      "Epoch 146 training loss: 0.07702981680631638\n",
      "Epoch 147 training loss: 0.07703104615211487\n",
      "Epoch 148 training loss: 0.07702022045850754\n",
      "Epoch 149 training loss: 0.07701580226421356\n",
      "Epoch 150 training loss: 0.07703147828578949\n",
      "Epoch 151 training loss: 0.07703186571598053\n",
      "Epoch 152 training loss: 0.07701794803142548\n",
      "Epoch 153 training loss: 0.07702413946390152\n",
      "Epoch 154 training loss: 0.07703357934951782\n",
      "Epoch 155 training loss: 0.07702314108610153\n",
      "Epoch 156 training loss: 0.07703682780265808\n",
      "Epoch 157 training loss: 0.07703470438718796\n",
      "Epoch 158 training loss: 0.07702498137950897\n",
      "Epoch 159 training loss: 0.07701396942138672\n",
      "Epoch 160 training loss: 0.07702178508043289\n",
      "Epoch 161 training loss: 0.07700908929109573\n",
      "Epoch 162 training loss: 0.07701052725315094\n",
      "Epoch 163 training loss: 0.0770343616604805\n",
      "Epoch 164 training loss: 0.07701237499713898\n",
      "Epoch 165 training loss: 0.07701359689235687\n",
      "Epoch 166 training loss: 0.07702820003032684\n",
      "Epoch 167 training loss: 0.07703246176242828\n",
      "Epoch 168 training loss: 0.07702775299549103\n",
      "Epoch 169 training loss: 0.07704504579305649\n",
      "Epoch 170 training loss: 0.07701685279607773\n",
      "Epoch 171 training loss: 0.07703884690999985\n",
      "Epoch 172 training loss: 0.07703987509012222\n",
      "Epoch 173 training loss: 0.07701688259840012\n",
      "Epoch 174 training loss: 0.07703504711389542\n",
      "Epoch 175 training loss: 0.07703150063753128\n",
      "Epoch 176 training loss: 0.07702753692865372\n",
      "Epoch 177 training loss: 0.07702451944351196\n",
      "Epoch 178 training loss: 0.07702696323394775\n",
      "Epoch 179 training loss: 0.07703723013401031\n",
      "Epoch 180 training loss: 0.07703053206205368\n",
      "Epoch 181 training loss: 0.0770321935415268\n",
      "Epoch 182 training loss: 0.07702634483575821\n",
      "Epoch 183 training loss: 0.07703079283237457\n",
      "Epoch 184 training loss: 0.07704038918018341\n",
      "Epoch 185 training loss: 0.07702688872814178\n",
      "Epoch 186 training loss: 0.07701125741004944\n",
      "Epoch 187 training loss: 0.07702894508838654\n",
      "Epoch 188 training loss: 0.07702581584453583\n",
      "Epoch 189 training loss: 0.07704629749059677\n",
      "Epoch 190 training loss: 0.07702954113483429\n",
      "Epoch 191 training loss: 0.07702025026082993\n",
      "Epoch 192 training loss: 0.07700329273939133\n",
      "Epoch 193 training loss: 0.07702518999576569\n",
      "Epoch 194 training loss: 0.07702670246362686\n",
      "Epoch 195 training loss: 0.07702674716711044\n",
      "Epoch 196 training loss: 0.07700877636671066\n",
      "Epoch 197 training loss: 0.07702724635601044\n",
      "Epoch 198 training loss: 0.07703027129173279\n",
      "Epoch 199 training loss: 0.07702340930700302\n",
      "Epoch 200 training loss: 0.07702601701021194\n",
      "Epoch 201 training loss: 0.07704586535692215\n",
      "Epoch 202 training loss: 0.07702969014644623\n",
      "Epoch 203 training loss: 0.07702009379863739\n",
      "Epoch 204 training loss: 0.0770278349518776\n",
      "Epoch 205 training loss: 0.07703326642513275\n",
      "Epoch 206 training loss: 0.07701940834522247\n",
      "Epoch 207 training loss: 0.07703382521867752\n",
      "Epoch 208 training loss: 0.07702875137329102\n",
      "Epoch 209 training loss: 0.07701776176691055\n",
      "Epoch 210 training loss: 0.07703078538179398\n",
      "Epoch 211 training loss: 0.07701732963323593\n",
      "Epoch 212 training loss: 0.07701758295297623\n",
      "Epoch 213 training loss: 0.07702015340328217\n",
      "Epoch 214 training loss: 0.0770338624715805\n",
      "Epoch 215 training loss: 0.07702308148145676\n",
      "Epoch 216 training loss: 0.07702900469303131\n",
      "Epoch 217 training loss: 0.07703094929456711\n",
      "Epoch 218 training loss: 0.0770266205072403\n",
      "Epoch 219 training loss: 0.07702970504760742\n",
      "Epoch 220 training loss: 0.07701800018548965\n",
      "Epoch 221 training loss: 0.07701873034238815\n",
      "Epoch 222 training loss: 0.07701938599348068\n",
      "Epoch 223 training loss: 0.07697942852973938\n",
      "Epoch 224 training loss: 0.07701562345027924\n",
      "Epoch 225 training loss: 0.07702654600143433\n",
      "Epoch 226 training loss: 0.0770338699221611\n",
      "Epoch 227 training loss: 0.07702809572219849\n",
      "Epoch 228 training loss: 0.0770164206624031\n",
      "Epoch 229 training loss: 0.07703149318695068\n",
      "Epoch 230 training loss: 0.07700888812541962\n",
      "Epoch 231 training loss: 0.07704063504934311\n",
      "Epoch 232 training loss: 0.07702436298131943\n",
      "Epoch 233 training loss: 0.07703782618045807\n",
      "Epoch 234 training loss: 0.07704062759876251\n",
      "Epoch 235 training loss: 0.07703305780887604\n",
      "Epoch 236 training loss: 0.07702900469303131\n",
      "Epoch 237 training loss: 0.07701142132282257\n",
      "Epoch 238 training loss: 0.07702313363552094\n",
      "Epoch 239 training loss: 0.07703787833452225\n",
      "Epoch 240 training loss: 0.07701330631971359\n",
      "Epoch 241 training loss: 0.07703802734613419\n",
      "Epoch 242 training loss: 0.07701446861028671\n",
      "Epoch 243 training loss: 0.07702381908893585\n",
      "Epoch 244 training loss: 0.07703627645969391\n",
      "Epoch 245 training loss: 0.07703565061092377\n",
      "Epoch 246 training loss: 0.0770408883690834\n",
      "Epoch 247 training loss: 0.07700607180595398\n",
      "Epoch 248 training loss: 0.07702819257974625\n",
      "Epoch 249 training loss: 0.07703020423650742\n",
      "Epoch 250 training loss: 0.07705322653055191\n",
      "Epoch 251 training loss: 0.07701022922992706\n",
      "Epoch 252 training loss: 0.07701870799064636\n",
      "Epoch 253 training loss: 0.07703284174203873\n",
      "Epoch 254 training loss: 0.07703067362308502\n",
      "Epoch 255 training loss: 0.07703249156475067\n",
      "Epoch 256 training loss: 0.07703877985477448\n",
      "0.6038787878787879\n",
      "[[1338  120]\n",
      " [1514 1153]]\n",
      "Accuracy: 0.6038787878787879\n",
      "Precision:  0.9057344854673999\n",
      "Recall:  0.432320959880015\n",
      "F1:  0.5852791878172589\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07081352174282074\n",
      "Epoch 2 training loss: 0.05177604779601097\n",
      "Epoch 3 training loss: 0.04481825605034828\n",
      "Epoch 4 training loss: 0.042032934725284576\n",
      "Epoch 5 training loss: 0.03975117206573486\n",
      "Epoch 6 training loss: 0.036157939583063126\n",
      "Epoch 7 training loss: 0.03292706608772278\n",
      "Epoch 8 training loss: 0.030977612361311913\n",
      "Epoch 9 training loss: 0.029723944142460823\n",
      "Epoch 10 training loss: 0.028918273746967316\n",
      "Epoch 11 training loss: 0.02801123820245266\n",
      "Epoch 12 training loss: 0.027473557740449905\n",
      "Epoch 13 training loss: 0.02687661163508892\n",
      "Epoch 14 training loss: 0.026449065655469894\n",
      "Epoch 15 training loss: 0.02608298324048519\n",
      "Epoch 16 training loss: 0.02545854263007641\n",
      "Epoch 17 training loss: 0.025329427793622017\n",
      "Epoch 18 training loss: 0.024881193414330482\n",
      "Epoch 19 training loss: 0.024671414867043495\n",
      "Epoch 20 training loss: 0.024505024775862694\n",
      "Epoch 21 training loss: 0.024299871176481247\n",
      "Epoch 22 training loss: 0.02397942915558815\n",
      "Epoch 23 training loss: 0.023837454617023468\n",
      "Epoch 24 training loss: 0.023697637021541595\n",
      "Epoch 25 training loss: 0.023405183106660843\n",
      "Epoch 26 training loss: 0.02320265583693981\n",
      "Epoch 27 training loss: 0.023119444027543068\n",
      "Epoch 28 training loss: 0.02296951785683632\n",
      "Epoch 29 training loss: 0.022724036127328873\n",
      "Epoch 30 training loss: 0.02265932783484459\n",
      "Epoch 31 training loss: 0.022531751543283463\n",
      "Epoch 32 training loss: 0.022266661748290062\n",
      "Epoch 33 training loss: 0.022111665457487106\n",
      "Epoch 34 training loss: 0.021976502612233162\n",
      "Epoch 35 training loss: 0.021871916949748993\n",
      "Epoch 36 training loss: 0.0215756855905056\n",
      "Epoch 37 training loss: 0.021504340693354607\n",
      "Epoch 38 training loss: 0.021440714597702026\n",
      "Epoch 39 training loss: 0.0213240385055542\n",
      "Epoch 40 training loss: 0.02123262733221054\n",
      "Epoch 41 training loss: 0.020975656807422638\n",
      "Epoch 42 training loss: 0.020951854065060616\n",
      "Epoch 43 training loss: 0.020735614001750946\n",
      "Epoch 44 training loss: 0.020570144057273865\n",
      "Epoch 45 training loss: 0.0205476526170969\n",
      "Epoch 46 training loss: 0.0204680897295475\n",
      "Epoch 47 training loss: 0.020305106416344643\n",
      "Epoch 48 training loss: 0.020412979647517204\n",
      "Epoch 49 training loss: 0.020313970744609833\n",
      "Epoch 50 training loss: 0.019998932257294655\n",
      "Epoch 51 training loss: 0.01995219476521015\n",
      "Epoch 52 training loss: 0.01997661218047142\n",
      "Epoch 53 training loss: 0.01973620429635048\n",
      "Epoch 54 training loss: 0.019737521186470985\n",
      "Epoch 55 training loss: 0.019749490544199944\n",
      "Epoch 56 training loss: 0.019465280696749687\n",
      "Epoch 57 training loss: 0.01959570124745369\n",
      "Epoch 58 training loss: 0.019428182393312454\n",
      "Epoch 59 training loss: 0.01931731030344963\n",
      "Epoch 60 training loss: 0.019147686660289764\n",
      "Epoch 61 training loss: 0.01914028264582157\n",
      "Epoch 62 training loss: 0.01894281432032585\n",
      "Epoch 63 training loss: 0.018742889165878296\n",
      "Epoch 64 training loss: 0.018763482570648193\n",
      "0.6392727272727273\n",
      "[[ 651  807]\n",
      " [ 681 1986]]\n",
      "Accuracy: 0.6392727272727273\n",
      "Precision:  0.7110633727175081\n",
      "Recall:  0.7446569178852643\n",
      "F1:  0.7274725274725274\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.1696965992450714\n",
      "Epoch 2 training loss: 0.16787873208522797\n",
      "Epoch 3 training loss: 0.1669291853904724\n",
      "Epoch 4 training loss: 0.1668277382850647\n",
      "Epoch 5 training loss: 0.16672559082508087\n",
      "Epoch 6 training loss: 0.16642449796199799\n",
      "Epoch 7 training loss: 0.16632813215255737\n",
      "Epoch 8 training loss: 0.1662880927324295\n",
      "Epoch 9 training loss: 0.1662030816078186\n",
      "Epoch 10 training loss: 0.16611048579216003\n",
      "Epoch 11 training loss: 0.1660909354686737\n",
      "Epoch 12 training loss: 0.1659357100725174\n",
      "Epoch 13 training loss: 0.16550952196121216\n",
      "Epoch 14 training loss: 0.1651533991098404\n",
      "Epoch 15 training loss: 0.16507366299629211\n",
      "Epoch 16 training loss: 0.1650806963443756\n",
      "Epoch 17 training loss: 0.1650279462337494\n",
      "Epoch 18 training loss: 0.16502952575683594\n",
      "Epoch 19 training loss: 0.16496992111206055\n",
      "Epoch 20 training loss: 0.1649792492389679\n",
      "Epoch 21 training loss: 0.1649915724992752\n",
      "Epoch 22 training loss: 0.16495685279369354\n",
      "Epoch 23 training loss: 0.16488680243492126\n",
      "Epoch 24 training loss: 0.16492563486099243\n",
      "Epoch 25 training loss: 0.16487309336662292\n",
      "Epoch 26 training loss: 0.16495220363140106\n",
      "Epoch 27 training loss: 0.1648828238248825\n",
      "Epoch 28 training loss: 0.1648811399936676\n",
      "Epoch 29 training loss: 0.16485434770584106\n",
      "Epoch 30 training loss: 0.1648762971162796\n",
      "Epoch 31 training loss: 0.16476258635520935\n",
      "Epoch 32 training loss: 0.16461719572544098\n",
      "Epoch 33 training loss: 0.16450954973697662\n",
      "Epoch 34 training loss: 0.164473295211792\n",
      "Epoch 35 training loss: 0.1645013988018036\n",
      "Epoch 36 training loss: 0.16450057923793793\n",
      "Epoch 37 training loss: 0.16447032988071442\n",
      "Epoch 38 training loss: 0.16444745659828186\n",
      "Epoch 39 training loss: 0.16443201899528503\n",
      "Epoch 40 training loss: 0.16440320014953613\n",
      "Epoch 41 training loss: 0.16439244151115417\n",
      "Epoch 42 training loss: 0.1643671691417694\n",
      "Epoch 43 training loss: 0.16440723836421967\n",
      "Epoch 44 training loss: 0.1643637865781784\n",
      "Epoch 45 training loss: 0.16430599987506866\n",
      "Epoch 46 training loss: 0.16434316337108612\n",
      "Epoch 47 training loss: 0.1643010675907135\n",
      "Epoch 48 training loss: 0.1642955094575882\n",
      "Epoch 49 training loss: 0.16434188187122345\n",
      "Epoch 50 training loss: 0.16427671909332275\n",
      "Epoch 51 training loss: 0.16429077088832855\n",
      "Epoch 52 training loss: 0.16422796249389648\n",
      "Epoch 53 training loss: 0.1642664074897766\n",
      "Epoch 54 training loss: 0.16420139372348785\n",
      "Epoch 55 training loss: 0.16420480608940125\n",
      "Epoch 56 training loss: 0.16417406499385834\n",
      "Epoch 57 training loss: 0.16414174437522888\n",
      "Epoch 58 training loss: 0.16417136788368225\n",
      "Epoch 59 training loss: 0.16415856778621674\n",
      "Epoch 60 training loss: 0.16417217254638672\n",
      "Epoch 61 training loss: 0.16416549682617188\n",
      "Epoch 62 training loss: 0.16421039402484894\n",
      "Epoch 63 training loss: 0.16415011882781982\n",
      "Epoch 64 training loss: 0.16410236060619354\n",
      "0.47393939393939394\n",
      "[[ 363 1095]\n",
      " [1075 1592]]\n",
      "Accuracy: 0.47393939393939394\n",
      "Precision:  0.5924823222925195\n",
      "Recall:  0.5969253843269592\n",
      "F1:  0.5946955547254389\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17117565870285034\n",
      "Epoch 2 training loss: 0.17089219391345978\n",
      "Epoch 3 training loss: 0.17088712751865387\n",
      "Epoch 4 training loss: 0.17088398337364197\n",
      "Epoch 5 training loss: 0.17059989273548126\n",
      "Epoch 6 training loss: 0.16920214891433716\n",
      "Epoch 7 training loss: 0.16828873753547668\n",
      "Epoch 8 training loss: 0.1681583672761917\n",
      "Epoch 9 training loss: 0.1680826097726822\n",
      "Epoch 10 training loss: 0.1680685132741928\n",
      "Epoch 11 training loss: 0.16803313791751862\n",
      "Epoch 12 training loss: 0.16800935566425323\n",
      "Epoch 13 training loss: 0.168057382106781\n",
      "Epoch 14 training loss: 0.1680016666650772\n",
      "Epoch 15 training loss: 0.16803009808063507\n",
      "Epoch 16 training loss: 0.16799914836883545\n",
      "Epoch 17 training loss: 0.16800348460674286\n",
      "Epoch 18 training loss: 0.16800744831562042\n",
      "Epoch 19 training loss: 0.16804924607276917\n",
      "Epoch 20 training loss: 0.16799481213092804\n",
      "Epoch 21 training loss: 0.16799065470695496\n",
      "Epoch 22 training loss: 0.1679990142583847\n",
      "Epoch 23 training loss: 0.1679784655570984\n",
      "Epoch 24 training loss: 0.16798534989356995\n",
      "Epoch 25 training loss: 0.16797421872615814\n",
      "Epoch 26 training loss: 0.16798251867294312\n",
      "Epoch 27 training loss: 0.16802871227264404\n",
      "Epoch 28 training loss: 0.1679912656545639\n",
      "Epoch 29 training loss: 0.16801121830940247\n",
      "Epoch 30 training loss: 0.16802790760993958\n",
      "Epoch 31 training loss: 0.1680411547422409\n",
      "Epoch 32 training loss: 0.16803418099880219\n",
      "Epoch 33 training loss: 0.16799606382846832\n",
      "Epoch 34 training loss: 0.16797992587089539\n",
      "Epoch 35 training loss: 0.16796360909938812\n",
      "Epoch 36 training loss: 0.16796182096004486\n",
      "Epoch 37 training loss: 0.16797949373722076\n",
      "Epoch 38 training loss: 0.16797710955142975\n",
      "Epoch 39 training loss: 0.1680811047554016\n",
      "Epoch 40 training loss: 0.16799592971801758\n",
      "Epoch 41 training loss: 0.16800850629806519\n",
      "Epoch 42 training loss: 0.168021097779274\n",
      "Epoch 43 training loss: 0.1679965853691101\n",
      "Epoch 44 training loss: 0.1680385172367096\n",
      "Epoch 45 training loss: 0.1679650992155075\n",
      "Epoch 46 training loss: 0.16801804304122925\n",
      "Epoch 47 training loss: 0.1679927110671997\n",
      "Epoch 48 training loss: 0.16796480119228363\n",
      "Epoch 49 training loss: 0.16796045005321503\n",
      "Epoch 50 training loss: 0.16797663271427155\n",
      "Epoch 51 training loss: 0.16806751489639282\n",
      "Epoch 52 training loss: 0.16799931228160858\n",
      "Epoch 53 training loss: 0.16803543269634247\n",
      "Epoch 54 training loss: 0.1680329442024231\n",
      "Epoch 55 training loss: 0.16805870831012726\n",
      "Epoch 56 training loss: 0.1680757999420166\n",
      "Epoch 57 training loss: 0.16799744963645935\n",
      "Epoch 58 training loss: 0.16798177361488342\n",
      "Epoch 59 training loss: 0.16800926625728607\n",
      "Epoch 60 training loss: 0.1679632067680359\n",
      "Epoch 61 training loss: 0.16794443130493164\n",
      "Epoch 62 training loss: 0.16792601346969604\n",
      "Epoch 63 training loss: 0.16794176399707794\n",
      "Epoch 64 training loss: 0.16794586181640625\n",
      "Epoch 65 training loss: 0.16797718405723572\n",
      "Epoch 66 training loss: 0.16795514523983002\n",
      "Epoch 67 training loss: 0.1679099053144455\n",
      "Epoch 68 training loss: 0.16792942583560944\n",
      "Epoch 69 training loss: 0.16796092689037323\n",
      "Epoch 70 training loss: 0.16789768636226654\n",
      "Epoch 71 training loss: 0.167667955160141\n",
      "Epoch 72 training loss: 0.16696929931640625\n",
      "Epoch 73 training loss: 0.16685323417186737\n",
      "Epoch 74 training loss: 0.1667962372303009\n",
      "Epoch 75 training loss: 0.16677238047122955\n",
      "Epoch 76 training loss: 0.16678297519683838\n",
      "Epoch 77 training loss: 0.16672664880752563\n",
      "Epoch 78 training loss: 0.16673199832439423\n",
      "Epoch 79 training loss: 0.16663001477718353\n",
      "Epoch 80 training loss: 0.1662663221359253\n",
      "Epoch 81 training loss: 0.1658490002155304\n",
      "Epoch 82 training loss: 0.16563370823860168\n",
      "Epoch 83 training loss: 0.16552095115184784\n",
      "Epoch 84 training loss: 0.1654587984085083\n",
      "Epoch 85 training loss: 0.16537614166736603\n",
      "Epoch 86 training loss: 0.16537095606327057\n",
      "Epoch 87 training loss: 0.16531004011631012\n",
      "Epoch 88 training loss: 0.16529624164104462\n",
      "Epoch 89 training loss: 0.16529308259487152\n",
      "Epoch 90 training loss: 0.16527557373046875\n",
      "Epoch 91 training loss: 0.1652599573135376\n",
      "Epoch 92 training loss: 0.1651657074689865\n",
      "Epoch 93 training loss: 0.1652131825685501\n",
      "Epoch 94 training loss: 0.16520553827285767\n",
      "Epoch 95 training loss: 0.1651611328125\n",
      "Epoch 96 training loss: 0.1651621311903\n",
      "Epoch 97 training loss: 0.16519345343112946\n",
      "Epoch 98 training loss: 0.16517382860183716\n",
      "Epoch 99 training loss: 0.16509738564491272\n",
      "Epoch 100 training loss: 0.16507796943187714\n",
      "Epoch 101 training loss: 0.1651117503643036\n",
      "Epoch 102 training loss: 0.165104478597641\n",
      "Epoch 103 training loss: 0.16516909003257751\n",
      "Epoch 104 training loss: 0.16508862376213074\n",
      "Epoch 105 training loss: 0.16511014103889465\n",
      "Epoch 106 training loss: 0.16507498919963837\n",
      "Epoch 107 training loss: 0.16505128145217896\n",
      "Epoch 108 training loss: 0.16505971550941467\n",
      "Epoch 109 training loss: 0.1649939864873886\n",
      "Epoch 110 training loss: 0.16499775648117065\n",
      "Epoch 111 training loss: 0.16497525572776794\n",
      "Epoch 112 training loss: 0.16498683393001556\n",
      "Epoch 113 training loss: 0.1649920642375946\n",
      "Epoch 114 training loss: 0.1650433987379074\n",
      "Epoch 115 training loss: 0.16500139236450195\n",
      "Epoch 116 training loss: 0.1649690866470337\n",
      "Epoch 117 training loss: 0.1650148183107376\n",
      "Epoch 118 training loss: 0.16497363150119781\n",
      "Epoch 119 training loss: 0.16497468948364258\n",
      "Epoch 120 training loss: 0.1649494469165802\n",
      "Epoch 121 training loss: 0.16495713591575623\n",
      "Epoch 122 training loss: 0.16515107452869415\n",
      "Epoch 123 training loss: 0.1649789661169052\n",
      "Epoch 124 training loss: 0.16495779156684875\n",
      "Epoch 125 training loss: 0.1649857759475708\n",
      "Epoch 126 training loss: 0.16492027044296265\n",
      "Epoch 127 training loss: 0.1649385690689087\n",
      "Epoch 128 training loss: 0.16495110094547272\n",
      "Epoch 129 training loss: 0.1649756282567978\n",
      "Epoch 130 training loss: 0.16493456065654755\n",
      "Epoch 131 training loss: 0.1649821400642395\n",
      "Epoch 132 training loss: 0.16498489677906036\n",
      "Epoch 133 training loss: 0.164930060505867\n",
      "Epoch 134 training loss: 0.16489359736442566\n",
      "Epoch 135 training loss: 0.16491061449050903\n",
      "Epoch 136 training loss: 0.1649249941110611\n",
      "Epoch 137 training loss: 0.1650056391954422\n",
      "Epoch 138 training loss: 0.16496941447257996\n",
      "Epoch 139 training loss: 0.16494052112102509\n",
      "Epoch 140 training loss: 0.16495178639888763\n",
      "Epoch 141 training loss: 0.16487109661102295\n",
      "Epoch 142 training loss: 0.1648498773574829\n",
      "Epoch 143 training loss: 0.16483591496944427\n",
      "Epoch 144 training loss: 0.16483935713768005\n",
      "Epoch 145 training loss: 0.1648600548505783\n",
      "Epoch 146 training loss: 0.16486141085624695\n",
      "Epoch 147 training loss: 0.16491375863552094\n",
      "Epoch 148 training loss: 0.16492648422718048\n",
      "Epoch 149 training loss: 0.16493812203407288\n",
      "Epoch 150 training loss: 0.16490525007247925\n",
      "Epoch 151 training loss: 0.16487710177898407\n",
      "Epoch 152 training loss: 0.16487674415111542\n",
      "Epoch 153 training loss: 0.16483594477176666\n",
      "Epoch 154 training loss: 0.16483083367347717\n",
      "Epoch 155 training loss: 0.16483162343502045\n",
      "Epoch 156 training loss: 0.16494901478290558\n",
      "Epoch 157 training loss: 0.1650901883840561\n",
      "Epoch 158 training loss: 0.16496798396110535\n",
      "Epoch 159 training loss: 0.16495105624198914\n",
      "Epoch 160 training loss: 0.16491471230983734\n",
      "Epoch 161 training loss: 0.16486495733261108\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.16493481397628784\n",
      "Epoch 164 training loss: 0.1648513674736023\n",
      "Epoch 165 training loss: 0.16486524045467377\n",
      "Epoch 166 training loss: 0.16497018933296204\n",
      "Epoch 167 training loss: 0.16491641104221344\n",
      "Epoch 168 training loss: 0.16486111283302307\n",
      "Epoch 169 training loss: 0.164909228682518\n",
      "Epoch 170 training loss: 0.1649133265018463\n",
      "Epoch 171 training loss: 0.16487690806388855\n",
      "Epoch 172 training loss: 0.16485801339149475\n",
      "Epoch 173 training loss: 0.16486766934394836\n",
      "Epoch 174 training loss: 0.16484883427619934\n",
      "Epoch 175 training loss: 0.16482563316822052\n",
      "Epoch 176 training loss: 0.1648404896259308\n",
      "Epoch 177 training loss: 0.16486871242523193\n",
      "Epoch 178 training loss: 0.1648487150669098\n",
      "Epoch 179 training loss: 0.1648281067609787\n",
      "Epoch 180 training loss: 0.16482169926166534\n",
      "Epoch 181 training loss: 0.1648256778717041\n",
      "Epoch 182 training loss: 0.16484518349170685\n",
      "Epoch 183 training loss: 0.16496612131595612\n",
      "Epoch 184 training loss: 0.1648326814174652\n",
      "Epoch 185 training loss: 0.164798766374588\n",
      "Epoch 186 training loss: 0.164801687002182\n",
      "Epoch 187 training loss: 0.16480009257793427\n",
      "Epoch 188 training loss: 0.16483540832996368\n",
      "Epoch 189 training loss: 0.1649075597524643\n",
      "Epoch 190 training loss: 0.16480576992034912\n",
      "Epoch 191 training loss: 0.1647839993238449\n",
      "Epoch 192 training loss: 0.1648356318473816\n",
      "Epoch 193 training loss: 0.16480454802513123\n",
      "Epoch 194 training loss: 0.1647886484861374\n",
      "Epoch 195 training loss: 0.16479100286960602\n",
      "Epoch 196 training loss: 0.16481254994869232\n",
      "Epoch 197 training loss: 0.1648106724023819\n",
      "Epoch 198 training loss: 0.1648610383272171\n",
      "Epoch 199 training loss: 0.16488100588321686\n",
      "Epoch 200 training loss: 0.1648060381412506\n",
      "Epoch 201 training loss: 0.16479626297950745\n",
      "Epoch 202 training loss: 0.16479703783988953\n",
      "Epoch 203 training loss: 0.1648780256509781\n",
      "Epoch 204 training loss: 0.16486670076847076\n",
      "Epoch 205 training loss: 0.16480804979801178\n",
      "Epoch 206 training loss: 0.16483813524246216\n",
      "Epoch 207 training loss: 0.1648465096950531\n",
      "Epoch 208 training loss: 0.1648363471031189\n",
      "Epoch 209 training loss: 0.164854034781456\n",
      "Epoch 210 training loss: 0.1648809313774109\n",
      "Epoch 211 training loss: 0.1649470031261444\n",
      "Epoch 212 training loss: 0.1649084836244583\n",
      "Epoch 213 training loss: 0.16496233642101288\n",
      "Epoch 214 training loss: 0.16488410532474518\n",
      "Epoch 215 training loss: 0.16484808921813965\n",
      "Epoch 216 training loss: 0.1648230254650116\n",
      "Epoch 217 training loss: 0.16481532156467438\n",
      "Epoch 218 training loss: 0.16482269763946533\n",
      "Epoch 219 training loss: 0.16485249996185303\n",
      "Epoch 220 training loss: 0.16485776007175446\n",
      "Epoch 221 training loss: 0.16479940712451935\n",
      "Epoch 222 training loss: 0.1647922694683075\n",
      "Epoch 223 training loss: 0.16481277346611023\n",
      "Epoch 224 training loss: 0.1648196280002594\n",
      "Epoch 225 training loss: 0.1648918092250824\n",
      "Epoch 226 training loss: 0.16487829387187958\n",
      "Epoch 227 training loss: 0.16488078236579895\n",
      "Epoch 228 training loss: 0.164849191904068\n",
      "Epoch 229 training loss: 0.16477710008621216\n",
      "Epoch 230 training loss: 0.16477271914482117\n",
      "Epoch 231 training loss: 0.16476401686668396\n",
      "Epoch 232 training loss: 0.1647741198539734\n",
      "Epoch 233 training loss: 0.1648848056793213\n",
      "Epoch 234 training loss: 0.16478723287582397\n",
      "Epoch 235 training loss: 0.16479089856147766\n",
      "Epoch 236 training loss: 0.16477440297603607\n",
      "Epoch 237 training loss: 0.16477873921394348\n",
      "Epoch 238 training loss: 0.164784237742424\n",
      "Epoch 239 training loss: 0.16476275026798248\n",
      "Epoch 240 training loss: 0.16477172076702118\n",
      "Epoch 241 training loss: 0.16475346684455872\n",
      "Epoch 242 training loss: 0.1647641658782959\n",
      "Epoch 243 training loss: 0.16478458046913147\n",
      "Epoch 244 training loss: 0.16474385559558868\n",
      "Epoch 245 training loss: 0.16473190486431122\n",
      "Epoch 246 training loss: 0.16472983360290527\n",
      "Epoch 247 training loss: 0.164745032787323\n",
      "Epoch 248 training loss: 0.16475629806518555\n",
      "Epoch 249 training loss: 0.16477416455745697\n",
      "Epoch 250 training loss: 0.16477340459823608\n",
      "Epoch 251 training loss: 0.16476497054100037\n",
      "Epoch 252 training loss: 0.1647583246231079\n",
      "Epoch 253 training loss: 0.1647467464208603\n",
      "Epoch 254 training loss: 0.16475403308868408\n",
      "Epoch 255 training loss: 0.16489911079406738\n",
      "Epoch 256 training loss: 0.1648680567741394\n",
      "0.48218181818181816\n",
      "[[ 392 1066]\n",
      " [1070 1597]]\n",
      "Accuracy: 0.48218181818181816\n",
      "Precision:  0.5996995869320315\n",
      "Recall:  0.5988001499812523\n",
      "F1:  0.5992495309568481\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.Tanh(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17048297822475433\n",
      "Epoch 2 training loss: 0.16889290511608124\n",
      "Epoch 3 training loss: 0.16845721006393433\n",
      "Epoch 4 training loss: 0.16830764710903168\n",
      "Epoch 5 training loss: 0.16820263862609863\n",
      "Epoch 6 training loss: 0.16814833879470825\n",
      "Epoch 7 training loss: 0.16813574731349945\n",
      "Epoch 8 training loss: 0.16808335483074188\n",
      "Epoch 9 training loss: 0.16807159781455994\n",
      "Epoch 10 training loss: 0.16805912554264069\n",
      "Epoch 11 training loss: 0.16805769503116608\n",
      "Epoch 12 training loss: 0.16806276142597198\n",
      "Epoch 13 training loss: 0.1680343598127365\n",
      "Epoch 14 training loss: 0.1680484414100647\n",
      "Epoch 15 training loss: 0.16805490851402283\n",
      "Epoch 16 training loss: 0.16802315413951874\n",
      "Epoch 17 training loss: 0.1680458039045334\n",
      "Epoch 18 training loss: 0.16803789138793945\n",
      "Epoch 19 training loss: 0.16801948845386505\n",
      "Epoch 20 training loss: 0.16804732382297516\n",
      "Epoch 21 training loss: 0.1680135428905487\n",
      "Epoch 22 training loss: 0.16801586747169495\n",
      "Epoch 23 training loss: 0.16801030933856964\n",
      "Epoch 24 training loss: 0.16802343726158142\n",
      "Epoch 25 training loss: 0.16806061565876007\n",
      "Epoch 26 training loss: 0.1680029332637787\n",
      "Epoch 27 training loss: 0.16800610721111298\n",
      "Epoch 28 training loss: 0.16800940036773682\n",
      "Epoch 29 training loss: 0.16805070638656616\n",
      "Epoch 30 training loss: 0.16801466047763824\n",
      "Epoch 31 training loss: 0.16801109910011292\n",
      "Epoch 32 training loss: 0.16799896955490112\n",
      "Epoch 33 training loss: 0.1680048555135727\n",
      "Epoch 34 training loss: 0.16801267862319946\n",
      "Epoch 35 training loss: 0.16798287630081177\n",
      "Epoch 36 training loss: 0.16804930567741394\n",
      "Epoch 37 training loss: 0.16799293458461761\n",
      "Epoch 38 training loss: 0.16798101365566254\n",
      "Epoch 39 training loss: 0.1677250862121582\n",
      "Epoch 40 training loss: 0.166952446103096\n",
      "Epoch 41 training loss: 0.16678158938884735\n",
      "Epoch 42 training loss: 0.16661563515663147\n",
      "Epoch 43 training loss: 0.16582980751991272\n",
      "Epoch 44 training loss: 0.16569674015045166\n",
      "Epoch 45 training loss: 0.16553372144699097\n",
      "Epoch 46 training loss: 0.1655174195766449\n",
      "Epoch 47 training loss: 0.16547071933746338\n",
      "Epoch 48 training loss: 0.16548818349838257\n",
      "Epoch 49 training loss: 0.16539238393306732\n",
      "Epoch 50 training loss: 0.16536274552345276\n",
      "Epoch 51 training loss: 0.1654076725244522\n",
      "Epoch 52 training loss: 0.16531293094158173\n",
      "Epoch 53 training loss: 0.16527575254440308\n",
      "Epoch 54 training loss: 0.16525791585445404\n",
      "Epoch 55 training loss: 0.16516117751598358\n",
      "Epoch 56 training loss: 0.1651705503463745\n",
      "Epoch 57 training loss: 0.16516275703907013\n",
      "Epoch 58 training loss: 0.1651269644498825\n",
      "Epoch 59 training loss: 0.16513550281524658\n",
      "Epoch 60 training loss: 0.16511771082878113\n",
      "Epoch 61 training loss: 0.16506704688072205\n",
      "Epoch 62 training loss: 0.1650332510471344\n",
      "Epoch 63 training loss: 0.16504237055778503\n",
      "Epoch 64 training loss: 0.1650545299053192\n",
      "Epoch 65 training loss: 0.16508206725120544\n",
      "Epoch 66 training loss: 0.1649974286556244\n",
      "Epoch 67 training loss: 0.16497361660003662\n",
      "Epoch 68 training loss: 0.1649814248085022\n",
      "Epoch 69 training loss: 0.16504935920238495\n",
      "Epoch 70 training loss: 0.16501019895076752\n",
      "Epoch 71 training loss: 0.1649552583694458\n",
      "Epoch 72 training loss: 0.1650993376970291\n",
      "Epoch 73 training loss: 0.16508223116397858\n",
      "Epoch 74 training loss: 0.16517047584056854\n",
      "Epoch 75 training loss: 0.16521300375461578\n",
      "Epoch 76 training loss: 0.1650741696357727\n",
      "Epoch 77 training loss: 0.16509033739566803\n",
      "Epoch 78 training loss: 0.16508528590202332\n",
      "Epoch 79 training loss: 0.16500599682331085\n",
      "Epoch 80 training loss: 0.16493317484855652\n",
      "Epoch 81 training loss: 0.16506394743919373\n",
      "Epoch 82 training loss: 0.16497287154197693\n",
      "Epoch 83 training loss: 0.16493584215641022\n",
      "Epoch 84 training loss: 0.16493527591228485\n",
      "Epoch 85 training loss: 0.1649501770734787\n",
      "Epoch 86 training loss: 0.16490690410137177\n",
      "Epoch 87 training loss: 0.16495487093925476\n",
      "Epoch 88 training loss: 0.16495370864868164\n",
      "Epoch 89 training loss: 0.16503620147705078\n",
      "Epoch 90 training loss: 0.16493797302246094\n",
      "Epoch 91 training loss: 0.16498427093029022\n",
      "Epoch 92 training loss: 0.16493545472621918\n",
      "Epoch 93 training loss: 0.1649208962917328\n",
      "Epoch 94 training loss: 0.16495338082313538\n",
      "Epoch 95 training loss: 0.1649291068315506\n",
      "Epoch 96 training loss: 0.16492561995983124\n",
      "Epoch 97 training loss: 0.1648913323879242\n",
      "Epoch 98 training loss: 0.1649065613746643\n",
      "Epoch 99 training loss: 0.16493679583072662\n",
      "Epoch 100 training loss: 0.1649480015039444\n",
      "Epoch 101 training loss: 0.1650744378566742\n",
      "Epoch 102 training loss: 0.16502448916435242\n",
      "Epoch 103 training loss: 0.16501517593860626\n",
      "Epoch 104 training loss: 0.16496840119361877\n",
      "Epoch 105 training loss: 0.1649671494960785\n",
      "Epoch 106 training loss: 0.1649441123008728\n",
      "Epoch 107 training loss: 0.16488738358020782\n",
      "Epoch 108 training loss: 0.16491222381591797\n",
      "Epoch 109 training loss: 0.16490551829338074\n",
      "Epoch 110 training loss: 0.1649019420146942\n",
      "Epoch 111 training loss: 0.16496579349040985\n",
      "Epoch 112 training loss: 0.16494669020175934\n",
      "Epoch 113 training loss: 0.16490943729877472\n",
      "Epoch 114 training loss: 0.1649254858493805\n",
      "Epoch 115 training loss: 0.1649320423603058\n",
      "Epoch 116 training loss: 0.16492733359336853\n",
      "Epoch 117 training loss: 0.1649169772863388\n",
      "Epoch 118 training loss: 0.16515254974365234\n",
      "Epoch 119 training loss: 0.16511186957359314\n",
      "Epoch 120 training loss: 0.1650628000497818\n",
      "Epoch 121 training loss: 0.16499508917331696\n",
      "Epoch 122 training loss: 0.16491152346134186\n",
      "Epoch 123 training loss: 0.16504086554050446\n",
      "Epoch 124 training loss: 0.16507422924041748\n",
      "Epoch 125 training loss: 0.165010467171669\n",
      "Epoch 126 training loss: 0.16493843495845795\n",
      "Epoch 127 training loss: 0.16491617262363434\n",
      "Epoch 128 training loss: 0.1649228185415268\n",
      "Epoch 129 training loss: 0.1649019718170166\n",
      "Epoch 130 training loss: 0.16487954556941986\n",
      "Epoch 131 training loss: 0.16495588421821594\n",
      "Epoch 132 training loss: 0.1649574339389801\n",
      "Epoch 133 training loss: 0.16490253806114197\n",
      "Epoch 134 training loss: 0.1649157702922821\n",
      "Epoch 135 training loss: 0.16493511199951172\n",
      "Epoch 136 training loss: 0.16488772630691528\n",
      "Epoch 137 training loss: 0.16485793888568878\n",
      "Epoch 138 training loss: 0.1648665815591812\n",
      "Epoch 139 training loss: 0.16485625505447388\n",
      "Epoch 140 training loss: 0.16496773064136505\n",
      "Epoch 141 training loss: 0.165019229054451\n",
      "Epoch 142 training loss: 0.16496799886226654\n",
      "Epoch 143 training loss: 0.1649271547794342\n",
      "Epoch 144 training loss: 0.1649191528558731\n",
      "Epoch 145 training loss: 0.16488845646381378\n",
      "Epoch 146 training loss: 0.16489775478839874\n",
      "Epoch 147 training loss: 0.1648975908756256\n",
      "Epoch 148 training loss: 0.16485780477523804\n",
      "Epoch 149 training loss: 0.16485758125782013\n",
      "Epoch 150 training loss: 0.16484779119491577\n",
      "Epoch 151 training loss: 0.16487419605255127\n",
      "Epoch 152 training loss: 0.16494746506214142\n",
      "Epoch 153 training loss: 0.16492073237895966\n",
      "Epoch 154 training loss: 0.1648574322462082\n",
      "Epoch 155 training loss: 0.1648692786693573\n",
      "Epoch 156 training loss: 0.16485993564128876\n",
      "Epoch 157 training loss: 0.1649700254201889\n",
      "Epoch 158 training loss: 0.16499640047550201\n",
      "Epoch 159 training loss: 0.16493870317935944\n",
      "Epoch 160 training loss: 0.16493374109268188\n",
      "Epoch 161 training loss: 0.1649080067873001\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.1648789644241333\n",
      "Epoch 164 training loss: 0.16489528119564056\n",
      "Epoch 165 training loss: 0.16487745940685272\n",
      "Epoch 166 training loss: 0.1648242473602295\n",
      "Epoch 167 training loss: 0.1648487150669098\n",
      "Epoch 168 training loss: 0.1648620218038559\n",
      "Epoch 169 training loss: 0.1648893803358078\n",
      "Epoch 170 training loss: 0.1648567169904709\n",
      "Epoch 171 training loss: 0.16489006578922272\n",
      "Epoch 172 training loss: 0.16485270857810974\n",
      "Epoch 173 training loss: 0.16487672924995422\n",
      "Epoch 174 training loss: 0.16488471627235413\n",
      "Epoch 175 training loss: 0.16484229266643524\n",
      "Epoch 176 training loss: 0.16493725776672363\n",
      "Epoch 177 training loss: 0.1649102121591568\n",
      "Epoch 178 training loss: 0.16487260162830353\n",
      "Epoch 179 training loss: 0.16490183770656586\n",
      "Epoch 180 training loss: 0.16484715044498444\n",
      "Epoch 181 training loss: 0.16485095024108887\n",
      "Epoch 182 training loss: 0.16485632956027985\n",
      "Epoch 183 training loss: 0.16485297679901123\n",
      "Epoch 184 training loss: 0.16485093533992767\n",
      "Epoch 185 training loss: 0.16483157873153687\n",
      "Epoch 186 training loss: 0.16496655344963074\n",
      "Epoch 187 training loss: 0.16495396196842194\n",
      "Epoch 188 training loss: 0.16494569182395935\n",
      "Epoch 189 training loss: 0.16491083800792694\n",
      "Epoch 190 training loss: 0.1649261862039566\n",
      "Epoch 191 training loss: 0.16509567201137543\n",
      "Epoch 192 training loss: 0.1650398075580597\n",
      "Epoch 193 training loss: 0.16508892178535461\n",
      "Epoch 194 training loss: 0.1649836301803589\n",
      "Epoch 195 training loss: 0.1649351865053177\n",
      "Epoch 196 training loss: 0.16492505371570587\n",
      "Epoch 197 training loss: 0.16490179300308228\n",
      "Epoch 198 training loss: 0.16493715345859528\n",
      "Epoch 199 training loss: 0.16490860283374786\n",
      "Epoch 200 training loss: 0.16489098966121674\n",
      "Epoch 201 training loss: 0.16490168869495392\n",
      "Epoch 202 training loss: 0.16491514444351196\n",
      "Epoch 203 training loss: 0.16485656797885895\n",
      "Epoch 204 training loss: 0.16490283608436584\n",
      "Epoch 205 training loss: 0.164883553981781\n",
      "Epoch 206 training loss: 0.16486819088459015\n",
      "Epoch 207 training loss: 0.1648823767900467\n",
      "Epoch 208 training loss: 0.16494770348072052\n",
      "Epoch 209 training loss: 0.1649188995361328\n",
      "Epoch 210 training loss: 0.16486652195453644\n",
      "Epoch 211 training loss: 0.164960578083992\n",
      "Epoch 212 training loss: 0.16485953330993652\n",
      "Epoch 213 training loss: 0.16483987867832184\n",
      "Epoch 214 training loss: 0.1648828238248825\n",
      "Epoch 215 training loss: 0.16494596004486084\n",
      "Epoch 216 training loss: 0.16491232812404633\n",
      "Epoch 217 training loss: 0.16492034494876862\n",
      "Epoch 218 training loss: 0.16492900252342224\n",
      "Epoch 219 training loss: 0.16489380598068237\n",
      "Epoch 220 training loss: 0.16489483416080475\n",
      "Epoch 221 training loss: 0.16490140557289124\n",
      "Epoch 222 training loss: 0.16484548151493073\n",
      "Epoch 223 training loss: 0.16491907835006714\n",
      "Epoch 224 training loss: 0.16493640840053558\n",
      "Epoch 225 training loss: 0.164934903383255\n",
      "Epoch 226 training loss: 0.1648758202791214\n",
      "Epoch 227 training loss: 0.16483473777770996\n",
      "Epoch 228 training loss: 0.1648281216621399\n",
      "Epoch 229 training loss: 0.16489090025424957\n",
      "Epoch 230 training loss: 0.1648705154657364\n",
      "Epoch 231 training loss: 0.1648814082145691\n",
      "Epoch 232 training loss: 0.16482363641262054\n",
      "Epoch 233 training loss: 0.1649101823568344\n",
      "Epoch 234 training loss: 0.16488051414489746\n",
      "Epoch 235 training loss: 0.16489385068416595\n",
      "Epoch 236 training loss: 0.16489002108573914\n",
      "Epoch 237 training loss: 0.16491107642650604\n",
      "Epoch 238 training loss: 0.16488583385944366\n",
      "Epoch 239 training loss: 0.1649440973997116\n",
      "Epoch 240 training loss: 0.16485491394996643\n",
      "Epoch 241 training loss: 0.1648317128419876\n",
      "Epoch 242 training loss: 0.16481254994869232\n",
      "Epoch 243 training loss: 0.1648099273443222\n",
      "Epoch 244 training loss: 0.16481496393680573\n",
      "Epoch 245 training loss: 0.16482143104076385\n",
      "Epoch 246 training loss: 0.1648457646369934\n",
      "Epoch 247 training loss: 0.16480199992656708\n",
      "Epoch 248 training loss: 0.16478675603866577\n",
      "Epoch 249 training loss: 0.16480021178722382\n",
      "Epoch 250 training loss: 0.16483859717845917\n",
      "Epoch 251 training loss: 0.16479599475860596\n",
      "Epoch 252 training loss: 0.164817675948143\n",
      "Epoch 253 training loss: 0.16481716930866241\n",
      "Epoch 254 training loss: 0.16489620506763458\n",
      "Epoch 255 training loss: 0.16483238339424133\n",
      "Epoch 256 training loss: 0.1648678183555603\n",
      "0.49066666666666664\n",
      "[[ 359 1099]\n",
      " [1002 1665]]\n",
      "Accuracy: 0.49066666666666664\n",
      "Precision:  0.6023878437047757\n",
      "Recall:  0.6242969628796401\n",
      "F1:  0.6131467501380962\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=False\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.01, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(5, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [1000,1200,1600,2000,2400,2800,3200]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with rbf with anomalies being 10% of the data and one_hot=True\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=True)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 16, 32, 32)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06971529126167297\n",
      "Epoch 2 training loss: 0.04786883294582367\n",
      "Epoch 3 training loss: 0.04360324516892433\n",
      "Epoch 4 training loss: 0.04194796457886696\n",
      "Epoch 5 training loss: 0.040111180394887924\n",
      "Epoch 6 training loss: 0.03824681043624878\n",
      "Epoch 7 training loss: 0.03677377104759216\n",
      "Epoch 8 training loss: 0.03536611422896385\n",
      "Epoch 9 training loss: 0.03354225680232048\n",
      "Epoch 10 training loss: 0.03137403726577759\n",
      "Epoch 11 training loss: 0.029583103954792023\n",
      "Epoch 12 training loss: 0.028284654021263123\n",
      "Epoch 13 training loss: 0.02725023590028286\n",
      "Epoch 14 training loss: 0.026495160534977913\n",
      "Epoch 15 training loss: 0.026067033410072327\n",
      "Epoch 16 training loss: 0.02549590729176998\n",
      "Epoch 17 training loss: 0.024979382753372192\n",
      "Epoch 18 training loss: 0.024581756442785263\n",
      "Epoch 19 training loss: 0.024252163246273994\n",
      "Epoch 20 training loss: 0.023938970640301704\n",
      "Epoch 21 training loss: 0.0234003197401762\n",
      "Epoch 22 training loss: 0.023160770535469055\n",
      "Epoch 23 training loss: 0.022858845070004463\n",
      "Epoch 24 training loss: 0.02254897728562355\n",
      "Epoch 25 training loss: 0.02224528230726719\n",
      "Epoch 26 training loss: 0.022055665031075478\n",
      "Epoch 27 training loss: 0.021860601380467415\n",
      "Epoch 28 training loss: 0.021508967503905296\n",
      "Epoch 29 training loss: 0.02132820524275303\n",
      "Epoch 30 training loss: 0.021300602704286575\n",
      "Epoch 31 training loss: 0.020873505622148514\n",
      "Epoch 32 training loss: 0.02075488120317459\n",
      "Epoch 33 training loss: 0.020622408017516136\n",
      "Epoch 34 training loss: 0.020474329590797424\n",
      "Epoch 35 training loss: 0.020251190289855003\n",
      "Epoch 36 training loss: 0.02016971819102764\n",
      "Epoch 37 training loss: 0.020298535004258156\n",
      "Epoch 38 training loss: 0.019755329936742783\n",
      "Epoch 39 training loss: 0.019767414778470993\n",
      "Epoch 40 training loss: 0.019633740186691284\n",
      "Epoch 41 training loss: 0.0194877739995718\n",
      "Epoch 42 training loss: 0.01956487074494362\n",
      "Epoch 43 training loss: 0.019184475764632225\n",
      "Epoch 44 training loss: 0.019263993948698044\n",
      "Epoch 45 training loss: 0.01911487616598606\n",
      "Epoch 46 training loss: 0.019197430461645126\n",
      "Epoch 47 training loss: 0.01920635998249054\n",
      "Epoch 48 training loss: 0.018751351162791252\n",
      "Epoch 49 training loss: 0.01861399970948696\n",
      "Epoch 50 training loss: 0.018730437383055687\n",
      "Epoch 51 training loss: 0.0187191441655159\n",
      "Epoch 52 training loss: 0.01851801574230194\n",
      "Epoch 53 training loss: 0.01863197237253189\n",
      "Epoch 54 training loss: 0.018345829099416733\n",
      "Epoch 55 training loss: 0.018362069502472878\n",
      "Epoch 56 training loss: 0.018251044675707817\n",
      "Epoch 57 training loss: 0.01821674406528473\n",
      "Epoch 58 training loss: 0.018235333263874054\n",
      "Epoch 59 training loss: 0.018105700612068176\n",
      "Epoch 60 training loss: 0.018001500517129898\n",
      "Epoch 61 training loss: 0.01796252653002739\n",
      "Epoch 62 training loss: 0.018044082447886467\n",
      "Epoch 63 training loss: 0.017860349267721176\n",
      "Epoch 64 training loss: 0.01774909906089306\n",
      "Epoch 65 training loss: 0.017950033769011497\n",
      "Epoch 66 training loss: 0.017742974683642387\n",
      "Epoch 67 training loss: 0.017618278041481972\n",
      "Epoch 68 training loss: 0.017749140039086342\n",
      "Epoch 69 training loss: 0.017547868192195892\n",
      "Epoch 70 training loss: 0.017704833298921585\n",
      "Epoch 71 training loss: 0.017376607283949852\n",
      "Epoch 72 training loss: 0.01747717894613743\n",
      "Epoch 73 training loss: 0.01743854209780693\n",
      "Epoch 74 training loss: 0.017357533797621727\n",
      "Epoch 75 training loss: 0.0173368901014328\n",
      "Epoch 76 training loss: 0.017211036756634712\n",
      "Epoch 77 training loss: 0.0172377098351717\n",
      "Epoch 78 training loss: 0.017185280099511147\n",
      "Epoch 79 training loss: 0.017232786864042282\n",
      "Epoch 80 training loss: 0.01714184321463108\n",
      "Epoch 81 training loss: 0.016957063227891922\n",
      "Epoch 82 training loss: 0.016967248171567917\n",
      "Epoch 83 training loss: 0.016986513510346413\n",
      "Epoch 84 training loss: 0.016889620572328568\n",
      "Epoch 85 training loss: 0.016919441521167755\n",
      "Epoch 86 training loss: 0.016626089811325073\n",
      "Epoch 87 training loss: 0.01669413223862648\n",
      "Epoch 88 training loss: 0.016765782609581947\n",
      "Epoch 89 training loss: 0.01650291495025158\n",
      "Epoch 90 training loss: 0.016583943739533424\n",
      "Epoch 91 training loss: 0.016262969002127647\n",
      "Epoch 92 training loss: 0.016220485791563988\n",
      "Epoch 93 training loss: 0.016070257872343063\n",
      "Epoch 94 training loss: 0.01596401445567608\n",
      "Epoch 95 training loss: 0.015809426084160805\n",
      "Epoch 96 training loss: 0.015720020979642868\n",
      "Epoch 97 training loss: 0.015735849738121033\n",
      "Epoch 98 training loss: 0.015719745308160782\n",
      "Epoch 99 training loss: 0.015509548597037792\n",
      "Epoch 100 training loss: 0.015322119928896427\n",
      "Epoch 101 training loss: 0.01532778050750494\n",
      "Epoch 102 training loss: 0.015076628886163235\n",
      "Epoch 103 training loss: 0.0149458646774292\n",
      "Epoch 104 training loss: 0.014805941842496395\n",
      "Epoch 105 training loss: 0.0147250946611166\n",
      "Epoch 106 training loss: 0.014789283275604248\n",
      "Epoch 107 training loss: 0.014615237712860107\n",
      "Epoch 108 training loss: 0.014453504234552383\n",
      "Epoch 109 training loss: 0.014508918859064579\n",
      "Epoch 110 training loss: 0.014261085540056229\n",
      "Epoch 111 training loss: 0.01426205225288868\n",
      "Epoch 112 training loss: 0.014262132346630096\n",
      "Epoch 113 training loss: 0.014112000353634357\n",
      "Epoch 114 training loss: 0.014012403786182404\n",
      "Epoch 115 training loss: 0.01408609189093113\n",
      "Epoch 116 training loss: 0.013886873610317707\n",
      "Epoch 117 training loss: 0.01374044269323349\n",
      "Epoch 118 training loss: 0.01387003343552351\n",
      "Epoch 119 training loss: 0.013772225007414818\n",
      "Epoch 120 training loss: 0.013795035891234875\n",
      "Epoch 121 training loss: 0.013466940261423588\n",
      "Epoch 122 training loss: 0.013755028136074543\n",
      "Epoch 123 training loss: 0.013726615346968174\n",
      "Epoch 124 training loss: 0.0134730851277709\n",
      "Epoch 125 training loss: 0.013547927141189575\n",
      "Epoch 126 training loss: 0.013528102077543736\n",
      "Epoch 127 training loss: 0.013424591161310673\n",
      "Epoch 128 training loss: 0.01351673249155283\n",
      "Epoch 129 training loss: 0.013498596847057343\n",
      "Epoch 130 training loss: 0.013442899100482464\n",
      "Epoch 131 training loss: 0.013364133425056934\n",
      "Epoch 132 training loss: 0.013354748487472534\n",
      "Epoch 133 training loss: 0.013236195780336857\n",
      "Epoch 134 training loss: 0.013323521241545677\n",
      "Epoch 135 training loss: 0.013115270994603634\n",
      "Epoch 136 training loss: 0.013142894953489304\n",
      "Epoch 137 training loss: 0.013322302140295506\n",
      "Epoch 138 training loss: 0.013042558915913105\n",
      "Epoch 139 training loss: 0.013162856921553612\n",
      "Epoch 140 training loss: 0.012983868829905987\n",
      "Epoch 141 training loss: 0.013026939705014229\n",
      "Epoch 142 training loss: 0.01309625431895256\n",
      "Epoch 143 training loss: 0.013126621954143047\n",
      "Epoch 144 training loss: 0.012845205143094063\n",
      "Epoch 145 training loss: 0.012928196229040623\n",
      "Epoch 146 training loss: 0.012924396432936192\n",
      "Epoch 147 training loss: 0.012866094708442688\n",
      "Epoch 148 training loss: 0.012874449603259563\n",
      "Epoch 149 training loss: 0.012811403721570969\n",
      "Epoch 150 training loss: 0.012868626043200493\n",
      "Epoch 151 training loss: 0.012910211458802223\n",
      "Epoch 152 training loss: 0.012596533633768559\n",
      "Epoch 153 training loss: 0.012956635095179081\n",
      "Epoch 154 training loss: 0.012805708684027195\n",
      "Epoch 155 training loss: 0.012791200540959835\n",
      "Epoch 156 training loss: 0.012770901434123516\n",
      "Epoch 157 training loss: 0.012653087265789509\n",
      "Epoch 158 training loss: 0.01258409209549427\n",
      "Epoch 159 training loss: 0.012826205231249332\n",
      "Epoch 160 training loss: 0.01245005801320076\n",
      "Epoch 161 training loss: 0.01264494564384222\n",
      "Epoch 162 training loss: 0.012617778964340687\n",
      "Epoch 163 training loss: 0.012514456175267696\n",
      "Epoch 164 training loss: 0.012502624653279781\n",
      "Epoch 165 training loss: 0.012357398867607117\n",
      "Epoch 166 training loss: 0.012393324635922909\n",
      "Epoch 167 training loss: 0.012389593757689\n",
      "Epoch 168 training loss: 0.012367045506834984\n",
      "Epoch 169 training loss: 0.012400475330650806\n",
      "Epoch 170 training loss: 0.012233864516019821\n",
      "Epoch 171 training loss: 0.012260227464139462\n",
      "Epoch 172 training loss: 0.01239768136292696\n",
      "Epoch 173 training loss: 0.012634100392460823\n",
      "Epoch 174 training loss: 0.012349237687885761\n",
      "Epoch 175 training loss: 0.012171860784292221\n",
      "Epoch 176 training loss: 0.012198963202536106\n",
      "Epoch 177 training loss: 0.01197326835244894\n",
      "Epoch 178 training loss: 0.012044228613376617\n",
      "Epoch 179 training loss: 0.012370489537715912\n",
      "Epoch 180 training loss: 0.012172732502222061\n",
      "Epoch 181 training loss: 0.012168047949671745\n",
      "Epoch 182 training loss: 0.012094411067664623\n",
      "Epoch 183 training loss: 0.012061355635523796\n",
      "Epoch 184 training loss: 0.012200168333947659\n",
      "Epoch 185 training loss: 0.011829286813735962\n",
      "Epoch 186 training loss: 0.01208648830652237\n",
      "Epoch 187 training loss: 0.012231837958097458\n",
      "Epoch 188 training loss: 0.012299724854528904\n",
      "Epoch 189 training loss: 0.01174398697912693\n",
      "Epoch 190 training loss: 0.011918370611965656\n",
      "Epoch 191 training loss: 0.011898685246706009\n",
      "Epoch 192 training loss: 0.011908282525837421\n",
      "Epoch 193 training loss: 0.011898086406290531\n",
      "Epoch 194 training loss: 0.01197363343089819\n",
      "Epoch 195 training loss: 0.011655939742922783\n",
      "Epoch 196 training loss: 0.01202722080051899\n",
      "Epoch 197 training loss: 0.01174541749060154\n",
      "Epoch 198 training loss: 0.011885097250342369\n",
      "Epoch 199 training loss: 0.011889418587088585\n",
      "Epoch 200 training loss: 0.011561723425984383\n",
      "Epoch 201 training loss: 0.012064131908118725\n",
      "Epoch 202 training loss: 0.011695858091115952\n",
      "Epoch 203 training loss: 0.011639691889286041\n",
      "Epoch 204 training loss: 0.011742192320525646\n",
      "Epoch 205 training loss: 0.011739774607121944\n",
      "Epoch 206 training loss: 0.011679609306156635\n",
      "Epoch 207 training loss: 0.011578892357647419\n",
      "Epoch 208 training loss: 0.011658375151455402\n",
      "Epoch 209 training loss: 0.011785699985921383\n",
      "Epoch 210 training loss: 0.011810307390987873\n",
      "Epoch 211 training loss: 0.011679754592478275\n",
      "Epoch 212 training loss: 0.011694684624671936\n",
      "Epoch 213 training loss: 0.011764619499444962\n",
      "Epoch 214 training loss: 0.011479787528514862\n",
      "Epoch 215 training loss: 0.011653930880129337\n",
      "Epoch 216 training loss: 0.01200628187507391\n",
      "Epoch 217 training loss: 0.01153975073248148\n",
      "Epoch 218 training loss: 0.0112830251455307\n",
      "Epoch 219 training loss: 0.011388145387172699\n",
      "Epoch 220 training loss: 0.011851916089653969\n",
      "Epoch 221 training loss: 0.011348015628755093\n",
      "Epoch 222 training loss: 0.011732806451618671\n",
      "Epoch 223 training loss: 0.011590567417442799\n",
      "Epoch 224 training loss: 0.011516428552567959\n",
      "Epoch 225 training loss: 0.011593179777264595\n",
      "Epoch 226 training loss: 0.011507459916174412\n",
      "Epoch 227 training loss: 0.011482157744467258\n",
      "Epoch 228 training loss: 0.011658458039164543\n",
      "Epoch 229 training loss: 0.01136043295264244\n",
      "Epoch 230 training loss: 0.011310987174510956\n",
      "Epoch 231 training loss: 0.01154673844575882\n",
      "Epoch 232 training loss: 0.01141483522951603\n",
      "Epoch 233 training loss: 0.01157707441598177\n",
      "Epoch 234 training loss: 0.01124538667500019\n",
      "Epoch 235 training loss: 0.011882472783327103\n",
      "Epoch 236 training loss: 0.011054693721234798\n",
      "Epoch 237 training loss: 0.011390742845833302\n",
      "Epoch 238 training loss: 0.011319820769131184\n",
      "Epoch 239 training loss: 0.011219548992812634\n",
      "Epoch 240 training loss: 0.011469433084130287\n",
      "Epoch 241 training loss: 0.011268419213593006\n",
      "Epoch 242 training loss: 0.011506297625601292\n",
      "Epoch 243 training loss: 0.011308002285659313\n",
      "Epoch 244 training loss: 0.011324524879455566\n",
      "Epoch 245 training loss: 0.01146594900637865\n",
      "Epoch 246 training loss: 0.011356067843735218\n",
      "Epoch 247 training loss: 0.011158529669046402\n",
      "Epoch 248 training loss: 0.011280433274805546\n",
      "Epoch 249 training loss: 0.01135330367833376\n",
      "Epoch 250 training loss: 0.011020055040717125\n",
      "Epoch 251 training loss: 0.011329383589327335\n",
      "Epoch 252 training loss: 0.011040044017136097\n",
      "Epoch 253 training loss: 0.01134117878973484\n",
      "Epoch 254 training loss: 0.011233450844883919\n",
      "Epoch 255 training loss: 0.011138062924146652\n",
      "Epoch 256 training loss: 0.011081528849899769\n",
      "Epoch 257 training loss: 0.011081446893513203\n",
      "Epoch 258 training loss: 0.011301102116703987\n",
      "Epoch 259 training loss: 0.011142807081341743\n",
      "Epoch 260 training loss: 0.011223028413951397\n",
      "Epoch 261 training loss: 0.01106283813714981\n",
      "Epoch 262 training loss: 0.01103614829480648\n",
      "Epoch 263 training loss: 0.010996030643582344\n",
      "Epoch 264 training loss: 0.011049854569137096\n",
      "Epoch 265 training loss: 0.011121783405542374\n",
      "Epoch 266 training loss: 0.01113135740160942\n",
      "Epoch 267 training loss: 0.010935545898973942\n",
      "Epoch 268 training loss: 0.010910521261394024\n",
      "Epoch 269 training loss: 0.010923976078629494\n",
      "Epoch 270 training loss: 0.011444998905062675\n",
      "Epoch 271 training loss: 0.011054503731429577\n",
      "Epoch 272 training loss: 0.011215504258871078\n",
      "Epoch 273 training loss: 0.010934562422335148\n",
      "Epoch 274 training loss: 0.01084035262465477\n",
      "Epoch 275 training loss: 0.010942122898995876\n",
      "Epoch 276 training loss: 0.010909593664109707\n",
      "Epoch 277 training loss: 0.010754035785794258\n",
      "Epoch 278 training loss: 0.010961291380226612\n",
      "Epoch 279 training loss: 0.01077063474804163\n",
      "Epoch 280 training loss: 0.010956691578030586\n",
      "Epoch 281 training loss: 0.010772631503641605\n",
      "Epoch 282 training loss: 0.010638123378157616\n",
      "Epoch 283 training loss: 0.010591333732008934\n",
      "Epoch 284 training loss: 0.01067272387444973\n",
      "Epoch 285 training loss: 0.010974961332976818\n",
      "Epoch 286 training loss: 0.010780639946460724\n",
      "Epoch 287 training loss: 0.010791517794132233\n",
      "Epoch 288 training loss: 0.0105669517070055\n",
      "Epoch 289 training loss: 0.01073646079748869\n",
      "Epoch 290 training loss: 0.010761368088424206\n",
      "Epoch 291 training loss: 0.010698641650378704\n",
      "Epoch 292 training loss: 0.010801289230585098\n",
      "Epoch 293 training loss: 0.010589657351374626\n",
      "Epoch 294 training loss: 0.010684750974178314\n",
      "Epoch 295 training loss: 0.010689185932278633\n",
      "Epoch 296 training loss: 0.010466506704688072\n",
      "Epoch 297 training loss: 0.010655274614691734\n",
      "Epoch 298 training loss: 0.010381422005593777\n",
      "Epoch 299 training loss: 0.010593751445412636\n",
      "Epoch 300 training loss: 0.010622636415064335\n",
      "Epoch 301 training loss: 0.010421764105558395\n",
      "Epoch 302 training loss: 0.010420805774629116\n",
      "Epoch 303 training loss: 0.010652933269739151\n",
      "Epoch 304 training loss: 0.010435684584081173\n",
      "Epoch 305 training loss: 0.010253746062517166\n",
      "Epoch 306 training loss: 0.01019638404250145\n",
      "Epoch 307 training loss: 0.010276221670210361\n",
      "Epoch 308 training loss: 0.010261121205985546\n",
      "Epoch 309 training loss: 0.010358776897192001\n",
      "Epoch 310 training loss: 0.010143717750906944\n",
      "Epoch 311 training loss: 0.010404807515442371\n",
      "Epoch 312 training loss: 0.009961500763893127\n",
      "Epoch 313 training loss: 0.010207260958850384\n",
      "Epoch 314 training loss: 0.010304387658834457\n",
      "Epoch 315 training loss: 0.010022367350757122\n",
      "Epoch 316 training loss: 0.010237142443656921\n",
      "Epoch 317 training loss: 0.01025999803096056\n",
      "Epoch 318 training loss: 0.010450112633407116\n",
      "Epoch 319 training loss: 0.009985913522541523\n",
      "Epoch 320 training loss: 0.010069666430354118\n",
      "Epoch 321 training loss: 0.009971365332603455\n",
      "Epoch 322 training loss: 0.009976687841117382\n",
      "Epoch 323 training loss: 0.010095523670315742\n",
      "Epoch 324 training loss: 0.009875748306512833\n",
      "Epoch 325 training loss: 0.010242658667266369\n",
      "Epoch 326 training loss: 0.010075428523123264\n",
      "Epoch 327 training loss: 0.010135315358638763\n",
      "Epoch 328 training loss: 0.009928209707140923\n",
      "Epoch 329 training loss: 0.009937942959368229\n",
      "Epoch 330 training loss: 0.009918405674397945\n",
      "Epoch 331 training loss: 0.01000137533992529\n",
      "Epoch 332 training loss: 0.010177052579820156\n",
      "Epoch 333 training loss: 0.009866567328572273\n",
      "Epoch 334 training loss: 0.010059470310807228\n",
      "Epoch 335 training loss: 0.010027477517724037\n",
      "Epoch 336 training loss: 0.009678058326244354\n",
      "Epoch 337 training loss: 0.010004648938775063\n",
      "Epoch 338 training loss: 0.010023151524364948\n",
      "Epoch 339 training loss: 0.009657816030085087\n",
      "Epoch 340 training loss: 0.01002924982458353\n",
      "Epoch 341 training loss: 0.009686440229415894\n",
      "Epoch 342 training loss: 0.009809969924390316\n",
      "Epoch 343 training loss: 0.010121417231857777\n",
      "Epoch 344 training loss: 0.0098272655159235\n",
      "Epoch 345 training loss: 0.009696667082607746\n",
      "Epoch 346 training loss: 0.010033882223069668\n",
      "Epoch 347 training loss: 0.009900040924549103\n",
      "Epoch 348 training loss: 0.009824531152844429\n",
      "Epoch 349 training loss: 0.009669742546975613\n",
      "Epoch 350 training loss: 0.00971104484051466\n",
      "Epoch 351 training loss: 0.009999791160225868\n",
      "Epoch 352 training loss: 0.00955488346517086\n",
      "Epoch 353 training loss: 0.009670785628259182\n",
      "Epoch 354 training loss: 0.009819824248552322\n",
      "Epoch 355 training loss: 0.00966666266322136\n",
      "Epoch 356 training loss: 0.009682944975793362\n",
      "Epoch 357 training loss: 0.009882813319563866\n",
      "Epoch 358 training loss: 0.0096292644739151\n",
      "Epoch 359 training loss: 0.009934237226843834\n",
      "Epoch 360 training loss: 0.00956020224839449\n",
      "Epoch 361 training loss: 0.00965520367026329\n",
      "Epoch 362 training loss: 0.009952129796147346\n",
      "Epoch 363 training loss: 0.009937339462339878\n",
      "Epoch 364 training loss: 0.009598472155630589\n",
      "Epoch 365 training loss: 0.009525857865810394\n",
      "Epoch 366 training loss: 0.009858150035142899\n",
      "Epoch 367 training loss: 0.009870336391031742\n",
      "Epoch 368 training loss: 0.009424243122339249\n",
      "Epoch 369 training loss: 0.009602248668670654\n",
      "Epoch 370 training loss: 0.00990298017859459\n",
      "Epoch 371 training loss: 0.009403186850249767\n",
      "Epoch 372 training loss: 0.00973527505993843\n",
      "Epoch 373 training loss: 0.009549005888402462\n",
      "Epoch 374 training loss: 0.009692604653537273\n",
      "Epoch 375 training loss: 0.009650996886193752\n",
      "Epoch 376 training loss: 0.009488692507147789\n",
      "Epoch 377 training loss: 0.009469796903431416\n",
      "Epoch 378 training loss: 0.009694532491266727\n",
      "Epoch 379 training loss: 0.009678853675723076\n",
      "Epoch 380 training loss: 0.009566822089254856\n",
      "Epoch 381 training loss: 0.009565792046487331\n",
      "Epoch 382 training loss: 0.009568976238369942\n",
      "Epoch 383 training loss: 0.009527474641799927\n",
      "Epoch 384 training loss: 0.01006406731903553\n",
      "Epoch 385 training loss: 0.009586887434124947\n",
      "Epoch 386 training loss: 0.009327422827482224\n",
      "Epoch 387 training loss: 0.009741404093801975\n",
      "Epoch 388 training loss: 0.00947154313325882\n",
      "Epoch 389 training loss: 0.00936540961265564\n",
      "Epoch 390 training loss: 0.009350317530333996\n",
      "Epoch 391 training loss: 0.009850761853158474\n",
      "Epoch 392 training loss: 0.009269769303500652\n",
      "Epoch 393 training loss: 0.009540095925331116\n",
      "Epoch 394 training loss: 0.009438412263989449\n",
      "Epoch 395 training loss: 0.009274279698729515\n",
      "Epoch 396 training loss: 0.009731369093060493\n",
      "Epoch 397 training loss: 0.009290158748626709\n",
      "Epoch 398 training loss: 0.00943857990205288\n",
      "Epoch 399 training loss: 0.00940011814236641\n",
      "Epoch 400 training loss: 0.009673196822404861\n",
      "Epoch 401 training loss: 0.009381996467709541\n",
      "Epoch 402 training loss: 0.009437784552574158\n",
      "Epoch 403 training loss: 0.00949112419039011\n",
      "Epoch 404 training loss: 0.009520154446363449\n",
      "Epoch 405 training loss: 0.009454549290239811\n",
      "Epoch 406 training loss: 0.009419705718755722\n",
      "Epoch 407 training loss: 0.009524883702397346\n",
      "Epoch 408 training loss: 0.009538916870951653\n",
      "Epoch 409 training loss: 0.009261060506105423\n",
      "Epoch 410 training loss: 0.009212708100676537\n",
      "Epoch 411 training loss: 0.009389158338308334\n",
      "Epoch 412 training loss: 0.009568961337208748\n",
      "Epoch 413 training loss: 0.009404712356626987\n",
      "Epoch 414 training loss: 0.009479761123657227\n",
      "Epoch 415 training loss: 0.009104921482503414\n",
      "Epoch 416 training loss: 0.009668322280049324\n",
      "Epoch 417 training loss: 0.00929603073745966\n",
      "Epoch 418 training loss: 0.009567617438733578\n",
      "Epoch 419 training loss: 0.009306446649134159\n",
      "Epoch 420 training loss: 0.009673557244241238\n",
      "Epoch 421 training loss: 0.009112948551774025\n",
      "Epoch 422 training loss: 0.009274529293179512\n",
      "Epoch 423 training loss: 0.009748215787112713\n",
      "Epoch 424 training loss: 0.009228074923157692\n",
      "Epoch 425 training loss: 0.00903441570699215\n",
      "Epoch 426 training loss: 0.009638877585530281\n",
      "Epoch 427 training loss: 0.009023194201290607\n",
      "Epoch 428 training loss: 0.009368078783154488\n",
      "Epoch 429 training loss: 0.00941532850265503\n",
      "Epoch 430 training loss: 0.009169190190732479\n",
      "Epoch 431 training loss: 0.009141091257333755\n",
      "Epoch 432 training loss: 0.009646044112741947\n",
      "Epoch 433 training loss: 0.009081192314624786\n",
      "Epoch 434 training loss: 0.009540260769426823\n",
      "Epoch 435 training loss: 0.009164859540760517\n",
      "Epoch 436 training loss: 0.009486817754805088\n",
      "Epoch 437 training loss: 0.009112107567489147\n",
      "Epoch 438 training loss: 0.009274814277887344\n",
      "Epoch 439 training loss: 0.009296133182942867\n",
      "Epoch 440 training loss: 0.009168080985546112\n",
      "Epoch 441 training loss: 0.009260637685656548\n",
      "Epoch 442 training loss: 0.009262514300644398\n",
      "Epoch 443 training loss: 0.009085562080144882\n",
      "Epoch 444 training loss: 0.009202882647514343\n",
      "Epoch 445 training loss: 0.009413398802280426\n",
      "Epoch 446 training loss: 0.009133348241448402\n",
      "Epoch 447 training loss: 0.009137040004134178\n",
      "Epoch 448 training loss: 0.009242397733032703\n",
      "Epoch 449 training loss: 0.009239732287824154\n",
      "Epoch 450 training loss: 0.009371457621455193\n",
      "Epoch 451 training loss: 0.00934488046914339\n",
      "Epoch 452 training loss: 0.009194781072437763\n",
      "Epoch 453 training loss: 0.009371655993163586\n",
      "Epoch 454 training loss: 0.009027007035911083\n",
      "Epoch 455 training loss: 0.009318487718701363\n",
      "Epoch 456 training loss: 0.009240019135177135\n",
      "Epoch 457 training loss: 0.009474236518144608\n",
      "Epoch 458 training loss: 0.009028620086610317\n",
      "Epoch 459 training loss: 0.009033833630383015\n",
      "Epoch 460 training loss: 0.008953500539064407\n",
      "Epoch 461 training loss: 0.009265515953302383\n",
      "Epoch 462 training loss: 0.009097679518163204\n",
      "Epoch 463 training loss: 0.009329671040177345\n",
      "Epoch 464 training loss: 0.009469401091337204\n",
      "Epoch 465 training loss: 0.008978722617030144\n",
      "Epoch 466 training loss: 0.009172405116260052\n",
      "Epoch 467 training loss: 0.00906308926641941\n",
      "Epoch 468 training loss: 0.00920933485031128\n",
      "Epoch 469 training loss: 0.009196162223815918\n",
      "Epoch 470 training loss: 0.00904603861272335\n",
      "Epoch 471 training loss: 0.009224949404597282\n",
      "Epoch 472 training loss: 0.00923863798379898\n",
      "Epoch 473 training loss: 0.009064044803380966\n",
      "Epoch 474 training loss: 0.00897980760782957\n",
      "Epoch 475 training loss: 0.009276341646909714\n",
      "Epoch 476 training loss: 0.009001745842397213\n",
      "Epoch 477 training loss: 0.008977817371487617\n",
      "Epoch 478 training loss: 0.009038344956934452\n",
      "Epoch 479 training loss: 0.00916667003184557\n",
      "Epoch 480 training loss: 0.00911634974181652\n",
      "Epoch 481 training loss: 0.00910236407071352\n",
      "Epoch 482 training loss: 0.009248162619769573\n",
      "Epoch 483 training loss: 0.009188884869217873\n",
      "Epoch 484 training loss: 0.009109900332987309\n",
      "Epoch 485 training loss: 0.008822126314043999\n",
      "Epoch 486 training loss: 0.008925259113311768\n",
      "Epoch 487 training loss: 0.009184682741761208\n",
      "Epoch 488 training loss: 0.008965014480054379\n",
      "Epoch 489 training loss: 0.008863549679517746\n",
      "Epoch 490 training loss: 0.00938031729310751\n",
      "Epoch 491 training loss: 0.008836635388433933\n",
      "Epoch 492 training loss: 0.009050318971276283\n",
      "Epoch 493 training loss: 0.009250686503946781\n",
      "Epoch 494 training loss: 0.008893124759197235\n",
      "Epoch 495 training loss: 0.008961492218077183\n",
      "Epoch 496 training loss: 0.008829416707158089\n",
      "Epoch 497 training loss: 0.009320725686848164\n",
      "Epoch 498 training loss: 0.008952563628554344\n",
      "Epoch 499 training loss: 0.008839874528348446\n",
      "Epoch 500 training loss: 0.009156439453363419\n",
      "Epoch 501 training loss: 0.008898827247321606\n",
      "Epoch 502 training loss: 0.008777234703302383\n",
      "Epoch 503 training loss: 0.008794979192316532\n",
      "Epoch 504 training loss: 0.008908894844353199\n",
      "Epoch 505 training loss: 0.008988459594547749\n",
      "Epoch 506 training loss: 0.008977235294878483\n",
      "Epoch 507 training loss: 0.00879860669374466\n",
      "Epoch 508 training loss: 0.008927769958972931\n",
      "Epoch 509 training loss: 0.008836448192596436\n",
      "Epoch 510 training loss: 0.009097130037844181\n",
      "Epoch 511 training loss: 0.00904318131506443\n",
      "Epoch 512 training loss: 0.00870087742805481\n",
      "Epoch 513 training loss: 0.008634395897388458\n",
      "Epoch 514 training loss: 0.008848683908581734\n",
      "Epoch 515 training loss: 0.009062153287231922\n",
      "Epoch 516 training loss: 0.008889962919056416\n",
      "Epoch 517 training loss: 0.008752786554396152\n",
      "Epoch 518 training loss: 0.008854387328028679\n",
      "Epoch 519 training loss: 0.008867713622748852\n",
      "Epoch 520 training loss: 0.008795225992798805\n",
      "Epoch 521 training loss: 0.00894156564027071\n",
      "Epoch 522 training loss: 0.00883578322827816\n",
      "Epoch 523 training loss: 0.008985261432826519\n",
      "Epoch 524 training loss: 0.008717541582882404\n",
      "Epoch 525 training loss: 0.008579688146710396\n",
      "Epoch 526 training loss: 0.008715174160897732\n",
      "Epoch 527 training loss: 0.008909651078283787\n",
      "Epoch 528 training loss: 0.008900923654437065\n",
      "Epoch 529 training loss: 0.008715376257896423\n",
      "Epoch 530 training loss: 0.008561994880437851\n",
      "Epoch 531 training loss: 0.008716908283531666\n",
      "Epoch 532 training loss: 0.00891061034053564\n",
      "Epoch 533 training loss: 0.008749389089643955\n",
      "Epoch 534 training loss: 0.008957895450294018\n",
      "Epoch 535 training loss: 0.00877994392067194\n",
      "Epoch 536 training loss: 0.008759369142353535\n",
      "Epoch 537 training loss: 0.008641195483505726\n",
      "Epoch 538 training loss: 0.008670295588672161\n",
      "Epoch 539 training loss: 0.008944286033511162\n",
      "Epoch 540 training loss: 0.008929719217121601\n",
      "Epoch 541 training loss: 0.00873473659157753\n",
      "Epoch 542 training loss: 0.008933340199291706\n",
      "Epoch 543 training loss: 0.008694100193679333\n",
      "Epoch 544 training loss: 0.008475381880998611\n",
      "Epoch 545 training loss: 0.008677411824464798\n",
      "Epoch 546 training loss: 0.008898697793483734\n",
      "Epoch 547 training loss: 0.00868223886936903\n",
      "Epoch 548 training loss: 0.008795568719506264\n",
      "Epoch 549 training loss: 0.00891898013651371\n",
      "Epoch 550 training loss: 0.008634427562355995\n",
      "Epoch 551 training loss: 0.008677683770656586\n",
      "Epoch 552 training loss: 0.008790411055088043\n",
      "Epoch 553 training loss: 0.008700699545443058\n",
      "Epoch 554 training loss: 0.009051620028913021\n",
      "Epoch 555 training loss: 0.008593450300395489\n",
      "Epoch 556 training loss: 0.008639809675514698\n",
      "Epoch 557 training loss: 0.008811318315565586\n",
      "Epoch 558 training loss: 0.008699143305420876\n",
      "Epoch 559 training loss: 0.008663637563586235\n",
      "Epoch 560 training loss: 0.008647129870951176\n",
      "Epoch 561 training loss: 0.008563620038330555\n",
      "Epoch 562 training loss: 0.008810639381408691\n",
      "Epoch 563 training loss: 0.008913333527743816\n",
      "Epoch 564 training loss: 0.008843692019581795\n",
      "Epoch 565 training loss: 0.008675199933350086\n",
      "Epoch 566 training loss: 0.008512914180755615\n",
      "Epoch 567 training loss: 0.008758391253650188\n",
      "Epoch 568 training loss: 0.008729410357773304\n",
      "Epoch 569 training loss: 0.008661090396344662\n",
      "Epoch 570 training loss: 0.008738640695810318\n",
      "Epoch 571 training loss: 0.008451426401734352\n",
      "Epoch 572 training loss: 0.008696211501955986\n",
      "Epoch 573 training loss: 0.008737375028431416\n",
      "Epoch 574 training loss: 0.008923017419874668\n",
      "Epoch 575 training loss: 0.008602838031947613\n",
      "Epoch 576 training loss: 0.008652178570628166\n",
      "Epoch 577 training loss: 0.008894597180187702\n",
      "Epoch 578 training loss: 0.008335504680871964\n",
      "Epoch 579 training loss: 0.008541040122509003\n",
      "Epoch 580 training loss: 0.00883355550467968\n",
      "Epoch 581 training loss: 0.0084293894469738\n",
      "Epoch 582 training loss: 0.00897175632417202\n",
      "Epoch 583 training loss: 0.00840186607092619\n",
      "Epoch 584 training loss: 0.008730503730475903\n",
      "Epoch 585 training loss: 0.008630086667835712\n",
      "Epoch 586 training loss: 0.008576112799346447\n",
      "Epoch 587 training loss: 0.00872646551579237\n",
      "Epoch 588 training loss: 0.008646802045404911\n",
      "Epoch 589 training loss: 0.008578276261687279\n",
      "Epoch 590 training loss: 0.008513168431818485\n",
      "Epoch 591 training loss: 0.008587310090661049\n",
      "Epoch 592 training loss: 0.008604536764323711\n",
      "Epoch 593 training loss: 0.00893746130168438\n",
      "Epoch 594 training loss: 0.008510453626513481\n",
      "Epoch 595 training loss: 0.008675678633153439\n",
      "Epoch 596 training loss: 0.008451935835182667\n",
      "Epoch 597 training loss: 0.008521700277924538\n",
      "Epoch 598 training loss: 0.009011649526655674\n",
      "Epoch 599 training loss: 0.008504745550453663\n",
      "Epoch 600 training loss: 0.008573257364332676\n",
      "Epoch 601 training loss: 0.008619491942226887\n",
      "Epoch 602 training loss: 0.008843472227454185\n",
      "Epoch 603 training loss: 0.008994358591735363\n",
      "Epoch 604 training loss: 0.008469993248581886\n",
      "Epoch 605 training loss: 0.008445791900157928\n",
      "Epoch 606 training loss: 0.008532444015145302\n",
      "Epoch 607 training loss: 0.008463999256491661\n",
      "Epoch 608 training loss: 0.0086775878444314\n",
      "Epoch 609 training loss: 0.00856398232281208\n",
      "Epoch 610 training loss: 0.008671989664435387\n",
      "Epoch 611 training loss: 0.008435714058578014\n",
      "Epoch 612 training loss: 0.008434413000941277\n",
      "Epoch 613 training loss: 0.008898388594388962\n",
      "Epoch 614 training loss: 0.008828600868582726\n",
      "Epoch 615 training loss: 0.008280147798359394\n",
      "Epoch 616 training loss: 0.00852435827255249\n",
      "Epoch 617 training loss: 0.00848072487860918\n",
      "Epoch 618 training loss: 0.008488787338137627\n",
      "Epoch 619 training loss: 0.00874719861894846\n",
      "Epoch 620 training loss: 0.00855766050517559\n",
      "Epoch 621 training loss: 0.008398032747209072\n",
      "Epoch 622 training loss: 0.008556804619729519\n",
      "Epoch 623 training loss: 0.008449752815067768\n",
      "Epoch 624 training loss: 0.008492459543049335\n",
      "Epoch 625 training loss: 0.008899562060832977\n",
      "Epoch 626 training loss: 0.008824048563838005\n",
      "Epoch 627 training loss: 0.00858842022716999\n",
      "Epoch 628 training loss: 0.008401916362345219\n",
      "Epoch 629 training loss: 0.008345392532646656\n",
      "Epoch 630 training loss: 0.00889828335493803\n",
      "Epoch 631 training loss: 0.008557728491723537\n",
      "Epoch 632 training loss: 0.008350343443453312\n",
      "Epoch 633 training loss: 0.00859352108091116\n",
      "Epoch 634 training loss: 0.008457453921437263\n",
      "Epoch 635 training loss: 0.008814291097223759\n",
      "Epoch 636 training loss: 0.008441533893346786\n",
      "Epoch 637 training loss: 0.008450169116258621\n",
      "Epoch 638 training loss: 0.008427351713180542\n",
      "Epoch 639 training loss: 0.0085174310952425\n",
      "Epoch 640 training loss: 0.008523254655301571\n",
      "Epoch 641 training loss: 0.008497603237628937\n",
      "Epoch 642 training loss: 0.008581779897212982\n",
      "Epoch 643 training loss: 0.008732667192816734\n",
      "Epoch 644 training loss: 0.008699421770870686\n",
      "Epoch 645 training loss: 0.008363373577594757\n",
      "Epoch 646 training loss: 0.008329274132847786\n",
      "Epoch 647 training loss: 0.008902073837816715\n",
      "Epoch 648 training loss: 0.00874546729028225\n",
      "Epoch 649 training loss: 0.008514892309904099\n",
      "Epoch 650 training loss: 0.008297733031213284\n",
      "Epoch 651 training loss: 0.0088099529966712\n",
      "Epoch 652 training loss: 0.008288092911243439\n",
      "Epoch 653 training loss: 0.008332960307598114\n",
      "Epoch 654 training loss: 0.008592951111495495\n",
      "Epoch 655 training loss: 0.00888681598007679\n",
      "Epoch 656 training loss: 0.008335870690643787\n",
      "Epoch 657 training loss: 0.008519683964550495\n",
      "Epoch 658 training loss: 0.008554061874747276\n",
      "Epoch 659 training loss: 0.008631521835923195\n",
      "Epoch 660 training loss: 0.008430644869804382\n",
      "Epoch 661 training loss: 0.00824875570833683\n",
      "Epoch 662 training loss: 0.008599348366260529\n",
      "Epoch 663 training loss: 0.008515299297869205\n",
      "Epoch 664 training loss: 0.008835644461214542\n",
      "Epoch 665 training loss: 0.008942041546106339\n",
      "Epoch 666 training loss: 0.008754200302064419\n",
      "Epoch 667 training loss: 0.008416247554123402\n",
      "Epoch 668 training loss: 0.008281665854156017\n",
      "Epoch 669 training loss: 0.008340160362422466\n",
      "Epoch 670 training loss: 0.008376482874155045\n",
      "Epoch 671 training loss: 0.008862965740263462\n",
      "Epoch 672 training loss: 0.008259245194494724\n",
      "Epoch 673 training loss: 0.008938350714743137\n",
      "Epoch 674 training loss: 0.00822127889841795\n",
      "Epoch 675 training loss: 0.008435368537902832\n",
      "Epoch 676 training loss: 0.008347258903086185\n",
      "Epoch 677 training loss: 0.008762971498072147\n",
      "Epoch 678 training loss: 0.008370406925678253\n",
      "Epoch 679 training loss: 0.008297846652567387\n",
      "Epoch 680 training loss: 0.008381886407732964\n",
      "Epoch 681 training loss: 0.008609370328485966\n",
      "Epoch 682 training loss: 0.008612595498561859\n",
      "Epoch 683 training loss: 0.008434093557298183\n",
      "Epoch 684 training loss: 0.008489913307130337\n",
      "Epoch 685 training loss: 0.008396521210670471\n",
      "Epoch 686 training loss: 0.008247874677181244\n",
      "Epoch 687 training loss: 0.008861501701176167\n",
      "Epoch 688 training loss: 0.008430159650743008\n",
      "Epoch 689 training loss: 0.008324438706040382\n",
      "Epoch 690 training loss: 0.008738507516682148\n",
      "Epoch 691 training loss: 0.00825980119407177\n",
      "Epoch 692 training loss: 0.008512438274919987\n",
      "Epoch 693 training loss: 0.008478742092847824\n",
      "Epoch 694 training loss: 0.008423840627074242\n",
      "Epoch 695 training loss: 0.008339869789779186\n",
      "Epoch 696 training loss: 0.00839537288993597\n",
      "Epoch 697 training loss: 0.008555484935641289\n",
      "Epoch 698 training loss: 0.008531409315764904\n",
      "Epoch 699 training loss: 0.00833289884030819\n",
      "Epoch 700 training loss: 0.008344187401235104\n",
      "Epoch 701 training loss: 0.008707464672625065\n",
      "Epoch 702 training loss: 0.008305457420647144\n",
      "Epoch 703 training loss: 0.008581327274441719\n",
      "Epoch 704 training loss: 0.008271670900285244\n",
      "Epoch 705 training loss: 0.008621017448604107\n",
      "Epoch 706 training loss: 0.008348560892045498\n",
      "Epoch 707 training loss: 0.008338822983205318\n",
      "Epoch 708 training loss: 0.008211687207221985\n",
      "Epoch 709 training loss: 0.008209341205656528\n",
      "Epoch 710 training loss: 0.00874833669513464\n",
      "Epoch 711 training loss: 0.008205406367778778\n",
      "Epoch 712 training loss: 0.008615368977189064\n",
      "Epoch 713 training loss: 0.008283472619950771\n",
      "Epoch 714 training loss: 0.008651024661958218\n",
      "Epoch 715 training loss: 0.008192772045731544\n",
      "Epoch 716 training loss: 0.008076432161033154\n",
      "Epoch 717 training loss: 0.00862258579581976\n",
      "Epoch 718 training loss: 0.008246304467320442\n",
      "Epoch 719 training loss: 0.008493509143590927\n",
      "Epoch 720 training loss: 0.008234602399170399\n",
      "Epoch 721 training loss: 0.008709704503417015\n",
      "Epoch 722 training loss: 0.00837749894708395\n",
      "Epoch 723 training loss: 0.008326312527060509\n",
      "Epoch 724 training loss: 0.008404752239584923\n",
      "Epoch 725 training loss: 0.008679263293743134\n",
      "Epoch 726 training loss: 0.008234835229814053\n",
      "Epoch 727 training loss: 0.008328838273882866\n",
      "Epoch 728 training loss: 0.008505468256771564\n",
      "Epoch 729 training loss: 0.008255240507423878\n",
      "Epoch 730 training loss: 0.00834964495152235\n",
      "Epoch 731 training loss: 0.008417591452598572\n",
      "Epoch 732 training loss: 0.008446235209703445\n",
      "Epoch 733 training loss: 0.00825412105768919\n",
      "Epoch 734 training loss: 0.008568170480430126\n",
      "Epoch 735 training loss: 0.008329366333782673\n",
      "Epoch 736 training loss: 0.008233094587922096\n",
      "Epoch 737 training loss: 0.008374525234103203\n",
      "Epoch 738 training loss: 0.008557328954339027\n",
      "Epoch 739 training loss: 0.008348801173269749\n",
      "Epoch 740 training loss: 0.008513232693076134\n",
      "Epoch 741 training loss: 0.008199235424399376\n",
      "Epoch 742 training loss: 0.008383841253817081\n",
      "Epoch 743 training loss: 0.008428363129496574\n",
      "Epoch 744 training loss: 0.008375494740903378\n",
      "Epoch 745 training loss: 0.008556438609957695\n",
      "Epoch 746 training loss: 0.008249896578490734\n",
      "Epoch 747 training loss: 0.008369605988264084\n",
      "Epoch 748 training loss: 0.00842728465795517\n",
      "Epoch 749 training loss: 0.008390555158257484\n",
      "Epoch 750 training loss: 0.00822382140904665\n",
      "Epoch 751 training loss: 0.008525162935256958\n",
      "Epoch 752 training loss: 0.008210500702261925\n",
      "Epoch 753 training loss: 0.008203105069696903\n",
      "Epoch 754 training loss: 0.008248377591371536\n",
      "Epoch 755 training loss: 0.008400524035096169\n",
      "Epoch 756 training loss: 0.008361446671187878\n",
      "Epoch 757 training loss: 0.008429838344454765\n",
      "Epoch 758 training loss: 0.008207744918763638\n",
      "Epoch 759 training loss: 0.008533376269042492\n",
      "Epoch 760 training loss: 0.00832377839833498\n",
      "Epoch 761 training loss: 0.008343493565917015\n",
      "Epoch 762 training loss: 0.008451957255601883\n",
      "Epoch 763 training loss: 0.008307973854243755\n",
      "Epoch 764 training loss: 0.008073871023952961\n",
      "Epoch 765 training loss: 0.008419377729296684\n",
      "Epoch 766 training loss: 0.00831508170813322\n",
      "Epoch 767 training loss: 0.008404633030295372\n",
      "Epoch 768 training loss: 0.008276611566543579\n",
      "Epoch 769 training loss: 0.008289454504847527\n",
      "Epoch 770 training loss: 0.00835469551384449\n",
      "Epoch 771 training loss: 0.008271627128124237\n",
      "Epoch 772 training loss: 0.008123443461954594\n",
      "Epoch 773 training loss: 0.008311482146382332\n",
      "Epoch 774 training loss: 0.008332481607794762\n",
      "Epoch 775 training loss: 0.00850042887032032\n",
      "Epoch 776 training loss: 0.008141395635902882\n",
      "Epoch 777 training loss: 0.00825379230082035\n",
      "Epoch 778 training loss: 0.00854759942740202\n",
      "Epoch 779 training loss: 0.008038002997636795\n",
      "Epoch 780 training loss: 0.00849326141178608\n",
      "Epoch 781 training loss: 0.008361494168639183\n",
      "Epoch 782 training loss: 0.008202245458960533\n",
      "Epoch 783 training loss: 0.008329632692039013\n",
      "Epoch 784 training loss: 0.0082162544131279\n",
      "Epoch 785 training loss: 0.008536974899470806\n",
      "Epoch 786 training loss: 0.008297205902636051\n",
      "Epoch 787 training loss: 0.008304029703140259\n",
      "Epoch 788 training loss: 0.008089307695627213\n",
      "Epoch 789 training loss: 0.008323786780238152\n",
      "Epoch 790 training loss: 0.008280452340841293\n",
      "Epoch 791 training loss: 0.008332964964210987\n",
      "Epoch 792 training loss: 0.008190670982003212\n",
      "Epoch 793 training loss: 0.00806380808353424\n",
      "Epoch 794 training loss: 0.00832232553511858\n",
      "Epoch 795 training loss: 0.008089407347142696\n",
      "Epoch 796 training loss: 0.00834847241640091\n",
      "Epoch 797 training loss: 0.008181707002222538\n",
      "Epoch 798 training loss: 0.008219531737267971\n",
      "Epoch 799 training loss: 0.008350132964551449\n",
      "Epoch 800 training loss: 0.008372977375984192\n",
      "Epoch 801 training loss: 0.00838476326316595\n",
      "Epoch 802 training loss: 0.008106423541903496\n",
      "Epoch 803 training loss: 0.008327372372150421\n",
      "Epoch 804 training loss: 0.008255809545516968\n",
      "Epoch 805 training loss: 0.008268911391496658\n",
      "Epoch 806 training loss: 0.008530613034963608\n",
      "Epoch 807 training loss: 0.00815817341208458\n",
      "Epoch 808 training loss: 0.008048356510698795\n",
      "Epoch 809 training loss: 0.008414116688072681\n",
      "Epoch 810 training loss: 0.008171522058546543\n",
      "Epoch 811 training loss: 0.008117974735796452\n",
      "Epoch 812 training loss: 0.0083633316680789\n",
      "Epoch 813 training loss: 0.008549927733838558\n",
      "Epoch 814 training loss: 0.008096208795905113\n",
      "Epoch 815 training loss: 0.008069025352597237\n",
      "Epoch 816 training loss: 0.008234530687332153\n",
      "Epoch 817 training loss: 0.008362926542758942\n",
      "Epoch 818 training loss: 0.0082937590777874\n",
      "Epoch 819 training loss: 0.00838085263967514\n",
      "Epoch 820 training loss: 0.007946819998323917\n",
      "Epoch 821 training loss: 0.008629276417195797\n",
      "Epoch 822 training loss: 0.008136161603033543\n",
      "Epoch 823 training loss: 0.008451404981315136\n",
      "Epoch 824 training loss: 0.008016875945031643\n",
      "Epoch 825 training loss: 0.008135020732879639\n",
      "Epoch 826 training loss: 0.008340690284967422\n",
      "Epoch 827 training loss: 0.008241104893386364\n",
      "Epoch 828 training loss: 0.00804860983043909\n",
      "Epoch 829 training loss: 0.008165804669260979\n",
      "Epoch 830 training loss: 0.008503119461238384\n",
      "Epoch 831 training loss: 0.008041801862418652\n",
      "Epoch 832 training loss: 0.008150472305715084\n",
      "Epoch 833 training loss: 0.008238273672759533\n",
      "Epoch 834 training loss: 0.00842207483947277\n",
      "Epoch 835 training loss: 0.00794163066893816\n",
      "Epoch 836 training loss: 0.007966366596519947\n",
      "Epoch 837 training loss: 0.008485333994030952\n",
      "Epoch 838 training loss: 0.008179837837815285\n",
      "Epoch 839 training loss: 0.008123173378407955\n",
      "Epoch 840 training loss: 0.008738611824810505\n",
      "Epoch 841 training loss: 0.00811455026268959\n",
      "Epoch 842 training loss: 0.008233245462179184\n",
      "Epoch 843 training loss: 0.008221287280321121\n",
      "Epoch 844 training loss: 0.008015399798750877\n",
      "Epoch 845 training loss: 0.00825290847569704\n",
      "Epoch 846 training loss: 0.008078740909695625\n",
      "Epoch 847 training loss: 0.00810407567769289\n",
      "Epoch 848 training loss: 0.008353499695658684\n",
      "Epoch 849 training loss: 0.007969272322952747\n",
      "Epoch 850 training loss: 0.008077025413513184\n",
      "Epoch 851 training loss: 0.008188758045434952\n",
      "Epoch 852 training loss: 0.008319087326526642\n",
      "Epoch 853 training loss: 0.008403852581977844\n",
      "Epoch 854 training loss: 0.008066114038228989\n",
      "Epoch 855 training loss: 0.00811253022402525\n",
      "Epoch 856 training loss: 0.008299460634589195\n",
      "Epoch 857 training loss: 0.008071167394518852\n",
      "Epoch 858 training loss: 0.008334613405168056\n",
      "Epoch 859 training loss: 0.008091285824775696\n",
      "Epoch 860 training loss: 0.008197973482310772\n",
      "Epoch 861 training loss: 0.008229468017816544\n",
      "Epoch 862 training loss: 0.008094945922493935\n",
      "Epoch 863 training loss: 0.007987619377672672\n",
      "Epoch 864 training loss: 0.007987672463059425\n",
      "Epoch 865 training loss: 0.00832451693713665\n",
      "Epoch 866 training loss: 0.008015799336135387\n",
      "Epoch 867 training loss: 0.008071036078035831\n",
      "Epoch 868 training loss: 0.008461899124085903\n",
      "Epoch 869 training loss: 0.007908281870186329\n",
      "Epoch 870 training loss: 0.008162085898220539\n",
      "Epoch 871 training loss: 0.0081868851557374\n",
      "Epoch 872 training loss: 0.007957461290061474\n",
      "Epoch 873 training loss: 0.008269550278782845\n",
      "Epoch 874 training loss: 0.008311313576996326\n",
      "Epoch 875 training loss: 0.008181355893611908\n",
      "Epoch 876 training loss: 0.008286547847092152\n",
      "Epoch 877 training loss: 0.00796560663729906\n",
      "Epoch 878 training loss: 0.008442847989499569\n",
      "Epoch 879 training loss: 0.0078525859862566\n",
      "Epoch 880 training loss: 0.008054917678236961\n",
      "Epoch 881 training loss: 0.00786519143730402\n",
      "Epoch 882 training loss: 0.008084520697593689\n",
      "Epoch 883 training loss: 0.00812488328665495\n",
      "Epoch 884 training loss: 0.008223351091146469\n",
      "Epoch 885 training loss: 0.008452627807855606\n",
      "Epoch 886 training loss: 0.008224216289818287\n",
      "Epoch 887 training loss: 0.008170844055712223\n",
      "Epoch 888 training loss: 0.008170673623681068\n",
      "Epoch 889 training loss: 0.008139021694660187\n",
      "Epoch 890 training loss: 0.008048996329307556\n",
      "Epoch 891 training loss: 0.007927724160254002\n",
      "Epoch 892 training loss: 0.008257443085312843\n",
      "Epoch 893 training loss: 0.007967157289385796\n",
      "Epoch 894 training loss: 0.008310025557875633\n",
      "Epoch 895 training loss: 0.008067003451287746\n",
      "Epoch 896 training loss: 0.007879507727921009\n",
      "Epoch 897 training loss: 0.008204521611332893\n",
      "Epoch 898 training loss: 0.008250783197581768\n",
      "Epoch 899 training loss: 0.008380993269383907\n",
      "Epoch 900 training loss: 0.0081655103713274\n",
      "Epoch 901 training loss: 0.007930419407784939\n",
      "Epoch 902 training loss: 0.008173671551048756\n",
      "Epoch 903 training loss: 0.008150468580424786\n",
      "Epoch 904 training loss: 0.00801034364849329\n",
      "Epoch 905 training loss: 0.008282851427793503\n",
      "Epoch 906 training loss: 0.00824336800724268\n",
      "Epoch 907 training loss: 0.008055733516812325\n",
      "Epoch 908 training loss: 0.007957198657095432\n",
      "Epoch 909 training loss: 0.008485960774123669\n",
      "Epoch 910 training loss: 0.008157143369317055\n",
      "Epoch 911 training loss: 0.007952407002449036\n",
      "Epoch 912 training loss: 0.00816657766699791\n",
      "Epoch 913 training loss: 0.00822355691343546\n",
      "Epoch 914 training loss: 0.008089002221822739\n",
      "Epoch 915 training loss: 0.008222655393183231\n",
      "Epoch 916 training loss: 0.007993419654667377\n",
      "Epoch 917 training loss: 0.007968823425471783\n",
      "Epoch 918 training loss: 0.00786726363003254\n",
      "Epoch 919 training loss: 0.008179262280464172\n",
      "Epoch 920 training loss: 0.008089500479400158\n",
      "Epoch 921 training loss: 0.008252661675214767\n",
      "Epoch 922 training loss: 0.007902967743575573\n",
      "Epoch 923 training loss: 0.008088603615760803\n",
      "Epoch 924 training loss: 0.00795393530279398\n",
      "Epoch 925 training loss: 0.00843742024153471\n",
      "Epoch 926 training loss: 0.008203946053981781\n",
      "Epoch 927 training loss: 0.008110200054943562\n",
      "Epoch 928 training loss: 0.008017758838832378\n",
      "Epoch 929 training loss: 0.007927575148642063\n",
      "Epoch 930 training loss: 0.008204332552850246\n",
      "Epoch 931 training loss: 0.008019914850592613\n",
      "Epoch 932 training loss: 0.007878653705120087\n",
      "Epoch 933 training loss: 0.008142504841089249\n",
      "Epoch 934 training loss: 0.00791613943874836\n",
      "Epoch 935 training loss: 0.008241127245128155\n",
      "Epoch 936 training loss: 0.007954247295856476\n",
      "Epoch 937 training loss: 0.008061355911195278\n",
      "Epoch 938 training loss: 0.008000930771231651\n",
      "Epoch 939 training loss: 0.00848045852035284\n",
      "Epoch 940 training loss: 0.007844259962439537\n",
      "Epoch 941 training loss: 0.008175992406904697\n",
      "Epoch 942 training loss: 0.008373628370463848\n",
      "Epoch 943 training loss: 0.00788891687989235\n",
      "Epoch 944 training loss: 0.007885022088885307\n",
      "Epoch 945 training loss: 0.008033249527215958\n",
      "Epoch 946 training loss: 0.008127689361572266\n",
      "Epoch 947 training loss: 0.007993209175765514\n",
      "Epoch 948 training loss: 0.008069591596722603\n",
      "Epoch 949 training loss: 0.008113004267215729\n",
      "Epoch 950 training loss: 0.008223136886954308\n",
      "Epoch 951 training loss: 0.0080986637622118\n",
      "Epoch 952 training loss: 0.008567364886403084\n",
      "Epoch 953 training loss: 0.007925034500658512\n",
      "Epoch 954 training loss: 0.007787894457578659\n",
      "Epoch 955 training loss: 0.008008666336536407\n",
      "Epoch 956 training loss: 0.008018103428184986\n",
      "Epoch 957 training loss: 0.008013835176825523\n",
      "Epoch 958 training loss: 0.008552249521017075\n",
      "Epoch 959 training loss: 0.008167658932507038\n",
      "Epoch 960 training loss: 0.00791058037430048\n",
      "Epoch 961 training loss: 0.00810912624001503\n",
      "Epoch 962 training loss: 0.007888155989348888\n",
      "Epoch 963 training loss: 0.008090468123555183\n",
      "Epoch 964 training loss: 0.007987131364643574\n",
      "Epoch 965 training loss: 0.00822514109313488\n",
      "Epoch 966 training loss: 0.007906931452453136\n",
      "Epoch 967 training loss: 0.00786307081580162\n",
      "Epoch 968 training loss: 0.00808698683977127\n",
      "Epoch 969 training loss: 0.008557235822081566\n",
      "Epoch 970 training loss: 0.008153512142598629\n",
      "Epoch 971 training loss: 0.00804845616221428\n",
      "Epoch 972 training loss: 0.00796901248395443\n",
      "Epoch 973 training loss: 0.007923468016088009\n",
      "Epoch 974 training loss: 0.008047785609960556\n",
      "Epoch 975 training loss: 0.007918903604149818\n",
      "Epoch 976 training loss: 0.008550225757062435\n",
      "Epoch 977 training loss: 0.007977095432579517\n",
      "Epoch 978 training loss: 0.007885240018367767\n",
      "Epoch 979 training loss: 0.00797110516577959\n",
      "Epoch 980 training loss: 0.007930153049528599\n",
      "Epoch 981 training loss: 0.007779119536280632\n",
      "Epoch 982 training loss: 0.007876764051616192\n",
      "Epoch 983 training loss: 0.008316676132380962\n",
      "Epoch 984 training loss: 0.007886066101491451\n",
      "Epoch 985 training loss: 0.008085246197879314\n",
      "Epoch 986 training loss: 0.008012279868125916\n",
      "Epoch 987 training loss: 0.00818746816366911\n",
      "Epoch 988 training loss: 0.007836793549358845\n",
      "Epoch 989 training loss: 0.007961583323776722\n",
      "Epoch 990 training loss: 0.007888180203735828\n",
      "Epoch 991 training loss: 0.007990285754203796\n",
      "Epoch 992 training loss: 0.007929353043437004\n",
      "Epoch 993 training loss: 0.008198077790439129\n",
      "Epoch 994 training loss: 0.007890029810369015\n",
      "Epoch 995 training loss: 0.008147367276251316\n",
      "Epoch 996 training loss: 0.007968208752572536\n",
      "Epoch 997 training loss: 0.00807669572532177\n",
      "Epoch 998 training loss: 0.007986797019839287\n",
      "Epoch 999 training loss: 0.00827275775372982\n",
      "Epoch 1000 training loss: 0.008104848675429821\n",
      "Epoch 1001 training loss: 0.007799683604389429\n",
      "Epoch 1002 training loss: 0.008114656433463097\n",
      "Epoch 1003 training loss: 0.007744221016764641\n",
      "Epoch 1004 training loss: 0.008463077247142792\n",
      "Epoch 1005 training loss: 0.007965967990458012\n",
      "Epoch 1006 training loss: 0.00810154527425766\n",
      "Epoch 1007 training loss: 0.008131440728902817\n",
      "Epoch 1008 training loss: 0.007962842471897602\n",
      "Epoch 1009 training loss: 0.007958635687828064\n",
      "Epoch 1010 training loss: 0.007896624505519867\n",
      "Epoch 1011 training loss: 0.008353947661817074\n",
      "Epoch 1012 training loss: 0.007967730984091759\n",
      "Epoch 1013 training loss: 0.007838782854378223\n",
      "Epoch 1014 training loss: 0.008032127283513546\n",
      "Epoch 1015 training loss: 0.00811082310974598\n",
      "Epoch 1016 training loss: 0.007881744764745235\n",
      "Epoch 1017 training loss: 0.007972819730639458\n",
      "Epoch 1018 training loss: 0.008037892170250416\n",
      "Epoch 1019 training loss: 0.008198486641049385\n",
      "Epoch 1020 training loss: 0.00777027290314436\n",
      "Epoch 1021 training loss: 0.007727242540568113\n",
      "Epoch 1022 training loss: 0.007845277898013592\n",
      "Epoch 1023 training loss: 0.008046802133321762\n",
      "Epoch 1024 training loss: 0.008339457213878632\n",
      "0.6751824817518248\n",
      "[[6474 1545]\n",
      " [1926  741]]\n",
      "Accuracy: 0.6751824817518248\n",
      "Precision:  0.3241469816272966\n",
      "Recall:  0.2778402699662542\n",
      "F1:  0.2992125984251969\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06506173312664032\n",
      "Epoch 2 training loss: 0.07763776928186417\n",
      "Epoch 3 training loss: 0.07750710844993591\n",
      "Epoch 4 training loss: 0.07750604301691055\n",
      "Epoch 5 training loss: 0.0775478407740593\n",
      "Epoch 6 training loss: 0.07753785699605942\n",
      "Epoch 7 training loss: 0.07755934447050095\n",
      "Epoch 8 training loss: 0.077571801841259\n",
      "Epoch 9 training loss: 0.07756922394037247\n",
      "Epoch 10 training loss: 0.07756340503692627\n",
      "Epoch 11 training loss: 0.07754787057638168\n",
      "Epoch 12 training loss: 0.07752231508493423\n",
      "Epoch 13 training loss: 0.07755836099386215\n",
      "Epoch 14 training loss: 0.07758618146181107\n",
      "Epoch 15 training loss: 0.07756387442350388\n",
      "Epoch 16 training loss: 0.07757235318422318\n",
      "Epoch 17 training loss: 0.07750573009252548\n",
      "Epoch 18 training loss: 0.07753414660692215\n",
      "Epoch 19 training loss: 0.07756569981575012\n",
      "Epoch 20 training loss: 0.07760342955589294\n",
      "Epoch 21 training loss: 0.07752853631973267\n",
      "Epoch 22 training loss: 0.07759113609790802\n",
      "Epoch 23 training loss: 0.0775730237364769\n",
      "Epoch 24 training loss: 0.07748489081859589\n",
      "Epoch 25 training loss: 0.07759560644626617\n",
      "Epoch 26 training loss: 0.07760005444288254\n",
      "Epoch 27 training loss: 0.07755468040704727\n",
      "Epoch 28 training loss: 0.07757961750030518\n",
      "Epoch 29 training loss: 0.07765664905309677\n",
      "Epoch 30 training loss: 0.07753808051347733\n",
      "Epoch 31 training loss: 0.07755281776189804\n",
      "Epoch 32 training loss: 0.07756466418504715\n",
      "Epoch 33 training loss: 0.07744160294532776\n",
      "Epoch 34 training loss: 0.07760224491357803\n",
      "Epoch 35 training loss: 0.07759104669094086\n",
      "Epoch 36 training loss: 0.0775335431098938\n",
      "Epoch 37 training loss: 0.07760794460773468\n",
      "Epoch 38 training loss: 0.07758035510778427\n",
      "Epoch 39 training loss: 0.07764425128698349\n",
      "Epoch 40 training loss: 0.07746368646621704\n",
      "Epoch 41 training loss: 0.07755619287490845\n",
      "Epoch 42 training loss: 0.0775802955031395\n",
      "Epoch 43 training loss: 0.07760118693113327\n",
      "Epoch 44 training loss: 0.07758918404579163\n",
      "Epoch 45 training loss: 0.07755822688341141\n",
      "Epoch 46 training loss: 0.07765565812587738\n",
      "Epoch 47 training loss: 0.07766027003526688\n",
      "Epoch 48 training loss: 0.07756874710321426\n",
      "Epoch 49 training loss: 0.07753007113933563\n",
      "Epoch 50 training loss: 0.07750634849071503\n",
      "Epoch 51 training loss: 0.07748124748468399\n",
      "Epoch 52 training loss: 0.07757296413183212\n",
      "Epoch 53 training loss: 0.07763794809579849\n",
      "Epoch 54 training loss: 0.07752776890993118\n",
      "Epoch 55 training loss: 0.0775943249464035\n",
      "Epoch 56 training loss: 0.07758201658725739\n",
      "Epoch 57 training loss: 0.07751638442277908\n",
      "Epoch 58 training loss: 0.07758913934230804\n",
      "Epoch 59 training loss: 0.07758311927318573\n",
      "Epoch 60 training loss: 0.07748141139745712\n",
      "Epoch 61 training loss: 0.0774914026260376\n",
      "Epoch 62 training loss: 0.07765224575996399\n",
      "Epoch 63 training loss: 0.07758308947086334\n",
      "Epoch 64 training loss: 0.0774996429681778\n",
      "0.7880404267265582\n",
      "[[7219  800]\n",
      " [1465 1202]]\n",
      "Accuracy: 0.7880404267265582\n",
      "Precision:  0.6003996003996004\n",
      "Recall:  0.4506936632920885\n",
      "F1:  0.5148854144356393\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.66,0.60,0.74 ,0.61 ,0.81]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['blue', 'red', 'lime', 'orange', 'green'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Recall\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.95, 0.19, 0.99, 0.92, 0.94]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['green', 'red', 'lime', 'orange', 'blue'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07836276292800903\n",
      "Epoch 2 training loss: 0.07754524797201157\n",
      "Epoch 3 training loss: 0.07745422422885895\n",
      "Epoch 4 training loss: 0.07746409624814987\n",
      "Epoch 5 training loss: 0.07742595672607422\n",
      "Epoch 6 training loss: 0.07742536813020706\n",
      "Epoch 7 training loss: 0.07751432806253433\n",
      "Epoch 8 training loss: 0.0775236040353775\n",
      "Epoch 9 training loss: 0.07753631472587585\n",
      "Epoch 10 training loss: 0.0775417909026146\n",
      "Epoch 11 training loss: 0.07748011499643326\n",
      "Epoch 12 training loss: 0.0774666965007782\n",
      "Epoch 13 training loss: 0.07744150608778\n",
      "Epoch 14 training loss: 0.07743609696626663\n",
      "Epoch 15 training loss: 0.07757815718650818\n",
      "Epoch 16 training loss: 0.07755087316036224\n",
      "Epoch 17 training loss: 0.07752357423305511\n",
      "Epoch 18 training loss: 0.07744619250297546\n",
      "Epoch 19 training loss: 0.07757273316383362\n",
      "Epoch 20 training loss: 0.07744909822940826\n",
      "Epoch 21 training loss: 0.07758618891239166\n",
      "Epoch 22 training loss: 0.07743437588214874\n",
      "Epoch 23 training loss: 0.07756263762712479\n",
      "Epoch 24 training loss: 0.07752298563718796\n",
      "Epoch 25 training loss: 0.07752542942762375\n",
      "Epoch 26 training loss: 0.07756495475769043\n",
      "Epoch 27 training loss: 0.07750868797302246\n",
      "Epoch 28 training loss: 0.07753882557153702\n",
      "Epoch 29 training loss: 0.07749243080615997\n",
      "Epoch 30 training loss: 0.07750589400529861\n",
      "Epoch 31 training loss: 0.0774526298046112\n",
      "Epoch 32 training loss: 0.07752778381109238\n",
      "Epoch 33 training loss: 0.07742828130722046\n",
      "Epoch 34 training loss: 0.07756347954273224\n",
      "Epoch 35 training loss: 0.07752636820077896\n",
      "Epoch 36 training loss: 0.07749959826469421\n",
      "Epoch 37 training loss: 0.07742299139499664\n",
      "Epoch 38 training loss: 0.07745842635631561\n",
      "Epoch 39 training loss: 0.0775073692202568\n",
      "Epoch 40 training loss: 0.07751455157995224\n",
      "Epoch 41 training loss: 0.07746180891990662\n",
      "Epoch 42 training loss: 0.07754518836736679\n",
      "Epoch 43 training loss: 0.07750449329614639\n",
      "Epoch 44 training loss: 0.07747507840394974\n",
      "Epoch 45 training loss: 0.0774766057729721\n",
      "Epoch 46 training loss: 0.07754531502723694\n",
      "Epoch 47 training loss: 0.07754786312580109\n",
      "Epoch 48 training loss: 0.07752633094787598\n",
      "Epoch 49 training loss: 0.07751888781785965\n",
      "Epoch 50 training loss: 0.07747573405504227\n",
      "Epoch 51 training loss: 0.0775204673409462\n",
      "Epoch 52 training loss: 0.07747606933116913\n",
      "Epoch 53 training loss: 0.0774846151471138\n",
      "Epoch 54 training loss: 0.07765131443738937\n",
      "Epoch 55 training loss: 0.07753846049308777\n",
      "Epoch 56 training loss: 0.07741639763116837\n",
      "Epoch 57 training loss: 0.07751066237688065\n",
      "Epoch 58 training loss: 0.07752508670091629\n",
      "Epoch 59 training loss: 0.07754455506801605\n",
      "Epoch 60 training loss: 0.07749159634113312\n",
      "Epoch 61 training loss: 0.07746665179729462\n",
      "Epoch 62 training loss: 0.07758487015962601\n",
      "Epoch 63 training loss: 0.07753114402294159\n",
      "Epoch 64 training loss: 0.07749012112617493\n",
      "Epoch 65 training loss: 0.07751606404781342\n",
      "Epoch 66 training loss: 0.07754688709974289\n",
      "Epoch 67 training loss: 0.07752802968025208\n",
      "Epoch 68 training loss: 0.07753319293260574\n",
      "Epoch 69 training loss: 0.07754796743392944\n",
      "Epoch 70 training loss: 0.07742898911237717\n",
      "Epoch 71 training loss: 0.07756330072879791\n",
      "Epoch 72 training loss: 0.07753963768482208\n",
      "Epoch 73 training loss: 0.0775553435087204\n",
      "Epoch 74 training loss: 0.07745508849620819\n",
      "Epoch 75 training loss: 0.07741343230009079\n",
      "Epoch 76 training loss: 0.07757149636745453\n",
      "Epoch 77 training loss: 0.07751405984163284\n",
      "Epoch 78 training loss: 0.07746954262256622\n",
      "Epoch 79 training loss: 0.07744664698839188\n",
      "Epoch 80 training loss: 0.07746455073356628\n",
      "Epoch 81 training loss: 0.07753638178110123\n",
      "Epoch 82 training loss: 0.07749690115451813\n",
      "Epoch 83 training loss: 0.077464260160923\n",
      "Epoch 84 training loss: 0.07753986865282059\n",
      "Epoch 85 training loss: 0.07742704451084137\n",
      "Epoch 86 training loss: 0.07747697085142136\n",
      "Epoch 87 training loss: 0.0775236040353775\n",
      "Epoch 88 training loss: 0.07745207101106644\n",
      "Epoch 89 training loss: 0.0774829089641571\n",
      "Epoch 90 training loss: 0.07758104056119919\n",
      "Epoch 91 training loss: 0.0774734690785408\n",
      "Epoch 92 training loss: 0.07754381000995636\n",
      "Epoch 93 training loss: 0.07755278795957565\n",
      "Epoch 94 training loss: 0.07750848680734634\n",
      "Epoch 95 training loss: 0.07762368768453598\n",
      "Epoch 96 training loss: 0.07752431929111481\n",
      "Epoch 97 training loss: 0.0774645060300827\n",
      "Epoch 98 training loss: 0.07745029032230377\n",
      "Epoch 99 training loss: 0.07766041904687881\n",
      "Epoch 100 training loss: 0.07756766676902771\n",
      "Epoch 101 training loss: 0.07752783596515656\n",
      "Epoch 102 training loss: 0.07756087183952332\n",
      "Epoch 103 training loss: 0.0775577574968338\n",
      "Epoch 104 training loss: 0.0775250792503357\n",
      "Epoch 105 training loss: 0.0774771049618721\n",
      "Epoch 106 training loss: 0.0775931254029274\n",
      "Epoch 107 training loss: 0.07745718955993652\n",
      "Epoch 108 training loss: 0.07746520638465881\n",
      "Epoch 109 training loss: 0.07755742967128754\n",
      "Epoch 110 training loss: 0.07747315615415573\n",
      "Epoch 111 training loss: 0.07748185843229294\n",
      "Epoch 112 training loss: 0.07748344540596008\n",
      "Epoch 113 training loss: 0.07749829441308975\n",
      "Epoch 114 training loss: 0.07748587429523468\n",
      "Epoch 115 training loss: 0.07752258330583572\n",
      "Epoch 116 training loss: 0.07752960175275803\n",
      "Epoch 117 training loss: 0.07753312587738037\n",
      "Epoch 118 training loss: 0.07755168527364731\n",
      "Epoch 119 training loss: 0.077485591173172\n",
      "Epoch 120 training loss: 0.07754621654748917\n",
      "Epoch 121 training loss: 0.07759495079517365\n",
      "Epoch 122 training loss: 0.07758749276399612\n",
      "Epoch 123 training loss: 0.07742922008037567\n",
      "Epoch 124 training loss: 0.07758192718029022\n",
      "Epoch 125 training loss: 0.07751648873090744\n",
      "Epoch 126 training loss: 0.07739662379026413\n",
      "Epoch 127 training loss: 0.07750488817691803\n",
      "Epoch 128 training loss: 0.07753628492355347\n",
      "0.7885083286543141\n",
      "[[7254  765]\n",
      " [1495 1172]]\n",
      "Accuracy: 0.7885083286543141\n",
      "Precision:  0.6050593701600413\n",
      "Recall:  0.4394450693663292\n",
      "F1:  0.5091225021720244\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=128, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
