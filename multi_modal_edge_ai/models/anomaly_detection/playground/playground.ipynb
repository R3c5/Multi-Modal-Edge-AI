{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multi_modal_edge_ai.models.anomaly_detection.data_access.parser import parse_file_with_idle\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.local_outlier_factor import LOF\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.isolation_forest import IForest\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.one_class_svm import OCSVM\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.autoencoder import Autoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.lstm_autoencoder import LSTMAutoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.model_validator import model_train_eval\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.hyperparameter_config import HyperparameterConfig as Hparams\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "data = parse_file_with_idle(\"../../public_datasets/Aruba_Idle_Squashed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5905411163187048\n",
      "[[1376   82]\n",
      " [ 879   10]]\n",
      "Accuracy:  0.5905411163187048\n",
      "Precision:  0.943758573388203\n",
      "Recall:  0.61019955654102\n",
      "F1:  0.7411796391058443\n"
     ]
    }
   ],
   "source": [
    " # Basic LOF model with changed hyperparameters\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 300,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 40,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.1,\n",
    "             window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "0.5069867431028305\n",
      "[[ 111 1347]\n",
      " [  29 1304]]\n",
      "Accuracy:  0.5069867431028305\n",
      "Precision:  0.07613168724279835\n",
      "Recall:  0.7928571428571428\n",
      "F1:  0.13892365456821026\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 16\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 4000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 400,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": 0.001,\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5206019347903977\n",
      "[[ 166 1292]\n",
      " [  46 1287]]\n",
      "Accuracy:  0.5206019347903977\n",
      "Precision:  0.11385459533607682\n",
      "Recall:  0.7830188679245284\n",
      "F1:  0.19880239520958085\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 64\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 2000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 300,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iforest = IForest()\n",
    "\n",
    "iforestParams = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_samples\": \"auto\",\n",
    "    \"contamination\": 0.1,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "hp = Hparams(i_forest_hparams=iforestParams, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(iforest, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[1 0 1 ... 1 1 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "0.7200546946216956\n",
      "[[6917 1831]\n",
      " [1240  982]]\n",
      "Accuracy: 0.7200546946216956\n",
      "Precision:  0.790695016003658\n",
      "Recall:  0.8479833272036288\n",
      "F1:  0.818337769890565\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.01,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2400,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.25,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05389717221260071\n",
      "Epoch 2 training loss: 0.030980540439486504\n",
      "Epoch 3 training loss: 0.023487167432904243\n",
      "Epoch 4 training loss: 0.020996732637286186\n",
      "Epoch 5 training loss: 0.019222674891352654\n",
      "Epoch 6 training loss: 0.017535774037241936\n",
      "Epoch 7 training loss: 0.016274292021989822\n",
      "Epoch 8 training loss: 0.01566317304968834\n",
      "Epoch 9 training loss: 0.015262589789927006\n",
      "Epoch 10 training loss: 0.014593065716326237\n",
      "0.2847235417062512\n",
      "[[1318 7430]\n",
      " [  99 1679]]\n",
      "Accuracy: 0.2847235417062512\n",
      "Precision:  0.1843231968382918\n",
      "Recall:  0.9443194600674916\n",
      "F1:  0.30844126021860935\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([96,64,32,16], [16,32,64,96],nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=16,  n_epochs=10, anomaly_generation_ratio = 11, clean_test_data_ratio=0.2, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0.4505562422744129\n",
      "[[1458    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.4505562422744129\n",
      "Precision:  1.0\n",
      "Recall:  0.4505562422744129\n",
      "F1:  0.6212185769066894\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=32, n_epochs=16, clean_test_data_ratio=0.2,window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07760713093037244\n",
      "Epoch 2 training loss: 0.07725755977669586\n",
      "Epoch 3 training loss: 0.0772386808702628\n",
      "Epoch 4 training loss: 0.07725333552849474\n",
      "Epoch 5 training loss: 0.07720437902104449\n",
      "Epoch 6 training loss: 0.07721456329896627\n",
      "Epoch 7 training loss: 0.0772136117869106\n",
      "Epoch 8 training loss: 0.07721375107463486\n",
      "Epoch 9 training loss: 0.07718426470631153\n",
      "Epoch 10 training loss: 0.07720113484895001\n",
      "Epoch 11 training loss: 0.07719907346490616\n",
      "Epoch 12 training loss: 0.07718318898107174\n",
      "Epoch 13 training loss: 0.07717363352660002\n",
      "Epoch 14 training loss: 0.07718654360969973\n",
      "Epoch 15 training loss: 0.07717847603837319\n",
      "Epoch 16 training loss: 0.07719744948766648\n",
      "Epoch 17 training loss: 0.07721296846458041\n",
      "Epoch 18 training loss: 0.07720602415680164\n",
      "Epoch 19 training loss: 0.07716414992486968\n",
      "Epoch 20 training loss: 0.07716112782344792\n",
      "Epoch 21 training loss: 0.07718445156324819\n",
      "Epoch 22 training loss: 0.07716832268737986\n",
      "Epoch 23 training loss: 0.07718266583130569\n",
      "Epoch 24 training loss: 0.0771673936056365\n",
      "Epoch 25 training loss: 0.07717815300615251\n",
      "Epoch 26 training loss: 0.07719496329638717\n",
      "Epoch 27 training loss: 0.0771747040143289\n",
      "Epoch 28 training loss: 0.07716852902509147\n",
      "Epoch 29 training loss: 0.07714804287327305\n",
      "Epoch 30 training loss: 0.07716223987655377\n",
      "Epoch 31 training loss: 0.07717982528331041\n",
      "Epoch 32 training loss: 0.07717942436989418\n",
      "0.6146477132262052\n",
      "[[ 340 1118]\n",
      " [ 129 1649]]\n",
      "Accuracy: 0.6146477132262052\n",
      "Precision:  0.5959522949042284\n",
      "Recall:  0.9274465691788526\n",
      "F1:  0.7256325632563256\n"
     ]
    }
   ],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=16, n_epochs=32, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.7262534184138559\n",
      "[[6972 1776]\n",
      " [1227  995]]\n",
      "Accuracy:  0.7262534184138559\n",
      "Precision:  0.3590761457957416\n",
      "Recall:  0.44779477947794777\n",
      "F1:  0.3985579811736431\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  1e-05\n",
      "0.697538742023701\n",
      "[[6620 2128]\n",
      " [1190 1032]]\n",
      "Accuracy:  0.697538742023701\n",
      "Precision:  0.3265822784810127\n",
      "Recall:  0.46444644464446444\n",
      "F1:  0.3835005574136009\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.7302643573381951\n",
      "[[6966 1782]\n",
      " [1177 1045]]\n",
      "Accuracy:  0.7302643573381951\n",
      "Precision:  0.36964980544747084\n",
      "Recall:  0.47029702970297027\n",
      "F1:  0.4139433551198257\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  1e-05\n",
      "0.703281677301732\n",
      "[[6672 2076]\n",
      " [1179 1043]]\n",
      "Accuracy:  0.703281677301732\n",
      "Precision:  0.3344020519397243\n",
      "Recall:  0.4693969396939694\n",
      "F1:  0.3905635648754915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.7003646308113035\n",
      "[[6583 2165]\n",
      " [1122 1100]]\n",
      "Accuracy:  0.7003646308113035\n",
      "Precision:  0.33690658499234305\n",
      "Recall:  0.49504950495049505\n",
      "F1:  0.40094769455075635\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  1e-05\n",
      "0.7062898814949863\n",
      "[[6674 2074]\n",
      " [1148 1074]]\n",
      "Accuracy:  0.7062898814949863\n",
      "Precision:  0.3411689961880559\n",
      "Recall:  0.48334833483348333\n",
      "F1:  0.4\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.692616226071103\n",
      "[[6469 2279]\n",
      " [1093 1129]]\n",
      "Accuracy:  0.692616226071103\n",
      "Precision:  0.3312793427230047\n",
      "Recall:  0.508100810081008\n",
      "F1:  0.4010657193605684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  1e-05\n",
      "0.7061987237921604\n",
      "[[6658 2090]\n",
      " [1133 1089]]\n",
      "Accuracy:  0.7061987237921604\n",
      "Precision:  0.34256055363321797\n",
      "Recall:  0.4900990099009901\n",
      "F1:  0.40325865580448067\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.46298997265268915\n",
      "[[3161 5587]\n",
      " [ 304 1918]]\n",
      "Accuracy:  0.46298997265268915\n",
      "Precision:  0.25556295802798135\n",
      "Recall:  0.8631863186318632\n",
      "F1:  0.39436619718309857\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  1e-05\n",
      "0.47265268915223335\n",
      "[[3271 5477]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47265268915223335\n",
      "Precision:  0.2589636043837099\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39821075626755437\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.4610756608933455\n",
      "[[3173 5575]\n",
      " [ 337 1885]]\n",
      "Accuracy:  0.4610756608933455\n",
      "Precision:  0.2526809651474531\n",
      "Recall:  0.8483348334833484\n",
      "F1:  0.38938235901673207\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  1e-05\n",
      "0.4740200546946217\n",
      "[[3323 5425]\n",
      " [ 345 1877]]\n",
      "Accuracy:  0.4740200546946217\n",
      "Precision:  0.25705286222952617\n",
      "Recall:  0.8447344734473448\n",
      "F1:  0.3941621167576648\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.46362807657247035\n",
      "[[3200 5548]\n",
      " [ 336 1886]]\n",
      "Accuracy:  0.46362807657247035\n",
      "Precision:  0.2536992198009147\n",
      "Recall:  0.8487848784878488\n",
      "F1:  0.39063794531897267\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  1e-05\n",
      "0.4690975387420237\n",
      "[[3250 5498]\n",
      " [ 326 1896]]\n",
      "Accuracy:  0.4690975387420237\n",
      "Precision:  0.2564241276710847\n",
      "Recall:  0.8532853285328533\n",
      "F1:  0.39434276206322794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.47201458523245216\n",
      "[[3264 5484]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47201458523245216\n",
      "Precision:  0.2587185725871857\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39792099792099794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  1e-05\n",
      "0.4545123062898815\n",
      "[[3079 5669]\n",
      " [ 315 1907]]\n",
      "Accuracy:  0.4545123062898815\n",
      "Precision:  0.25171594508975714\n",
      "Recall:  0.8582358235823583\n",
      "F1:  0.3892631149214125\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for OCSVM\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "kernels = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.0001, 0.00001]\n",
    "\n",
    "for k in kernels:\n",
    "    for d in degrees:\n",
    "        for g in gammas:\n",
    "            for c in coef0s:\n",
    "                for t in tols:\n",
    "                    ovscmParams = {\n",
    "                        \"kernel\": k,\n",
    "                        \"degree\": d if k == \"poly\" else 3,\n",
    "                        \"gamma\": g,\n",
    "                        \"coef0\": c if k == \"poly\" or k == \"sigmoid\" else 0.0,\n",
    "                        \"tol\": t,\n",
    "                        \"nu\": 0.001,\n",
    "                        \"shrinking\": True,\n",
    "                        \"cache_size\": 3200,\n",
    "                        \"verbose\": False,\n",
    "                        \"max_iter\": -1,\n",
    "                    }\n",
    "\n",
    "                    hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=11,\n",
    "                                 clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                                 one_hot=False)\n",
    "                    (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                    print(\"for kernel: \", k, \" degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                    print(avg)\n",
    "                    print(cm)\n",
    "                    (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                    print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                    print(\"Precision: \", tp / (tp + fp))\n",
    "                    print(\"Recall: \", tp / (tp + fn))\n",
    "                    print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                    print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6910205869469995\n",
      "[[6581 2167]\n",
      " [1360 1307]]\n",
      "Accuracy:  0.6910205869469995\n",
      "Precision:  0.37622337363270003\n",
      "Recall:  0.49006374203224595\n",
      "F1:  0.425663572708028\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.0001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  26 1432]\n",
      " [   7 2215]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6073485056210584\n",
      "Recall:  0.9968496849684968\n",
      "F1:  0.7548134264781053\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  23 1435]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074945295404814\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7556992174208914\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  25 1433]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078270388615217\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7559564329475834\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6168478260869565\n",
      "[[  54 1404]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6168478260869565\n",
      "Precision:  0.6121546961325967\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7586442998972954\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6103260869565217\n",
      "[[  24 1434]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6077680525164114\n",
      "Recall:  1.0\n",
      "F1:  0.7560394692072133\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  17 1441]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064991807755324\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7549286199864038\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6078804347826087\n",
      "[[  16 1442]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6078804347826087\n",
      "Precision:  0.6063336063336063\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7548003398470688\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  19 1439]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069379950833106\n",
      "Recall:  1.0\n",
      "F1:  0.7553969063403025\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6108695652173913\n",
      "[[  26 1432]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6081007115489874\n",
      "Recall:  1.0\n",
      "F1:  0.7562968005445881\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.647554347826087\n",
      "[[ 310 1148]\n",
      " [ 149 2073]]\n",
      "Accuracy:  0.647554347826087\n",
      "Precision:  0.6435889475318224\n",
      "Recall:  0.9329432943294329\n",
      "F1:  0.7617122910159838\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.61875\n",
      "[[  75 1383]\n",
      " [  20 2202]]\n",
      "Accuracy:  0.61875\n",
      "Precision:  0.6142259414225941\n",
      "Recall:  0.990999099909991\n",
      "F1:  0.7583950404684002\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6108695652173913\n",
      "[[  32 1426]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6084568918176826\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7557980900409277\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6105978260869566\n",
      "[[  25 1433]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6105978260869566\n",
      "Precision:  0.6079343365253078\n",
      "Recall:  1.0\n",
      "F1:  0.7561681129828144\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6111413043478261\n",
      "[[  31 1427]\n",
      " [   4 2218]]\n",
      "Accuracy:  0.6111413043478261\n",
      "Precision:  0.6085048010973937\n",
      "Recall:  0.9981998199819982\n",
      "F1:  0.7560934037838759\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  26 1432]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078860898138007\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7558733401430031\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6144021739130435\n",
      "[[  41 1417]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6144021739130435\n",
      "Precision:  0.6103931811932912\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7578084997439836\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6059782608695652\n",
      "[[   8 1450]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6059782608695652\n",
      "Precision:  0.605119825708061\n",
      "Recall:  1.0\n",
      "F1:  0.7539871055310485\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6293478260869565\n",
      "[[ 732  726]\n",
      " [ 638 1584]]\n",
      "Accuracy:  0.6293478260869565\n",
      "Precision:  0.6857142857142857\n",
      "Recall:  0.7128712871287128\n",
      "F1:  0.6990291262135923\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6157608695652174\n",
      "[[  49 1409]\n",
      " [   5 2217]]\n",
      "Accuracy:  0.6157608695652174\n",
      "Precision:  0.6114175399889685\n",
      "Recall:  0.9977497749774977\n",
      "F1:  0.7582079343365253\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6141304347826086\n",
      "[[  40 1418]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6141304347826086\n",
      "Precision:  0.6102253985706432\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.757679180887372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  20 1438]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069964471166985\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7553137221560959\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 10% of the data\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6518787878787878\n",
      "[[  23 1435]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6518787878787878\n",
      "Precision:  0.6500853450377957\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7878250591016549\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"poly\",\n",
    "    \"degree\": 5,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.3,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  200\n",
      "0.6681212121212121\n",
      "[[ 217 1241]\n",
      " [ 128 2539]]\n",
      "Accuracy:  0.6681212121212121\n",
      "Precision:  0.6716931216931217\n",
      "Recall:  0.9520059992500938\n",
      "F1:  0.7876531720179929\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  250\n",
      "0.6678787878787878\n",
      "[[ 213 1245]\n",
      " [ 125 2542]]\n",
      "Accuracy:  0.6678787878787878\n",
      "Precision:  0.6712437285450225\n",
      "Recall:  0.9531308586426697\n",
      "F1:  0.7877285404400372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  300\n",
      "0.6693333333333333\n",
      "[[ 215 1243]\n",
      " [ 121 2546]]\n",
      "Accuracy:  0.6693333333333333\n",
      "Precision:  0.6719451042491422\n",
      "Recall:  0.9546306711661042\n",
      "F1:  0.788723667905824\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  350\n",
      "0.6676363636363636\n",
      "[[ 210 1248]\n",
      " [ 123 2544]]\n",
      "Accuracy:  0.6676363636363636\n",
      "Precision:  0.6708860759493671\n",
      "Recall:  0.953880764904387\n",
      "F1:  0.7877380399442638\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  200\n",
      "0.6557575757575758\n",
      "[[  78 1380]\n",
      " [  40 2627]]\n",
      "Accuracy:  0.6557575757575758\n",
      "Precision:  0.6556026952832543\n",
      "Recall:  0.9850018747656543\n",
      "F1:  0.7872340425531915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  250\n",
      "0.6564848484848484\n",
      "[[  85 1373]\n",
      " [  44 2623]]\n",
      "Accuracy:  0.6564848484848484\n",
      "Precision:  0.6564064064064065\n",
      "Recall:  0.9835020622422197\n",
      "F1:  0.7873330331682425\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  300\n",
      "0.6591515151515152\n",
      "[[  83 1375]\n",
      " [  31 2636]]\n",
      "Accuracy:  0.6591515151515152\n",
      "Precision:  0.6571927200199451\n",
      "Recall:  0.9883764529433821\n",
      "F1:  0.7894579215333932\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  350\n",
      "0.6574545454545454\n",
      "[[  88 1370]\n",
      " [  43 2624]]\n",
      "Accuracy:  0.6574545454545454\n",
      "Precision:  0.656985478217326\n",
      "Recall:  0.9838770153730784\n",
      "F1:  0.7878696892358504\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  200\n",
      "0.6484848484848484\n",
      "[[  20 1438]\n",
      " [  12 2655]]\n",
      "Accuracy:  0.6484848484848484\n",
      "Precision:  0.6486684583435133\n",
      "Recall:  0.9955005624296963\n",
      "F1:  0.7855029585798816\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  250\n",
      "0.6477575757575758\n",
      "[[  20 1438]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6477575757575758\n",
      "Precision:  0.6484107579462103\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.784963741305313\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  300\n",
      "0.6494545454545455\n",
      "[[  25 1433]\n",
      " [  13 2654]]\n",
      "Accuracy:  0.6494545454545455\n",
      "Precision:  0.6493760704673355\n",
      "Recall:  0.9951256092988376\n",
      "F1:  0.7859046490968316\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  350\n",
      "0.6487272727272727\n",
      "[[  24 1434]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6487272727272727\n",
      "Precision:  0.6490455212922174\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.7854286983562861\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  200\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  250\n",
      "0.6472727272727272\n",
      "[[   4 1454]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6472727272727272\n",
      "Precision:  0.6470873786407767\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7856195668189185\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  300\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  350\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for LOF\n",
    "\n",
    "lof = LOF()\n",
    "\n",
    "n_neighbors = [1500,2000,2500,3000]\n",
    "leaf_sizes = [200,250,300,350]\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for l in leaf_sizes:\n",
    "        lofParams = {\n",
    "            \"n_neighbors\": n,\n",
    "            \"algorithm\": \"auto\",\n",
    "            \"leaf_size\": l,\n",
    "            \"metric\": \"minkowski\",\n",
    "            \"p\": 2,\n",
    "            \"metric_params\": None,\n",
    "            \"contamination\": \"auto\",\n",
    "            \"novelty\": True,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "\n",
    "        hp = Hparams(lof_hparams=lofParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "        (avg, cm) = model_train_eval(lof, data, hp)\n",
    "        print(\"for n_neighbors: \", n, \" leaf_size: \", l)\n",
    "        print(avg)\n",
    "        print(cm)\n",
    "        (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "        print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "        print(\"Precision: \", tp / (tp + fp))\n",
    "        print(\"Recall: \", tp / (tp + fn))\n",
    "        print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "        print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.22790743091494972\n",
      "[[ 112 7907]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21937012538256492\n",
      "Recall:  1.0\n",
      "F1:  0.3598089223544652\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.22820037105751392\n",
      "[[ 116 7903]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22820037105751392\n",
      "Precision:  0.21937969182141445\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597926453912198\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.22800507762913777\n",
      "[[ 113 7906]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22800507762913777\n",
      "Precision:  0.21939178515007898\n",
      "Recall:  1.0\n",
      "F1:  0.35983805668016194\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.22868860462845425\n",
      "[[ 120 7899]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22868860462845425\n",
      "Precision:  0.2195435233672562\n",
      "Recall:  1.0\n",
      "F1:  0.3600421291420238\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.22595449663118836\n",
      "[[  92 7927]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22595449663118836\n",
      "Precision:  0.21893782638683615\n",
      "Recall:  1.0\n",
      "F1:  0.35922722496160375\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.22663802363050484\n",
      "[[  99 7920]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22663802363050484\n",
      "Precision:  0.21908893709327548\n",
      "Recall:  1.0\n",
      "F1:  0.3594306049822064\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.0001\n",
      "0.22790743091494972\n",
      "[[ 113 7906]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21931470326849017\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597052392906308\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.22986036519871106\n",
      "[[ 132 7887]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22986036519871106\n",
      "Precision:  0.21980413492927095\n",
      "Recall:  1.0\n",
      "F1:  0.360392506690455\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.22810272434332585\n",
      "[[ 114 7905]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22810272434332585\n",
      "Precision:  0.21941344919522068\n",
      "Recall:  1.0\n",
      "F1:  0.35986719572435016\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.22546626306024803\n",
      "[[  88 7931]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22546626306024803\n",
      "Precision:  0.2187746256895193\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3589785033133991\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.22585684991700028\n",
      "[[  91 7928]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22585684991700028\n",
      "Precision:  0.21891625615763546\n",
      "Recall:  1.0\n",
      "F1:  0.35919818946007115\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.22634508348794063\n",
      "[[  96 7923]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22634508348794063\n",
      "Precision:  0.21902414982750124\n",
      "Recall:  1.0\n",
      "F1:  0.359343413924153\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.22605214334537643\n",
      "[[  93 7926]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22605214334537643\n",
      "Precision:  0.21895940086716595\n",
      "Recall:  1.0\n",
      "F1:  0.3592562651576395\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.2261497900595645\n",
      "[[  95 7924]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.2261497900595645\n",
      "Precision:  0.21892557910300642\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3591816932158163\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.22761449077238552\n",
      "[[ 110 7909]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22761449077238552\n",
      "Precision:  0.2192497532082922\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3596178756476684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 30\u001B[0m\n\u001B[1;32m     14\u001B[0m ovscmParams \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkernel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdegree\u001B[39m\u001B[38;5;124m\"\u001B[39m: d,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     25\u001B[0m }\n\u001B[1;32m     27\u001B[0m hp \u001B[38;5;241m=\u001B[39m Hparams(ocsvm_hparams\u001B[38;5;241m=\u001B[39movscmParams, anomaly_generation_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     28\u001B[0m              clean_test_data_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m, window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, window_slide\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     29\u001B[0m              one_hot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m (avg, cm) \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_train_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mocsvm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor degree: \u001B[39m\u001B[38;5;124m\"\u001B[39m, d, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m gamma: \u001B[39m\u001B[38;5;124m\"\u001B[39m, g, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m coef0: \u001B[39m\u001B[38;5;124m\"\u001B[39m, c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m tol: \u001B[39m\u001B[38;5;124m\"\u001B[39m, t)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(avg)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/model_validator.py:36\u001B[0m, in \u001B[0;36mmodel_train_eval\u001B[0;34m(model, data, hparams)\u001B[0m\n\u001B[1;32m     33\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m clean_df[:split_index]\n\u001B[1;32m     34\u001B[0m training_df \u001B[38;5;241m=\u001B[39m clean_df[split_index:]\n\u001B[0;32m---> 36\u001B[0m generated_anomalies_df \u001B[38;5;241m=\u001B[39m \u001B[43msynthetic_anomaly_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manomaly_generation_ratio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m anomalous_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([anomalous_df, generated_anomalies_df])\n\u001B[1;32m     39\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m testing_df\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:28\u001B[0m, in \u001B[0;36msynthetic_anomaly_generator\u001B[0;34m(anomalous_windows, anomaly_generation_ratio)\u001B[0m\n\u001B[1;32m     25\u001B[0m anomalous_windows \u001B[38;5;241m=\u001B[39m anomalous_windows\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index_window, window \u001B[38;5;129;01min\u001B[39;00m anomalous_windows\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# Get the activity, reason and type of anomaly window\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     activity, reason, type_anomaly, window \u001B[38;5;241m=\u001B[39m \u001B[43mextract_anomaly_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     new_activity: List[pd\u001B[38;5;241m.\u001B[39mDataFrame] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     30\u001B[0m     new_duration \u001B[38;5;241m=\u001B[39m timedelta(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:63\u001B[0m, in \u001B[0;36mextract_anomaly_details\u001B[0;34m(anomalous_windows, index_window)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_anomaly_details\u001B[39m(anomalous_windows: pd\u001B[38;5;241m.\u001B[39mDataFrame, index_window: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \\\n\u001B[1;32m     55\u001B[0m         Tuple[pd\u001B[38;5;241m.\u001B[39mDataFrame, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m, pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m     56\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    This function will process the window and return the activity, reason and type of anomaly\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m    :param anomalous_windows: the windows to be processed\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    :param index_window: the index of the window to be processed\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    :return: the activity, reason, type of anomaly and the window\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m     window \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     reason \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     65\u001B[0m     type_anomaly \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/frame.py:790\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    781\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m    782\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m nested_data_to_arrays(\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;00m\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    788\u001B[0m         dtype,\n\u001B[1;32m    789\u001B[0m     )\n\u001B[0;32m--> 790\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    798\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m ndarray_to_mgr(\n\u001B[1;32m    799\u001B[0m         data,\n\u001B[1;32m    800\u001B[0m         index,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    804\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[1;32m    805\u001B[0m     )\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:153\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    150\u001B[0m axes \u001B[38;5;241m=\u001B[39m [columns, index]\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_block_manager_from_column_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefs\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2137\u001B[0m, in \u001B[0;36mcreate_block_manager_from_column_arrays\u001B[0;34m(arrays, axes, consolidate, refs)\u001B[0m\n\u001B[1;32m   2119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_block_manager_from_column_arrays\u001B[39m(\n\u001B[1;32m   2120\u001B[0m     arrays: \u001B[38;5;28mlist\u001B[39m[ArrayLike],\n\u001B[1;32m   2121\u001B[0m     axes: \u001B[38;5;28mlist\u001B[39m[Index],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2133\u001B[0m     \u001B[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001B[39;00m\n\u001B[1;32m   2134\u001B[0m     \u001B[38;5;66;03m#  verify_integrity=False below.\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2137\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m \u001B[43m_form_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2138\u001B[0m         mgr \u001B[38;5;241m=\u001B[39m BlockManager(blocks, axes, verify_integrity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2203\u001B[0m, in \u001B[0;36m_form_blocks\u001B[0;34m(arrays, consolidate, refs)\u001B[0m\n\u001B[1;32m   2196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nbs\n\u001B[1;32m   2198\u001B[0m \u001B[38;5;66;03m# when consolidating, we can ignore refs (either stacking always copies,\u001B[39;00m\n\u001B[1;32m   2199\u001B[0m \u001B[38;5;66;03m# or the EA is already copied in the calling dict_to_mgr)\u001B[39;00m\n\u001B[1;32m   2200\u001B[0m \u001B[38;5;66;03m# TODO(CoW) check if this is also valid for rec_array_to_mgr\u001B[39;00m\n\u001B[1;32m   2201\u001B[0m \n\u001B[1;32m   2202\u001B[0m \u001B[38;5;66;03m# group by dtype\u001B[39;00m\n\u001B[0;32m-> 2203\u001B[0m grouper \u001B[38;5;241m=\u001B[39m \u001B[43mitertools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtuples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_grouping_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2205\u001B[0m nbs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   2206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (_, _, dtype), tup_block \u001B[38;5;129;01min\u001B[39;00m grouper:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [2,3,4]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.03566889837384224\n",
      "Epoch 2 training loss: 0.01830665022134781\n",
      "Epoch 3 training loss: 0.013418372720479965\n",
      "Epoch 4 training loss: 0.011778959073126316\n",
      "Epoch 5 training loss: 0.01130879856646061\n",
      "Epoch 6 training loss: 0.011056452058255672\n",
      "Epoch 7 training loss: 0.01079351082444191\n",
      "Epoch 8 training loss: 0.010838769376277924\n",
      "Epoch 9 training loss: 0.010137386620044708\n",
      "Epoch 10 training loss: 0.010338276624679565\n",
      "Epoch 11 training loss: 0.010535934008657932\n",
      "Epoch 12 training loss: 0.009792396798729897\n",
      "Epoch 13 training loss: 0.009921427816152573\n",
      "Epoch 14 training loss: 0.009429894387722015\n",
      "Epoch 15 training loss: 0.008734602481126785\n",
      "Epoch 16 training loss: 0.009123115800321102\n",
      "Epoch 17 training loss: 0.009666664525866508\n",
      "Epoch 18 training loss: 0.008979364298284054\n",
      "Epoch 19 training loss: 0.009243450127542019\n",
      "Epoch 20 training loss: 0.008844000287353992\n",
      "Epoch 21 training loss: 0.00910976342856884\n",
      "Epoch 22 training loss: 0.008840230293571949\n",
      "Epoch 23 training loss: 0.008172215893864632\n",
      "Epoch 24 training loss: 0.009498702362179756\n",
      "Epoch 25 training loss: 0.00898265652358532\n",
      "Epoch 26 training loss: 0.008497258648276329\n",
      "Epoch 27 training loss: 0.008709250018000603\n",
      "Epoch 28 training loss: 0.008624088950455189\n",
      "Epoch 29 training loss: 0.00815357081592083\n",
      "Epoch 30 training loss: 0.009594975039362907\n",
      "Epoch 31 training loss: 0.009273276664316654\n",
      "Epoch 32 training loss: 0.008483963087201118\n",
      "0.8185158722057773\n",
      "[[8019    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.8185158722057773\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/868071132.py:10: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=10, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  100  bootstrap:  True\n",
      "0.4315217391304348\n",
      "[[1457    1]\n",
      " [2091  131]]\n",
      "Accuracy:  0.4315217391304348\n",
      "Precision:  0.9924242424242424\n",
      "Recall:  0.05895589558955896\n",
      "F1:  0.11129991503823279\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  100  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  100  bootstrap:  False\n",
      "0.5076086956521739\n",
      "[[1426   32]\n",
      " [1780  442]]\n",
      "Accuracy:  0.5076086956521739\n",
      "Precision:  0.9324894514767933\n",
      "Recall:  0.19891989198919893\n",
      "F1:  0.327893175074184\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  100  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  200  bootstrap:  True\n",
      "0.40054347826086956\n",
      "[[1458    0]\n",
      " [2206   16]]\n",
      "Accuracy:  0.40054347826086956\n",
      "Precision:  1.0\n",
      "Recall:  0.0072007200720072\n",
      "F1:  0.014298480786416443\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  200  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  200  bootstrap:  False\n",
      "0.41440217391304346\n",
      "[[1454    4]\n",
      " [2151   71]]\n",
      "Accuracy:  0.41440217391304346\n",
      "Precision:  0.9466666666666667\n",
      "Recall:  0.03195319531953195\n",
      "F1:  0.06181976491075316\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  200  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  400  bootstrap:  True\n",
      "0.39918478260869567\n",
      "[[1458    0]\n",
      " [2211   11]]\n",
      "Accuracy:  0.39918478260869567\n",
      "Precision:  1.0\n",
      "Recall:  0.0049504950495049506\n",
      "F1:  0.009852216748768473\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  400  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  400  bootstrap:  False\n",
      "0.4008152173913043\n",
      "[[1458    0]\n",
      " [2205   17]]\n",
      "Accuracy:  0.4008152173913043\n",
      "Precision:  1.0\n",
      "Recall:  0.00765076507650765\n",
      "F1:  0.015185350602947744\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  400  bootstrap:  False\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  800  bootstrap:  True\n",
      "0.4016304347826087\n",
      "[[1458    0]\n",
      " [2202   20]]\n",
      "Accuracy:  0.4016304347826087\n",
      "Precision:  1.0\n",
      "Recall:  0.009000900090009001\n",
      "F1:  0.01784121320249777\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_estimators:  800  bootstrap:  True\n",
      "0.39619565217391306\n",
      "[[1458    0]\n",
      " [2222    0]]\n",
      "Accuracy:  0.39619565217391306\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/876659177.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "for n_estimators:  800  bootstrap:  False\n",
      "0.3967391304347826\n",
      "[[1458    0]\n",
      " [2220    2]]\n",
      "Accuracy:  0.3967391304347826\n",
      "Precision:  1.0\n",
      "Recall:  0.0009000900090009\n",
      "F1:  0.0017985611510791368\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [100, 200, 400, 800]\n",
    "bootstraps = [True, False]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "    for b in bootstraps:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"bootstrap\": b,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.04122960567474365\n",
      "Epoch 2 training loss: 0.032301533967256546\n",
      "Epoch 3 training loss: 0.029458846896886826\n",
      "Epoch 4 training loss: 0.029231302440166473\n",
      "Epoch 5 training loss: 0.026410769671201706\n",
      "Epoch 6 training loss: 0.024050837382674217\n",
      "Epoch 7 training loss: 0.02383429929614067\n",
      "Epoch 8 training loss: 0.02304689958691597\n",
      "Epoch 9 training loss: 0.023099087178707123\n",
      "Epoch 10 training loss: 0.022938374429941177\n",
      "Epoch 11 training loss: 0.021551033481955528\n",
      "Epoch 12 training loss: 0.020283041521906853\n",
      "Epoch 13 training loss: 0.019526956602931023\n",
      "Epoch 14 training loss: 0.018245210871100426\n",
      "Epoch 15 training loss: 0.01643492467701435\n",
      "Epoch 16 training loss: 0.01508433185517788\n",
      "Epoch 17 training loss: 0.014789544977247715\n",
      "Epoch 18 training loss: 0.014925693161785603\n",
      "Epoch 19 training loss: 0.013959605246782303\n",
      "Epoch 20 training loss: 0.014511987566947937\n",
      "Epoch 21 training loss: 0.014036488719284534\n",
      "Epoch 22 training loss: 0.01356128416955471\n",
      "Epoch 23 training loss: 0.013709161430597305\n",
      "Epoch 24 training loss: 0.014896780252456665\n",
      "Epoch 25 training loss: 0.013893967494368553\n",
      "Epoch 26 training loss: 0.013346045278012753\n",
      "Epoch 27 training loss: 0.013873269781470299\n",
      "Epoch 28 training loss: 0.013923469930887222\n",
      "Epoch 29 training loss: 0.013328543864190578\n",
      "Epoch 30 training loss: 0.013698993250727654\n",
      "Epoch 31 training loss: 0.013894200325012207\n",
      "Epoch 32 training loss: 0.01420495193451643\n",
      "0.7647831800262812\n",
      "[[8730   18]\n",
      " [2667    0]]\n",
      "Accuracy: 0.7647831800262812\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 24, 16, 8], [8, 16, 24, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=11, clean_test_data_ratio=0.3, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.058898258954286575\n",
      "Epoch 2 training loss: 0.05155443772673607\n",
      "Epoch 3 training loss: 0.05057140812277794\n",
      "Epoch 4 training loss: 0.04858234152197838\n",
      "Epoch 5 training loss: 0.04724820703268051\n",
      "Epoch 6 training loss: 0.04657506197690964\n",
      "Epoch 7 training loss: 0.045772116631269455\n",
      "Epoch 8 training loss: 0.045489318668842316\n",
      "Epoch 9 training loss: 0.045221779495477676\n",
      "Epoch 10 training loss: 0.04497319832444191\n",
      "Epoch 11 training loss: 0.043941523879766464\n",
      "Epoch 12 training loss: 0.04381180927157402\n",
      "Epoch 13 training loss: 0.04358973726630211\n",
      "Epoch 14 training loss: 0.043723657727241516\n",
      "Epoch 15 training loss: 0.04434793069958687\n",
      "Epoch 16 training loss: 0.04259477183222771\n",
      "Epoch 17 training loss: 0.04313371703028679\n",
      "Epoch 18 training loss: 0.04417560249567032\n",
      "Epoch 19 training loss: 0.04458608478307724\n",
      "Epoch 20 training loss: 0.04394195228815079\n",
      "Epoch 21 training loss: 0.04413234442472458\n",
      "Epoch 22 training loss: 0.04372331127524376\n",
      "Epoch 23 training loss: 0.04293721169233322\n",
      "Epoch 24 training loss: 0.04323875531554222\n",
      "Epoch 25 training loss: 0.042636580765247345\n",
      "Epoch 26 training loss: 0.042571090161800385\n",
      "Epoch 27 training loss: 0.042420923709869385\n",
      "Epoch 28 training loss: 0.042121902108192444\n",
      "Epoch 29 training loss: 0.04230155050754547\n",
      "Epoch 30 training loss: 0.04195545241236687\n",
      "Epoch 31 training loss: 0.04092766344547272\n",
      "Epoch 32 training loss: 0.04151569679379463\n",
      "Epoch 33 training loss: 0.040453843772411346\n",
      "Epoch 34 training loss: 0.04354794695973396\n",
      "Epoch 35 training loss: 0.040512025356292725\n",
      "Epoch 36 training loss: 0.040154166519641876\n",
      "Epoch 37 training loss: 0.04132810980081558\n",
      "Epoch 38 training loss: 0.040305864065885544\n",
      "Epoch 39 training loss: 0.04092114418745041\n",
      "Epoch 40 training loss: 0.04208670184016228\n",
      "Epoch 41 training loss: 0.04183347523212433\n",
      "Epoch 42 training loss: 0.04109683260321617\n",
      "Epoch 43 training loss: 0.04025791957974434\n",
      "Epoch 44 training loss: 0.03995497524738312\n",
      "Epoch 45 training loss: 0.04097607359290123\n",
      "Epoch 46 training loss: 0.05953291058540344\n",
      "Epoch 47 training loss: 0.07895916700363159\n",
      "Epoch 48 training loss: 0.06983033567667007\n",
      "Epoch 49 training loss: 0.06246565654873848\n",
      "Epoch 50 training loss: 0.04813380911946297\n",
      "Epoch 51 training loss: 0.0453314334154129\n",
      "Epoch 52 training loss: 0.04376808926463127\n",
      "Epoch 53 training loss: 0.042266298085451126\n",
      "Epoch 54 training loss: 0.053543899208307266\n",
      "Epoch 55 training loss: 0.06179076060652733\n",
      "Epoch 56 training loss: 0.05984741076827049\n",
      "Epoch 57 training loss: 0.061102330684661865\n",
      "Epoch 58 training loss: 0.05008704960346222\n",
      "Epoch 59 training loss: 0.04680133983492851\n",
      "Epoch 60 training loss: 0.045489221811294556\n",
      "Epoch 61 training loss: 0.045494046062231064\n",
      "Epoch 62 training loss: 0.045764144510030746\n",
      "Epoch 63 training loss: 0.0467325896024704\n",
      "Epoch 64 training loss: 0.04569430276751518\n",
      "0.47950589556428974\n",
      "[[2888 5131]\n",
      " [ 431 2236]]\n",
      "Accuracy: 0.47950589556428974\n",
      "Precision:  0.30351567802361884\n",
      "Recall:  0.838395200599925\n",
      "F1:  0.44568467211480967\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05829282104969025\n",
      "Epoch 2 training loss: 0.05097535252571106\n",
      "Epoch 3 training loss: 0.0498952753841877\n",
      "Epoch 4 training loss: 0.04790065810084343\n",
      "Epoch 5 training loss: 0.04763051122426987\n",
      "Epoch 6 training loss: 0.0501713901758194\n",
      "Epoch 7 training loss: 0.05156087875366211\n",
      "Epoch 8 training loss: 0.04860074445605278\n",
      "Epoch 9 training loss: 0.04710253328084946\n",
      "Epoch 10 training loss: 0.04843513295054436\n",
      "Epoch 11 training loss: 0.04762199521064758\n",
      "Epoch 12 training loss: 0.0470193512737751\n",
      "Epoch 13 training loss: 0.04710486903786659\n",
      "Epoch 14 training loss: 0.04916980862617493\n",
      "Epoch 15 training loss: 0.047472093254327774\n",
      "Epoch 16 training loss: 0.045982830226421356\n",
      "Epoch 17 training loss: 0.047718703746795654\n",
      "Epoch 18 training loss: 0.046799905598163605\n",
      "Epoch 19 training loss: 0.0502435639500618\n",
      "Epoch 20 training loss: 0.05604420229792595\n",
      "Epoch 21 training loss: 0.05407804250717163\n",
      "Epoch 22 training loss: 0.05319788679480553\n",
      "Epoch 23 training loss: 0.05286208540201187\n",
      "Epoch 24 training loss: 0.05242634192109108\n",
      "Epoch 25 training loss: 0.051848530769348145\n",
      "Epoch 26 training loss: 0.05164097994565964\n",
      "Epoch 27 training loss: 0.05050581693649292\n",
      "Epoch 28 training loss: 0.05016383156180382\n",
      "Epoch 29 training loss: 0.05007558315992355\n",
      "Epoch 30 training loss: 0.051285270601511\n",
      "Epoch 31 training loss: 0.07751145958900452\n",
      "Epoch 32 training loss: 0.0772283524274826\n",
      "Epoch 33 training loss: 0.07716844975948334\n",
      "Epoch 34 training loss: 0.07719416171312332\n",
      "Epoch 35 training loss: 0.07715099304914474\n",
      "Epoch 36 training loss: 0.07715228199958801\n",
      "Epoch 37 training loss: 0.07712981849908829\n",
      "Epoch 38 training loss: 0.0771409124135971\n",
      "Epoch 39 training loss: 0.07716117054224014\n",
      "Epoch 40 training loss: 0.07712095975875854\n",
      "Epoch 41 training loss: 0.07708992063999176\n",
      "Epoch 42 training loss: 0.07712382078170776\n",
      "Epoch 43 training loss: 0.07712587714195251\n",
      "Epoch 44 training loss: 0.07708938419818878\n",
      "Epoch 45 training loss: 0.07708832621574402\n",
      "Epoch 46 training loss: 0.07708758115768433\n",
      "Epoch 47 training loss: 0.07712376862764359\n",
      "Epoch 48 training loss: 0.07710060477256775\n",
      "Epoch 49 training loss: 0.07710567861795425\n",
      "Epoch 50 training loss: 0.07708684355020523\n",
      "Epoch 51 training loss: 0.07705388963222504\n",
      "Epoch 52 training loss: 0.07705193758010864\n",
      "Epoch 53 training loss: 0.07709895074367523\n",
      "Epoch 54 training loss: 0.07709212601184845\n",
      "Epoch 55 training loss: 0.07709264755249023\n",
      "Epoch 56 training loss: 0.07708374410867691\n",
      "Epoch 57 training loss: 0.07709158211946487\n",
      "Epoch 58 training loss: 0.07708431780338287\n",
      "Epoch 59 training loss: 0.07708100229501724\n",
      "Epoch 60 training loss: 0.0770869180560112\n",
      "Epoch 61 training loss: 0.07706017047166824\n",
      "Epoch 62 training loss: 0.07704704254865646\n",
      "Epoch 63 training loss: 0.07706429064273834\n",
      "Epoch 64 training loss: 0.07704580575227737\n",
      "Epoch 65 training loss: 0.07707887887954712\n",
      "Epoch 66 training loss: 0.07708815485239029\n",
      "Epoch 67 training loss: 0.07705943286418915\n",
      "Epoch 68 training loss: 0.07705625891685486\n",
      "Epoch 69 training loss: 0.07703600078821182\n",
      "Epoch 70 training loss: 0.07705473899841309\n",
      "Epoch 71 training loss: 0.07705537974834442\n",
      "Epoch 72 training loss: 0.07704281806945801\n",
      "Epoch 73 training loss: 0.07704969495534897\n",
      "Epoch 74 training loss: 0.07704239338636398\n",
      "Epoch 75 training loss: 0.07703730463981628\n",
      "Epoch 76 training loss: 0.07702906429767609\n",
      "Epoch 77 training loss: 0.07700634002685547\n",
      "Epoch 78 training loss: 0.07702942937612534\n",
      "Epoch 79 training loss: 0.07702913880348206\n",
      "Epoch 80 training loss: 0.07703971117734909\n",
      "Epoch 81 training loss: 0.0770302414894104\n",
      "Epoch 82 training loss: 0.07702948153018951\n",
      "Epoch 83 training loss: 0.07702755928039551\n",
      "Epoch 84 training loss: 0.07703575491905212\n",
      "Epoch 85 training loss: 0.07703883945941925\n",
      "Epoch 86 training loss: 0.07702811062335968\n",
      "Epoch 87 training loss: 0.07703310996294022\n",
      "Epoch 88 training loss: 0.07703188061714172\n",
      "Epoch 89 training loss: 0.0770219936966896\n",
      "Epoch 90 training loss: 0.07701282203197479\n",
      "Epoch 91 training loss: 0.07702331244945526\n",
      "Epoch 92 training loss: 0.07702778279781342\n",
      "Epoch 93 training loss: 0.07703300565481186\n",
      "Epoch 94 training loss: 0.07703817635774612\n",
      "Epoch 95 training loss: 0.07702020555734634\n",
      "Epoch 96 training loss: 0.07703542709350586\n",
      "Epoch 97 training loss: 0.07701947540044785\n",
      "Epoch 98 training loss: 0.07702722400426865\n",
      "Epoch 99 training loss: 0.07701949775218964\n",
      "Epoch 100 training loss: 0.07704021781682968\n",
      "Epoch 101 training loss: 0.07701585441827774\n",
      "Epoch 102 training loss: 0.07704055309295654\n",
      "Epoch 103 training loss: 0.07703545689582825\n",
      "Epoch 104 training loss: 0.07702822238206863\n",
      "Epoch 105 training loss: 0.07702390849590302\n",
      "Epoch 106 training loss: 0.07703152298927307\n",
      "Epoch 107 training loss: 0.07702185958623886\n",
      "Epoch 108 training loss: 0.07702432572841644\n",
      "Epoch 109 training loss: 0.07702384889125824\n",
      "Epoch 110 training loss: 0.07701688259840012\n",
      "Epoch 111 training loss: 0.07702643424272537\n",
      "Epoch 112 training loss: 0.07702019810676575\n",
      "Epoch 113 training loss: 0.07704506069421768\n",
      "Epoch 114 training loss: 0.07703159004449844\n",
      "Epoch 115 training loss: 0.07703684270381927\n",
      "Epoch 116 training loss: 0.07701458781957626\n",
      "Epoch 117 training loss: 0.07704203575849533\n",
      "Epoch 118 training loss: 0.07702761888504028\n",
      "Epoch 119 training loss: 0.07701722532510757\n",
      "Epoch 120 training loss: 0.07703814655542374\n",
      "Epoch 121 training loss: 0.07704107463359833\n",
      "Epoch 122 training loss: 0.0770290419459343\n",
      "Epoch 123 training loss: 0.07702631503343582\n",
      "Epoch 124 training loss: 0.07701708376407623\n",
      "Epoch 125 training loss: 0.07703052461147308\n",
      "Epoch 126 training loss: 0.07702711969614029\n",
      "Epoch 127 training loss: 0.07702083885669708\n",
      "Epoch 128 training loss: 0.07702898234128952\n",
      "Epoch 129 training loss: 0.07703835517168045\n",
      "Epoch 130 training loss: 0.07700995355844498\n",
      "Epoch 131 training loss: 0.07702820748090744\n",
      "Epoch 132 training loss: 0.07702609151601791\n",
      "Epoch 133 training loss: 0.07702130079269409\n",
      "Epoch 134 training loss: 0.07702707499265671\n",
      "Epoch 135 training loss: 0.07702576369047165\n",
      "Epoch 136 training loss: 0.07702488452196121\n",
      "Epoch 137 training loss: 0.07699967920780182\n",
      "Epoch 138 training loss: 0.07703471183776855\n",
      "Epoch 139 training loss: 0.07701798528432846\n",
      "Epoch 140 training loss: 0.07703015208244324\n",
      "Epoch 141 training loss: 0.07700847834348679\n",
      "Epoch 142 training loss: 0.07703274488449097\n",
      "Epoch 143 training loss: 0.0770258828997612\n",
      "Epoch 144 training loss: 0.07701438665390015\n",
      "Epoch 145 training loss: 0.07703520357608795\n",
      "Epoch 146 training loss: 0.07702981680631638\n",
      "Epoch 147 training loss: 0.07703104615211487\n",
      "Epoch 148 training loss: 0.07702022045850754\n",
      "Epoch 149 training loss: 0.07701580226421356\n",
      "Epoch 150 training loss: 0.07703147828578949\n",
      "Epoch 151 training loss: 0.07703186571598053\n",
      "Epoch 152 training loss: 0.07701794803142548\n",
      "Epoch 153 training loss: 0.07702413946390152\n",
      "Epoch 154 training loss: 0.07703357934951782\n",
      "Epoch 155 training loss: 0.07702314108610153\n",
      "Epoch 156 training loss: 0.07703682780265808\n",
      "Epoch 157 training loss: 0.07703470438718796\n",
      "Epoch 158 training loss: 0.07702498137950897\n",
      "Epoch 159 training loss: 0.07701396942138672\n",
      "Epoch 160 training loss: 0.07702178508043289\n",
      "Epoch 161 training loss: 0.07700908929109573\n",
      "Epoch 162 training loss: 0.07701052725315094\n",
      "Epoch 163 training loss: 0.0770343616604805\n",
      "Epoch 164 training loss: 0.07701237499713898\n",
      "Epoch 165 training loss: 0.07701359689235687\n",
      "Epoch 166 training loss: 0.07702820003032684\n",
      "Epoch 167 training loss: 0.07703246176242828\n",
      "Epoch 168 training loss: 0.07702775299549103\n",
      "Epoch 169 training loss: 0.07704504579305649\n",
      "Epoch 170 training loss: 0.07701685279607773\n",
      "Epoch 171 training loss: 0.07703884690999985\n",
      "Epoch 172 training loss: 0.07703987509012222\n",
      "Epoch 173 training loss: 0.07701688259840012\n",
      "Epoch 174 training loss: 0.07703504711389542\n",
      "Epoch 175 training loss: 0.07703150063753128\n",
      "Epoch 176 training loss: 0.07702753692865372\n",
      "Epoch 177 training loss: 0.07702451944351196\n",
      "Epoch 178 training loss: 0.07702696323394775\n",
      "Epoch 179 training loss: 0.07703723013401031\n",
      "Epoch 180 training loss: 0.07703053206205368\n",
      "Epoch 181 training loss: 0.0770321935415268\n",
      "Epoch 182 training loss: 0.07702634483575821\n",
      "Epoch 183 training loss: 0.07703079283237457\n",
      "Epoch 184 training loss: 0.07704038918018341\n",
      "Epoch 185 training loss: 0.07702688872814178\n",
      "Epoch 186 training loss: 0.07701125741004944\n",
      "Epoch 187 training loss: 0.07702894508838654\n",
      "Epoch 188 training loss: 0.07702581584453583\n",
      "Epoch 189 training loss: 0.07704629749059677\n",
      "Epoch 190 training loss: 0.07702954113483429\n",
      "Epoch 191 training loss: 0.07702025026082993\n",
      "Epoch 192 training loss: 0.07700329273939133\n",
      "Epoch 193 training loss: 0.07702518999576569\n",
      "Epoch 194 training loss: 0.07702670246362686\n",
      "Epoch 195 training loss: 0.07702674716711044\n",
      "Epoch 196 training loss: 0.07700877636671066\n",
      "Epoch 197 training loss: 0.07702724635601044\n",
      "Epoch 198 training loss: 0.07703027129173279\n",
      "Epoch 199 training loss: 0.07702340930700302\n",
      "Epoch 200 training loss: 0.07702601701021194\n",
      "Epoch 201 training loss: 0.07704586535692215\n",
      "Epoch 202 training loss: 0.07702969014644623\n",
      "Epoch 203 training loss: 0.07702009379863739\n",
      "Epoch 204 training loss: 0.0770278349518776\n",
      "Epoch 205 training loss: 0.07703326642513275\n",
      "Epoch 206 training loss: 0.07701940834522247\n",
      "Epoch 207 training loss: 0.07703382521867752\n",
      "Epoch 208 training loss: 0.07702875137329102\n",
      "Epoch 209 training loss: 0.07701776176691055\n",
      "Epoch 210 training loss: 0.07703078538179398\n",
      "Epoch 211 training loss: 0.07701732963323593\n",
      "Epoch 212 training loss: 0.07701758295297623\n",
      "Epoch 213 training loss: 0.07702015340328217\n",
      "Epoch 214 training loss: 0.0770338624715805\n",
      "Epoch 215 training loss: 0.07702308148145676\n",
      "Epoch 216 training loss: 0.07702900469303131\n",
      "Epoch 217 training loss: 0.07703094929456711\n",
      "Epoch 218 training loss: 0.0770266205072403\n",
      "Epoch 219 training loss: 0.07702970504760742\n",
      "Epoch 220 training loss: 0.07701800018548965\n",
      "Epoch 221 training loss: 0.07701873034238815\n",
      "Epoch 222 training loss: 0.07701938599348068\n",
      "Epoch 223 training loss: 0.07697942852973938\n",
      "Epoch 224 training loss: 0.07701562345027924\n",
      "Epoch 225 training loss: 0.07702654600143433\n",
      "Epoch 226 training loss: 0.0770338699221611\n",
      "Epoch 227 training loss: 0.07702809572219849\n",
      "Epoch 228 training loss: 0.0770164206624031\n",
      "Epoch 229 training loss: 0.07703149318695068\n",
      "Epoch 230 training loss: 0.07700888812541962\n",
      "Epoch 231 training loss: 0.07704063504934311\n",
      "Epoch 232 training loss: 0.07702436298131943\n",
      "Epoch 233 training loss: 0.07703782618045807\n",
      "Epoch 234 training loss: 0.07704062759876251\n",
      "Epoch 235 training loss: 0.07703305780887604\n",
      "Epoch 236 training loss: 0.07702900469303131\n",
      "Epoch 237 training loss: 0.07701142132282257\n",
      "Epoch 238 training loss: 0.07702313363552094\n",
      "Epoch 239 training loss: 0.07703787833452225\n",
      "Epoch 240 training loss: 0.07701330631971359\n",
      "Epoch 241 training loss: 0.07703802734613419\n",
      "Epoch 242 training loss: 0.07701446861028671\n",
      "Epoch 243 training loss: 0.07702381908893585\n",
      "Epoch 244 training loss: 0.07703627645969391\n",
      "Epoch 245 training loss: 0.07703565061092377\n",
      "Epoch 246 training loss: 0.0770408883690834\n",
      "Epoch 247 training loss: 0.07700607180595398\n",
      "Epoch 248 training loss: 0.07702819257974625\n",
      "Epoch 249 training loss: 0.07703020423650742\n",
      "Epoch 250 training loss: 0.07705322653055191\n",
      "Epoch 251 training loss: 0.07701022922992706\n",
      "Epoch 252 training loss: 0.07701870799064636\n",
      "Epoch 253 training loss: 0.07703284174203873\n",
      "Epoch 254 training loss: 0.07703067362308502\n",
      "Epoch 255 training loss: 0.07703249156475067\n",
      "Epoch 256 training loss: 0.07703877985477448\n",
      "0.6038787878787879\n",
      "[[1338  120]\n",
      " [1514 1153]]\n",
      "Accuracy: 0.6038787878787879\n",
      "Precision:  0.9057344854673999\n",
      "Recall:  0.432320959880015\n",
      "F1:  0.5852791878172589\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07081352174282074\n",
      "Epoch 2 training loss: 0.05177604779601097\n",
      "Epoch 3 training loss: 0.04481825605034828\n",
      "Epoch 4 training loss: 0.042032934725284576\n",
      "Epoch 5 training loss: 0.03975117206573486\n",
      "Epoch 6 training loss: 0.036157939583063126\n",
      "Epoch 7 training loss: 0.03292706608772278\n",
      "Epoch 8 training loss: 0.030977612361311913\n",
      "Epoch 9 training loss: 0.029723944142460823\n",
      "Epoch 10 training loss: 0.028918273746967316\n",
      "Epoch 11 training loss: 0.02801123820245266\n",
      "Epoch 12 training loss: 0.027473557740449905\n",
      "Epoch 13 training loss: 0.02687661163508892\n",
      "Epoch 14 training loss: 0.026449065655469894\n",
      "Epoch 15 training loss: 0.02608298324048519\n",
      "Epoch 16 training loss: 0.02545854263007641\n",
      "Epoch 17 training loss: 0.025329427793622017\n",
      "Epoch 18 training loss: 0.024881193414330482\n",
      "Epoch 19 training loss: 0.024671414867043495\n",
      "Epoch 20 training loss: 0.024505024775862694\n",
      "Epoch 21 training loss: 0.024299871176481247\n",
      "Epoch 22 training loss: 0.02397942915558815\n",
      "Epoch 23 training loss: 0.023837454617023468\n",
      "Epoch 24 training loss: 0.023697637021541595\n",
      "Epoch 25 training loss: 0.023405183106660843\n",
      "Epoch 26 training loss: 0.02320265583693981\n",
      "Epoch 27 training loss: 0.023119444027543068\n",
      "Epoch 28 training loss: 0.02296951785683632\n",
      "Epoch 29 training loss: 0.022724036127328873\n",
      "Epoch 30 training loss: 0.02265932783484459\n",
      "Epoch 31 training loss: 0.022531751543283463\n",
      "Epoch 32 training loss: 0.022266661748290062\n",
      "Epoch 33 training loss: 0.022111665457487106\n",
      "Epoch 34 training loss: 0.021976502612233162\n",
      "Epoch 35 training loss: 0.021871916949748993\n",
      "Epoch 36 training loss: 0.0215756855905056\n",
      "Epoch 37 training loss: 0.021504340693354607\n",
      "Epoch 38 training loss: 0.021440714597702026\n",
      "Epoch 39 training loss: 0.0213240385055542\n",
      "Epoch 40 training loss: 0.02123262733221054\n",
      "Epoch 41 training loss: 0.020975656807422638\n",
      "Epoch 42 training loss: 0.020951854065060616\n",
      "Epoch 43 training loss: 0.020735614001750946\n",
      "Epoch 44 training loss: 0.020570144057273865\n",
      "Epoch 45 training loss: 0.0205476526170969\n",
      "Epoch 46 training loss: 0.0204680897295475\n",
      "Epoch 47 training loss: 0.020305106416344643\n",
      "Epoch 48 training loss: 0.020412979647517204\n",
      "Epoch 49 training loss: 0.020313970744609833\n",
      "Epoch 50 training loss: 0.019998932257294655\n",
      "Epoch 51 training loss: 0.01995219476521015\n",
      "Epoch 52 training loss: 0.01997661218047142\n",
      "Epoch 53 training loss: 0.01973620429635048\n",
      "Epoch 54 training loss: 0.019737521186470985\n",
      "Epoch 55 training loss: 0.019749490544199944\n",
      "Epoch 56 training loss: 0.019465280696749687\n",
      "Epoch 57 training loss: 0.01959570124745369\n",
      "Epoch 58 training loss: 0.019428182393312454\n",
      "Epoch 59 training loss: 0.01931731030344963\n",
      "Epoch 60 training loss: 0.019147686660289764\n",
      "Epoch 61 training loss: 0.01914028264582157\n",
      "Epoch 62 training loss: 0.01894281432032585\n",
      "Epoch 63 training loss: 0.018742889165878296\n",
      "Epoch 64 training loss: 0.018763482570648193\n",
      "0.6392727272727273\n",
      "[[ 651  807]\n",
      " [ 681 1986]]\n",
      "Accuracy: 0.6392727272727273\n",
      "Precision:  0.7110633727175081\n",
      "Recall:  0.7446569178852643\n",
      "F1:  0.7274725274725274\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.1696965992450714\n",
      "Epoch 2 training loss: 0.16787873208522797\n",
      "Epoch 3 training loss: 0.1669291853904724\n",
      "Epoch 4 training loss: 0.1668277382850647\n",
      "Epoch 5 training loss: 0.16672559082508087\n",
      "Epoch 6 training loss: 0.16642449796199799\n",
      "Epoch 7 training loss: 0.16632813215255737\n",
      "Epoch 8 training loss: 0.1662880927324295\n",
      "Epoch 9 training loss: 0.1662030816078186\n",
      "Epoch 10 training loss: 0.16611048579216003\n",
      "Epoch 11 training loss: 0.1660909354686737\n",
      "Epoch 12 training loss: 0.1659357100725174\n",
      "Epoch 13 training loss: 0.16550952196121216\n",
      "Epoch 14 training loss: 0.1651533991098404\n",
      "Epoch 15 training loss: 0.16507366299629211\n",
      "Epoch 16 training loss: 0.1650806963443756\n",
      "Epoch 17 training loss: 0.1650279462337494\n",
      "Epoch 18 training loss: 0.16502952575683594\n",
      "Epoch 19 training loss: 0.16496992111206055\n",
      "Epoch 20 training loss: 0.1649792492389679\n",
      "Epoch 21 training loss: 0.1649915724992752\n",
      "Epoch 22 training loss: 0.16495685279369354\n",
      "Epoch 23 training loss: 0.16488680243492126\n",
      "Epoch 24 training loss: 0.16492563486099243\n",
      "Epoch 25 training loss: 0.16487309336662292\n",
      "Epoch 26 training loss: 0.16495220363140106\n",
      "Epoch 27 training loss: 0.1648828238248825\n",
      "Epoch 28 training loss: 0.1648811399936676\n",
      "Epoch 29 training loss: 0.16485434770584106\n",
      "Epoch 30 training loss: 0.1648762971162796\n",
      "Epoch 31 training loss: 0.16476258635520935\n",
      "Epoch 32 training loss: 0.16461719572544098\n",
      "Epoch 33 training loss: 0.16450954973697662\n",
      "Epoch 34 training loss: 0.164473295211792\n",
      "Epoch 35 training loss: 0.1645013988018036\n",
      "Epoch 36 training loss: 0.16450057923793793\n",
      "Epoch 37 training loss: 0.16447032988071442\n",
      "Epoch 38 training loss: 0.16444745659828186\n",
      "Epoch 39 training loss: 0.16443201899528503\n",
      "Epoch 40 training loss: 0.16440320014953613\n",
      "Epoch 41 training loss: 0.16439244151115417\n",
      "Epoch 42 training loss: 0.1643671691417694\n",
      "Epoch 43 training loss: 0.16440723836421967\n",
      "Epoch 44 training loss: 0.1643637865781784\n",
      "Epoch 45 training loss: 0.16430599987506866\n",
      "Epoch 46 training loss: 0.16434316337108612\n",
      "Epoch 47 training loss: 0.1643010675907135\n",
      "Epoch 48 training loss: 0.1642955094575882\n",
      "Epoch 49 training loss: 0.16434188187122345\n",
      "Epoch 50 training loss: 0.16427671909332275\n",
      "Epoch 51 training loss: 0.16429077088832855\n",
      "Epoch 52 training loss: 0.16422796249389648\n",
      "Epoch 53 training loss: 0.1642664074897766\n",
      "Epoch 54 training loss: 0.16420139372348785\n",
      "Epoch 55 training loss: 0.16420480608940125\n",
      "Epoch 56 training loss: 0.16417406499385834\n",
      "Epoch 57 training loss: 0.16414174437522888\n",
      "Epoch 58 training loss: 0.16417136788368225\n",
      "Epoch 59 training loss: 0.16415856778621674\n",
      "Epoch 60 training loss: 0.16417217254638672\n",
      "Epoch 61 training loss: 0.16416549682617188\n",
      "Epoch 62 training loss: 0.16421039402484894\n",
      "Epoch 63 training loss: 0.16415011882781982\n",
      "Epoch 64 training loss: 0.16410236060619354\n",
      "0.47393939393939394\n",
      "[[ 363 1095]\n",
      " [1075 1592]]\n",
      "Accuracy: 0.47393939393939394\n",
      "Precision:  0.5924823222925195\n",
      "Recall:  0.5969253843269592\n",
      "F1:  0.5946955547254389\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17117565870285034\n",
      "Epoch 2 training loss: 0.17089219391345978\n",
      "Epoch 3 training loss: 0.17088712751865387\n",
      "Epoch 4 training loss: 0.17088398337364197\n",
      "Epoch 5 training loss: 0.17059989273548126\n",
      "Epoch 6 training loss: 0.16920214891433716\n",
      "Epoch 7 training loss: 0.16828873753547668\n",
      "Epoch 8 training loss: 0.1681583672761917\n",
      "Epoch 9 training loss: 0.1680826097726822\n",
      "Epoch 10 training loss: 0.1680685132741928\n",
      "Epoch 11 training loss: 0.16803313791751862\n",
      "Epoch 12 training loss: 0.16800935566425323\n",
      "Epoch 13 training loss: 0.168057382106781\n",
      "Epoch 14 training loss: 0.1680016666650772\n",
      "Epoch 15 training loss: 0.16803009808063507\n",
      "Epoch 16 training loss: 0.16799914836883545\n",
      "Epoch 17 training loss: 0.16800348460674286\n",
      "Epoch 18 training loss: 0.16800744831562042\n",
      "Epoch 19 training loss: 0.16804924607276917\n",
      "Epoch 20 training loss: 0.16799481213092804\n",
      "Epoch 21 training loss: 0.16799065470695496\n",
      "Epoch 22 training loss: 0.1679990142583847\n",
      "Epoch 23 training loss: 0.1679784655570984\n",
      "Epoch 24 training loss: 0.16798534989356995\n",
      "Epoch 25 training loss: 0.16797421872615814\n",
      "Epoch 26 training loss: 0.16798251867294312\n",
      "Epoch 27 training loss: 0.16802871227264404\n",
      "Epoch 28 training loss: 0.1679912656545639\n",
      "Epoch 29 training loss: 0.16801121830940247\n",
      "Epoch 30 training loss: 0.16802790760993958\n",
      "Epoch 31 training loss: 0.1680411547422409\n",
      "Epoch 32 training loss: 0.16803418099880219\n",
      "Epoch 33 training loss: 0.16799606382846832\n",
      "Epoch 34 training loss: 0.16797992587089539\n",
      "Epoch 35 training loss: 0.16796360909938812\n",
      "Epoch 36 training loss: 0.16796182096004486\n",
      "Epoch 37 training loss: 0.16797949373722076\n",
      "Epoch 38 training loss: 0.16797710955142975\n",
      "Epoch 39 training loss: 0.1680811047554016\n",
      "Epoch 40 training loss: 0.16799592971801758\n",
      "Epoch 41 training loss: 0.16800850629806519\n",
      "Epoch 42 training loss: 0.168021097779274\n",
      "Epoch 43 training loss: 0.1679965853691101\n",
      "Epoch 44 training loss: 0.1680385172367096\n",
      "Epoch 45 training loss: 0.1679650992155075\n",
      "Epoch 46 training loss: 0.16801804304122925\n",
      "Epoch 47 training loss: 0.1679927110671997\n",
      "Epoch 48 training loss: 0.16796480119228363\n",
      "Epoch 49 training loss: 0.16796045005321503\n",
      "Epoch 50 training loss: 0.16797663271427155\n",
      "Epoch 51 training loss: 0.16806751489639282\n",
      "Epoch 52 training loss: 0.16799931228160858\n",
      "Epoch 53 training loss: 0.16803543269634247\n",
      "Epoch 54 training loss: 0.1680329442024231\n",
      "Epoch 55 training loss: 0.16805870831012726\n",
      "Epoch 56 training loss: 0.1680757999420166\n",
      "Epoch 57 training loss: 0.16799744963645935\n",
      "Epoch 58 training loss: 0.16798177361488342\n",
      "Epoch 59 training loss: 0.16800926625728607\n",
      "Epoch 60 training loss: 0.1679632067680359\n",
      "Epoch 61 training loss: 0.16794443130493164\n",
      "Epoch 62 training loss: 0.16792601346969604\n",
      "Epoch 63 training loss: 0.16794176399707794\n",
      "Epoch 64 training loss: 0.16794586181640625\n",
      "Epoch 65 training loss: 0.16797718405723572\n",
      "Epoch 66 training loss: 0.16795514523983002\n",
      "Epoch 67 training loss: 0.1679099053144455\n",
      "Epoch 68 training loss: 0.16792942583560944\n",
      "Epoch 69 training loss: 0.16796092689037323\n",
      "Epoch 70 training loss: 0.16789768636226654\n",
      "Epoch 71 training loss: 0.167667955160141\n",
      "Epoch 72 training loss: 0.16696929931640625\n",
      "Epoch 73 training loss: 0.16685323417186737\n",
      "Epoch 74 training loss: 0.1667962372303009\n",
      "Epoch 75 training loss: 0.16677238047122955\n",
      "Epoch 76 training loss: 0.16678297519683838\n",
      "Epoch 77 training loss: 0.16672664880752563\n",
      "Epoch 78 training loss: 0.16673199832439423\n",
      "Epoch 79 training loss: 0.16663001477718353\n",
      "Epoch 80 training loss: 0.1662663221359253\n",
      "Epoch 81 training loss: 0.1658490002155304\n",
      "Epoch 82 training loss: 0.16563370823860168\n",
      "Epoch 83 training loss: 0.16552095115184784\n",
      "Epoch 84 training loss: 0.1654587984085083\n",
      "Epoch 85 training loss: 0.16537614166736603\n",
      "Epoch 86 training loss: 0.16537095606327057\n",
      "Epoch 87 training loss: 0.16531004011631012\n",
      "Epoch 88 training loss: 0.16529624164104462\n",
      "Epoch 89 training loss: 0.16529308259487152\n",
      "Epoch 90 training loss: 0.16527557373046875\n",
      "Epoch 91 training loss: 0.1652599573135376\n",
      "Epoch 92 training loss: 0.1651657074689865\n",
      "Epoch 93 training loss: 0.1652131825685501\n",
      "Epoch 94 training loss: 0.16520553827285767\n",
      "Epoch 95 training loss: 0.1651611328125\n",
      "Epoch 96 training loss: 0.1651621311903\n",
      "Epoch 97 training loss: 0.16519345343112946\n",
      "Epoch 98 training loss: 0.16517382860183716\n",
      "Epoch 99 training loss: 0.16509738564491272\n",
      "Epoch 100 training loss: 0.16507796943187714\n",
      "Epoch 101 training loss: 0.1651117503643036\n",
      "Epoch 102 training loss: 0.165104478597641\n",
      "Epoch 103 training loss: 0.16516909003257751\n",
      "Epoch 104 training loss: 0.16508862376213074\n",
      "Epoch 105 training loss: 0.16511014103889465\n",
      "Epoch 106 training loss: 0.16507498919963837\n",
      "Epoch 107 training loss: 0.16505128145217896\n",
      "Epoch 108 training loss: 0.16505971550941467\n",
      "Epoch 109 training loss: 0.1649939864873886\n",
      "Epoch 110 training loss: 0.16499775648117065\n",
      "Epoch 111 training loss: 0.16497525572776794\n",
      "Epoch 112 training loss: 0.16498683393001556\n",
      "Epoch 113 training loss: 0.1649920642375946\n",
      "Epoch 114 training loss: 0.1650433987379074\n",
      "Epoch 115 training loss: 0.16500139236450195\n",
      "Epoch 116 training loss: 0.1649690866470337\n",
      "Epoch 117 training loss: 0.1650148183107376\n",
      "Epoch 118 training loss: 0.16497363150119781\n",
      "Epoch 119 training loss: 0.16497468948364258\n",
      "Epoch 120 training loss: 0.1649494469165802\n",
      "Epoch 121 training loss: 0.16495713591575623\n",
      "Epoch 122 training loss: 0.16515107452869415\n",
      "Epoch 123 training loss: 0.1649789661169052\n",
      "Epoch 124 training loss: 0.16495779156684875\n",
      "Epoch 125 training loss: 0.1649857759475708\n",
      "Epoch 126 training loss: 0.16492027044296265\n",
      "Epoch 127 training loss: 0.1649385690689087\n",
      "Epoch 128 training loss: 0.16495110094547272\n",
      "Epoch 129 training loss: 0.1649756282567978\n",
      "Epoch 130 training loss: 0.16493456065654755\n",
      "Epoch 131 training loss: 0.1649821400642395\n",
      "Epoch 132 training loss: 0.16498489677906036\n",
      "Epoch 133 training loss: 0.164930060505867\n",
      "Epoch 134 training loss: 0.16489359736442566\n",
      "Epoch 135 training loss: 0.16491061449050903\n",
      "Epoch 136 training loss: 0.1649249941110611\n",
      "Epoch 137 training loss: 0.1650056391954422\n",
      "Epoch 138 training loss: 0.16496941447257996\n",
      "Epoch 139 training loss: 0.16494052112102509\n",
      "Epoch 140 training loss: 0.16495178639888763\n",
      "Epoch 141 training loss: 0.16487109661102295\n",
      "Epoch 142 training loss: 0.1648498773574829\n",
      "Epoch 143 training loss: 0.16483591496944427\n",
      "Epoch 144 training loss: 0.16483935713768005\n",
      "Epoch 145 training loss: 0.1648600548505783\n",
      "Epoch 146 training loss: 0.16486141085624695\n",
      "Epoch 147 training loss: 0.16491375863552094\n",
      "Epoch 148 training loss: 0.16492648422718048\n",
      "Epoch 149 training loss: 0.16493812203407288\n",
      "Epoch 150 training loss: 0.16490525007247925\n",
      "Epoch 151 training loss: 0.16487710177898407\n",
      "Epoch 152 training loss: 0.16487674415111542\n",
      "Epoch 153 training loss: 0.16483594477176666\n",
      "Epoch 154 training loss: 0.16483083367347717\n",
      "Epoch 155 training loss: 0.16483162343502045\n",
      "Epoch 156 training loss: 0.16494901478290558\n",
      "Epoch 157 training loss: 0.1650901883840561\n",
      "Epoch 158 training loss: 0.16496798396110535\n",
      "Epoch 159 training loss: 0.16495105624198914\n",
      "Epoch 160 training loss: 0.16491471230983734\n",
      "Epoch 161 training loss: 0.16486495733261108\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.16493481397628784\n",
      "Epoch 164 training loss: 0.1648513674736023\n",
      "Epoch 165 training loss: 0.16486524045467377\n",
      "Epoch 166 training loss: 0.16497018933296204\n",
      "Epoch 167 training loss: 0.16491641104221344\n",
      "Epoch 168 training loss: 0.16486111283302307\n",
      "Epoch 169 training loss: 0.164909228682518\n",
      "Epoch 170 training loss: 0.1649133265018463\n",
      "Epoch 171 training loss: 0.16487690806388855\n",
      "Epoch 172 training loss: 0.16485801339149475\n",
      "Epoch 173 training loss: 0.16486766934394836\n",
      "Epoch 174 training loss: 0.16484883427619934\n",
      "Epoch 175 training loss: 0.16482563316822052\n",
      "Epoch 176 training loss: 0.1648404896259308\n",
      "Epoch 177 training loss: 0.16486871242523193\n",
      "Epoch 178 training loss: 0.1648487150669098\n",
      "Epoch 179 training loss: 0.1648281067609787\n",
      "Epoch 180 training loss: 0.16482169926166534\n",
      "Epoch 181 training loss: 0.1648256778717041\n",
      "Epoch 182 training loss: 0.16484518349170685\n",
      "Epoch 183 training loss: 0.16496612131595612\n",
      "Epoch 184 training loss: 0.1648326814174652\n",
      "Epoch 185 training loss: 0.164798766374588\n",
      "Epoch 186 training loss: 0.164801687002182\n",
      "Epoch 187 training loss: 0.16480009257793427\n",
      "Epoch 188 training loss: 0.16483540832996368\n",
      "Epoch 189 training loss: 0.1649075597524643\n",
      "Epoch 190 training loss: 0.16480576992034912\n",
      "Epoch 191 training loss: 0.1647839993238449\n",
      "Epoch 192 training loss: 0.1648356318473816\n",
      "Epoch 193 training loss: 0.16480454802513123\n",
      "Epoch 194 training loss: 0.1647886484861374\n",
      "Epoch 195 training loss: 0.16479100286960602\n",
      "Epoch 196 training loss: 0.16481254994869232\n",
      "Epoch 197 training loss: 0.1648106724023819\n",
      "Epoch 198 training loss: 0.1648610383272171\n",
      "Epoch 199 training loss: 0.16488100588321686\n",
      "Epoch 200 training loss: 0.1648060381412506\n",
      "Epoch 201 training loss: 0.16479626297950745\n",
      "Epoch 202 training loss: 0.16479703783988953\n",
      "Epoch 203 training loss: 0.1648780256509781\n",
      "Epoch 204 training loss: 0.16486670076847076\n",
      "Epoch 205 training loss: 0.16480804979801178\n",
      "Epoch 206 training loss: 0.16483813524246216\n",
      "Epoch 207 training loss: 0.1648465096950531\n",
      "Epoch 208 training loss: 0.1648363471031189\n",
      "Epoch 209 training loss: 0.164854034781456\n",
      "Epoch 210 training loss: 0.1648809313774109\n",
      "Epoch 211 training loss: 0.1649470031261444\n",
      "Epoch 212 training loss: 0.1649084836244583\n",
      "Epoch 213 training loss: 0.16496233642101288\n",
      "Epoch 214 training loss: 0.16488410532474518\n",
      "Epoch 215 training loss: 0.16484808921813965\n",
      "Epoch 216 training loss: 0.1648230254650116\n",
      "Epoch 217 training loss: 0.16481532156467438\n",
      "Epoch 218 training loss: 0.16482269763946533\n",
      "Epoch 219 training loss: 0.16485249996185303\n",
      "Epoch 220 training loss: 0.16485776007175446\n",
      "Epoch 221 training loss: 0.16479940712451935\n",
      "Epoch 222 training loss: 0.1647922694683075\n",
      "Epoch 223 training loss: 0.16481277346611023\n",
      "Epoch 224 training loss: 0.1648196280002594\n",
      "Epoch 225 training loss: 0.1648918092250824\n",
      "Epoch 226 training loss: 0.16487829387187958\n",
      "Epoch 227 training loss: 0.16488078236579895\n",
      "Epoch 228 training loss: 0.164849191904068\n",
      "Epoch 229 training loss: 0.16477710008621216\n",
      "Epoch 230 training loss: 0.16477271914482117\n",
      "Epoch 231 training loss: 0.16476401686668396\n",
      "Epoch 232 training loss: 0.1647741198539734\n",
      "Epoch 233 training loss: 0.1648848056793213\n",
      "Epoch 234 training loss: 0.16478723287582397\n",
      "Epoch 235 training loss: 0.16479089856147766\n",
      "Epoch 236 training loss: 0.16477440297603607\n",
      "Epoch 237 training loss: 0.16477873921394348\n",
      "Epoch 238 training loss: 0.164784237742424\n",
      "Epoch 239 training loss: 0.16476275026798248\n",
      "Epoch 240 training loss: 0.16477172076702118\n",
      "Epoch 241 training loss: 0.16475346684455872\n",
      "Epoch 242 training loss: 0.1647641658782959\n",
      "Epoch 243 training loss: 0.16478458046913147\n",
      "Epoch 244 training loss: 0.16474385559558868\n",
      "Epoch 245 training loss: 0.16473190486431122\n",
      "Epoch 246 training loss: 0.16472983360290527\n",
      "Epoch 247 training loss: 0.164745032787323\n",
      "Epoch 248 training loss: 0.16475629806518555\n",
      "Epoch 249 training loss: 0.16477416455745697\n",
      "Epoch 250 training loss: 0.16477340459823608\n",
      "Epoch 251 training loss: 0.16476497054100037\n",
      "Epoch 252 training loss: 0.1647583246231079\n",
      "Epoch 253 training loss: 0.1647467464208603\n",
      "Epoch 254 training loss: 0.16475403308868408\n",
      "Epoch 255 training loss: 0.16489911079406738\n",
      "Epoch 256 training loss: 0.1648680567741394\n",
      "0.48218181818181816\n",
      "[[ 392 1066]\n",
      " [1070 1597]]\n",
      "Accuracy: 0.48218181818181816\n",
      "Precision:  0.5996995869320315\n",
      "Recall:  0.5988001499812523\n",
      "F1:  0.5992495309568481\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.Tanh(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17048297822475433\n",
      "Epoch 2 training loss: 0.16889290511608124\n",
      "Epoch 3 training loss: 0.16845721006393433\n",
      "Epoch 4 training loss: 0.16830764710903168\n",
      "Epoch 5 training loss: 0.16820263862609863\n",
      "Epoch 6 training loss: 0.16814833879470825\n",
      "Epoch 7 training loss: 0.16813574731349945\n",
      "Epoch 8 training loss: 0.16808335483074188\n",
      "Epoch 9 training loss: 0.16807159781455994\n",
      "Epoch 10 training loss: 0.16805912554264069\n",
      "Epoch 11 training loss: 0.16805769503116608\n",
      "Epoch 12 training loss: 0.16806276142597198\n",
      "Epoch 13 training loss: 0.1680343598127365\n",
      "Epoch 14 training loss: 0.1680484414100647\n",
      "Epoch 15 training loss: 0.16805490851402283\n",
      "Epoch 16 training loss: 0.16802315413951874\n",
      "Epoch 17 training loss: 0.1680458039045334\n",
      "Epoch 18 training loss: 0.16803789138793945\n",
      "Epoch 19 training loss: 0.16801948845386505\n",
      "Epoch 20 training loss: 0.16804732382297516\n",
      "Epoch 21 training loss: 0.1680135428905487\n",
      "Epoch 22 training loss: 0.16801586747169495\n",
      "Epoch 23 training loss: 0.16801030933856964\n",
      "Epoch 24 training loss: 0.16802343726158142\n",
      "Epoch 25 training loss: 0.16806061565876007\n",
      "Epoch 26 training loss: 0.1680029332637787\n",
      "Epoch 27 training loss: 0.16800610721111298\n",
      "Epoch 28 training loss: 0.16800940036773682\n",
      "Epoch 29 training loss: 0.16805070638656616\n",
      "Epoch 30 training loss: 0.16801466047763824\n",
      "Epoch 31 training loss: 0.16801109910011292\n",
      "Epoch 32 training loss: 0.16799896955490112\n",
      "Epoch 33 training loss: 0.1680048555135727\n",
      "Epoch 34 training loss: 0.16801267862319946\n",
      "Epoch 35 training loss: 0.16798287630081177\n",
      "Epoch 36 training loss: 0.16804930567741394\n",
      "Epoch 37 training loss: 0.16799293458461761\n",
      "Epoch 38 training loss: 0.16798101365566254\n",
      "Epoch 39 training loss: 0.1677250862121582\n",
      "Epoch 40 training loss: 0.166952446103096\n",
      "Epoch 41 training loss: 0.16678158938884735\n",
      "Epoch 42 training loss: 0.16661563515663147\n",
      "Epoch 43 training loss: 0.16582980751991272\n",
      "Epoch 44 training loss: 0.16569674015045166\n",
      "Epoch 45 training loss: 0.16553372144699097\n",
      "Epoch 46 training loss: 0.1655174195766449\n",
      "Epoch 47 training loss: 0.16547071933746338\n",
      "Epoch 48 training loss: 0.16548818349838257\n",
      "Epoch 49 training loss: 0.16539238393306732\n",
      "Epoch 50 training loss: 0.16536274552345276\n",
      "Epoch 51 training loss: 0.1654076725244522\n",
      "Epoch 52 training loss: 0.16531293094158173\n",
      "Epoch 53 training loss: 0.16527575254440308\n",
      "Epoch 54 training loss: 0.16525791585445404\n",
      "Epoch 55 training loss: 0.16516117751598358\n",
      "Epoch 56 training loss: 0.1651705503463745\n",
      "Epoch 57 training loss: 0.16516275703907013\n",
      "Epoch 58 training loss: 0.1651269644498825\n",
      "Epoch 59 training loss: 0.16513550281524658\n",
      "Epoch 60 training loss: 0.16511771082878113\n",
      "Epoch 61 training loss: 0.16506704688072205\n",
      "Epoch 62 training loss: 0.1650332510471344\n",
      "Epoch 63 training loss: 0.16504237055778503\n",
      "Epoch 64 training loss: 0.1650545299053192\n",
      "Epoch 65 training loss: 0.16508206725120544\n",
      "Epoch 66 training loss: 0.1649974286556244\n",
      "Epoch 67 training loss: 0.16497361660003662\n",
      "Epoch 68 training loss: 0.1649814248085022\n",
      "Epoch 69 training loss: 0.16504935920238495\n",
      "Epoch 70 training loss: 0.16501019895076752\n",
      "Epoch 71 training loss: 0.1649552583694458\n",
      "Epoch 72 training loss: 0.1650993376970291\n",
      "Epoch 73 training loss: 0.16508223116397858\n",
      "Epoch 74 training loss: 0.16517047584056854\n",
      "Epoch 75 training loss: 0.16521300375461578\n",
      "Epoch 76 training loss: 0.1650741696357727\n",
      "Epoch 77 training loss: 0.16509033739566803\n",
      "Epoch 78 training loss: 0.16508528590202332\n",
      "Epoch 79 training loss: 0.16500599682331085\n",
      "Epoch 80 training loss: 0.16493317484855652\n",
      "Epoch 81 training loss: 0.16506394743919373\n",
      "Epoch 82 training loss: 0.16497287154197693\n",
      "Epoch 83 training loss: 0.16493584215641022\n",
      "Epoch 84 training loss: 0.16493527591228485\n",
      "Epoch 85 training loss: 0.1649501770734787\n",
      "Epoch 86 training loss: 0.16490690410137177\n",
      "Epoch 87 training loss: 0.16495487093925476\n",
      "Epoch 88 training loss: 0.16495370864868164\n",
      "Epoch 89 training loss: 0.16503620147705078\n",
      "Epoch 90 training loss: 0.16493797302246094\n",
      "Epoch 91 training loss: 0.16498427093029022\n",
      "Epoch 92 training loss: 0.16493545472621918\n",
      "Epoch 93 training loss: 0.1649208962917328\n",
      "Epoch 94 training loss: 0.16495338082313538\n",
      "Epoch 95 training loss: 0.1649291068315506\n",
      "Epoch 96 training loss: 0.16492561995983124\n",
      "Epoch 97 training loss: 0.1648913323879242\n",
      "Epoch 98 training loss: 0.1649065613746643\n",
      "Epoch 99 training loss: 0.16493679583072662\n",
      "Epoch 100 training loss: 0.1649480015039444\n",
      "Epoch 101 training loss: 0.1650744378566742\n",
      "Epoch 102 training loss: 0.16502448916435242\n",
      "Epoch 103 training loss: 0.16501517593860626\n",
      "Epoch 104 training loss: 0.16496840119361877\n",
      "Epoch 105 training loss: 0.1649671494960785\n",
      "Epoch 106 training loss: 0.1649441123008728\n",
      "Epoch 107 training loss: 0.16488738358020782\n",
      "Epoch 108 training loss: 0.16491222381591797\n",
      "Epoch 109 training loss: 0.16490551829338074\n",
      "Epoch 110 training loss: 0.1649019420146942\n",
      "Epoch 111 training loss: 0.16496579349040985\n",
      "Epoch 112 training loss: 0.16494669020175934\n",
      "Epoch 113 training loss: 0.16490943729877472\n",
      "Epoch 114 training loss: 0.1649254858493805\n",
      "Epoch 115 training loss: 0.1649320423603058\n",
      "Epoch 116 training loss: 0.16492733359336853\n",
      "Epoch 117 training loss: 0.1649169772863388\n",
      "Epoch 118 training loss: 0.16515254974365234\n",
      "Epoch 119 training loss: 0.16511186957359314\n",
      "Epoch 120 training loss: 0.1650628000497818\n",
      "Epoch 121 training loss: 0.16499508917331696\n",
      "Epoch 122 training loss: 0.16491152346134186\n",
      "Epoch 123 training loss: 0.16504086554050446\n",
      "Epoch 124 training loss: 0.16507422924041748\n",
      "Epoch 125 training loss: 0.165010467171669\n",
      "Epoch 126 training loss: 0.16493843495845795\n",
      "Epoch 127 training loss: 0.16491617262363434\n",
      "Epoch 128 training loss: 0.1649228185415268\n",
      "Epoch 129 training loss: 0.1649019718170166\n",
      "Epoch 130 training loss: 0.16487954556941986\n",
      "Epoch 131 training loss: 0.16495588421821594\n",
      "Epoch 132 training loss: 0.1649574339389801\n",
      "Epoch 133 training loss: 0.16490253806114197\n",
      "Epoch 134 training loss: 0.1649157702922821\n",
      "Epoch 135 training loss: 0.16493511199951172\n",
      "Epoch 136 training loss: 0.16488772630691528\n",
      "Epoch 137 training loss: 0.16485793888568878\n",
      "Epoch 138 training loss: 0.1648665815591812\n",
      "Epoch 139 training loss: 0.16485625505447388\n",
      "Epoch 140 training loss: 0.16496773064136505\n",
      "Epoch 141 training loss: 0.165019229054451\n",
      "Epoch 142 training loss: 0.16496799886226654\n",
      "Epoch 143 training loss: 0.1649271547794342\n",
      "Epoch 144 training loss: 0.1649191528558731\n",
      "Epoch 145 training loss: 0.16488845646381378\n",
      "Epoch 146 training loss: 0.16489775478839874\n",
      "Epoch 147 training loss: 0.1648975908756256\n",
      "Epoch 148 training loss: 0.16485780477523804\n",
      "Epoch 149 training loss: 0.16485758125782013\n",
      "Epoch 150 training loss: 0.16484779119491577\n",
      "Epoch 151 training loss: 0.16487419605255127\n",
      "Epoch 152 training loss: 0.16494746506214142\n",
      "Epoch 153 training loss: 0.16492073237895966\n",
      "Epoch 154 training loss: 0.1648574322462082\n",
      "Epoch 155 training loss: 0.1648692786693573\n",
      "Epoch 156 training loss: 0.16485993564128876\n",
      "Epoch 157 training loss: 0.1649700254201889\n",
      "Epoch 158 training loss: 0.16499640047550201\n",
      "Epoch 159 training loss: 0.16493870317935944\n",
      "Epoch 160 training loss: 0.16493374109268188\n",
      "Epoch 161 training loss: 0.1649080067873001\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.1648789644241333\n",
      "Epoch 164 training loss: 0.16489528119564056\n",
      "Epoch 165 training loss: 0.16487745940685272\n",
      "Epoch 166 training loss: 0.1648242473602295\n",
      "Epoch 167 training loss: 0.1648487150669098\n",
      "Epoch 168 training loss: 0.1648620218038559\n",
      "Epoch 169 training loss: 0.1648893803358078\n",
      "Epoch 170 training loss: 0.1648567169904709\n",
      "Epoch 171 training loss: 0.16489006578922272\n",
      "Epoch 172 training loss: 0.16485270857810974\n",
      "Epoch 173 training loss: 0.16487672924995422\n",
      "Epoch 174 training loss: 0.16488471627235413\n",
      "Epoch 175 training loss: 0.16484229266643524\n",
      "Epoch 176 training loss: 0.16493725776672363\n",
      "Epoch 177 training loss: 0.1649102121591568\n",
      "Epoch 178 training loss: 0.16487260162830353\n",
      "Epoch 179 training loss: 0.16490183770656586\n",
      "Epoch 180 training loss: 0.16484715044498444\n",
      "Epoch 181 training loss: 0.16485095024108887\n",
      "Epoch 182 training loss: 0.16485632956027985\n",
      "Epoch 183 training loss: 0.16485297679901123\n",
      "Epoch 184 training loss: 0.16485093533992767\n",
      "Epoch 185 training loss: 0.16483157873153687\n",
      "Epoch 186 training loss: 0.16496655344963074\n",
      "Epoch 187 training loss: 0.16495396196842194\n",
      "Epoch 188 training loss: 0.16494569182395935\n",
      "Epoch 189 training loss: 0.16491083800792694\n",
      "Epoch 190 training loss: 0.1649261862039566\n",
      "Epoch 191 training loss: 0.16509567201137543\n",
      "Epoch 192 training loss: 0.1650398075580597\n",
      "Epoch 193 training loss: 0.16508892178535461\n",
      "Epoch 194 training loss: 0.1649836301803589\n",
      "Epoch 195 training loss: 0.1649351865053177\n",
      "Epoch 196 training loss: 0.16492505371570587\n",
      "Epoch 197 training loss: 0.16490179300308228\n",
      "Epoch 198 training loss: 0.16493715345859528\n",
      "Epoch 199 training loss: 0.16490860283374786\n",
      "Epoch 200 training loss: 0.16489098966121674\n",
      "Epoch 201 training loss: 0.16490168869495392\n",
      "Epoch 202 training loss: 0.16491514444351196\n",
      "Epoch 203 training loss: 0.16485656797885895\n",
      "Epoch 204 training loss: 0.16490283608436584\n",
      "Epoch 205 training loss: 0.164883553981781\n",
      "Epoch 206 training loss: 0.16486819088459015\n",
      "Epoch 207 training loss: 0.1648823767900467\n",
      "Epoch 208 training loss: 0.16494770348072052\n",
      "Epoch 209 training loss: 0.1649188995361328\n",
      "Epoch 210 training loss: 0.16486652195453644\n",
      "Epoch 211 training loss: 0.164960578083992\n",
      "Epoch 212 training loss: 0.16485953330993652\n",
      "Epoch 213 training loss: 0.16483987867832184\n",
      "Epoch 214 training loss: 0.1648828238248825\n",
      "Epoch 215 training loss: 0.16494596004486084\n",
      "Epoch 216 training loss: 0.16491232812404633\n",
      "Epoch 217 training loss: 0.16492034494876862\n",
      "Epoch 218 training loss: 0.16492900252342224\n",
      "Epoch 219 training loss: 0.16489380598068237\n",
      "Epoch 220 training loss: 0.16489483416080475\n",
      "Epoch 221 training loss: 0.16490140557289124\n",
      "Epoch 222 training loss: 0.16484548151493073\n",
      "Epoch 223 training loss: 0.16491907835006714\n",
      "Epoch 224 training loss: 0.16493640840053558\n",
      "Epoch 225 training loss: 0.164934903383255\n",
      "Epoch 226 training loss: 0.1648758202791214\n",
      "Epoch 227 training loss: 0.16483473777770996\n",
      "Epoch 228 training loss: 0.1648281216621399\n",
      "Epoch 229 training loss: 0.16489090025424957\n",
      "Epoch 230 training loss: 0.1648705154657364\n",
      "Epoch 231 training loss: 0.1648814082145691\n",
      "Epoch 232 training loss: 0.16482363641262054\n",
      "Epoch 233 training loss: 0.1649101823568344\n",
      "Epoch 234 training loss: 0.16488051414489746\n",
      "Epoch 235 training loss: 0.16489385068416595\n",
      "Epoch 236 training loss: 0.16489002108573914\n",
      "Epoch 237 training loss: 0.16491107642650604\n",
      "Epoch 238 training loss: 0.16488583385944366\n",
      "Epoch 239 training loss: 0.1649440973997116\n",
      "Epoch 240 training loss: 0.16485491394996643\n",
      "Epoch 241 training loss: 0.1648317128419876\n",
      "Epoch 242 training loss: 0.16481254994869232\n",
      "Epoch 243 training loss: 0.1648099273443222\n",
      "Epoch 244 training loss: 0.16481496393680573\n",
      "Epoch 245 training loss: 0.16482143104076385\n",
      "Epoch 246 training loss: 0.1648457646369934\n",
      "Epoch 247 training loss: 0.16480199992656708\n",
      "Epoch 248 training loss: 0.16478675603866577\n",
      "Epoch 249 training loss: 0.16480021178722382\n",
      "Epoch 250 training loss: 0.16483859717845917\n",
      "Epoch 251 training loss: 0.16479599475860596\n",
      "Epoch 252 training loss: 0.164817675948143\n",
      "Epoch 253 training loss: 0.16481716930866241\n",
      "Epoch 254 training loss: 0.16489620506763458\n",
      "Epoch 255 training loss: 0.16483238339424133\n",
      "Epoch 256 training loss: 0.1648678183555603\n",
      "0.49066666666666664\n",
      "[[ 359 1099]\n",
      " [1002 1665]]\n",
      "Accuracy: 0.49066666666666664\n",
      "Precision:  0.6023878437047757\n",
      "Recall:  0.6242969628796401\n",
      "F1:  0.6131467501380962\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=False\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.01, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(5, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [1000,1200,1600,2000,2400,2800,3200]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with rbf with anomalies being 10% of the data and one_hot=True\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=True)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 16, 32, 32)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06971529126167297\n",
      "Epoch 2 training loss: 0.04786883294582367\n",
      "Epoch 3 training loss: 0.04360324516892433\n",
      "Epoch 4 training loss: 0.04194796457886696\n",
      "Epoch 5 training loss: 0.040111180394887924\n",
      "Epoch 6 training loss: 0.03824681043624878\n",
      "Epoch 7 training loss: 0.03677377104759216\n",
      "Epoch 8 training loss: 0.03536611422896385\n",
      "Epoch 9 training loss: 0.03354225680232048\n",
      "Epoch 10 training loss: 0.03137403726577759\n",
      "Epoch 11 training loss: 0.029583103954792023\n",
      "Epoch 12 training loss: 0.028284654021263123\n",
      "Epoch 13 training loss: 0.02725023590028286\n",
      "Epoch 14 training loss: 0.026495160534977913\n",
      "Epoch 15 training loss: 0.026067033410072327\n",
      "Epoch 16 training loss: 0.02549590729176998\n",
      "Epoch 17 training loss: 0.024979382753372192\n",
      "Epoch 18 training loss: 0.024581756442785263\n",
      "Epoch 19 training loss: 0.024252163246273994\n",
      "Epoch 20 training loss: 0.023938970640301704\n",
      "Epoch 21 training loss: 0.0234003197401762\n",
      "Epoch 22 training loss: 0.023160770535469055\n",
      "Epoch 23 training loss: 0.022858845070004463\n",
      "Epoch 24 training loss: 0.02254897728562355\n",
      "Epoch 25 training loss: 0.02224528230726719\n",
      "Epoch 26 training loss: 0.022055665031075478\n",
      "Epoch 27 training loss: 0.021860601380467415\n",
      "Epoch 28 training loss: 0.021508967503905296\n",
      "Epoch 29 training loss: 0.02132820524275303\n",
      "Epoch 30 training loss: 0.021300602704286575\n",
      "Epoch 31 training loss: 0.020873505622148514\n",
      "Epoch 32 training loss: 0.02075488120317459\n",
      "Epoch 33 training loss: 0.020622408017516136\n",
      "Epoch 34 training loss: 0.020474329590797424\n",
      "Epoch 35 training loss: 0.020251190289855003\n",
      "Epoch 36 training loss: 0.02016971819102764\n",
      "Epoch 37 training loss: 0.020298535004258156\n",
      "Epoch 38 training loss: 0.019755329936742783\n",
      "Epoch 39 training loss: 0.019767414778470993\n",
      "Epoch 40 training loss: 0.019633740186691284\n",
      "Epoch 41 training loss: 0.0194877739995718\n",
      "Epoch 42 training loss: 0.01956487074494362\n",
      "Epoch 43 training loss: 0.019184475764632225\n",
      "Epoch 44 training loss: 0.019263993948698044\n",
      "Epoch 45 training loss: 0.01911487616598606\n",
      "Epoch 46 training loss: 0.019197430461645126\n",
      "Epoch 47 training loss: 0.01920635998249054\n",
      "Epoch 48 training loss: 0.018751351162791252\n",
      "Epoch 49 training loss: 0.01861399970948696\n",
      "Epoch 50 training loss: 0.018730437383055687\n",
      "Epoch 51 training loss: 0.0187191441655159\n",
      "Epoch 52 training loss: 0.01851801574230194\n",
      "Epoch 53 training loss: 0.01863197237253189\n",
      "Epoch 54 training loss: 0.018345829099416733\n",
      "Epoch 55 training loss: 0.018362069502472878\n",
      "Epoch 56 training loss: 0.018251044675707817\n",
      "Epoch 57 training loss: 0.01821674406528473\n",
      "Epoch 58 training loss: 0.018235333263874054\n",
      "Epoch 59 training loss: 0.018105700612068176\n",
      "Epoch 60 training loss: 0.018001500517129898\n",
      "Epoch 61 training loss: 0.01796252653002739\n",
      "Epoch 62 training loss: 0.018044082447886467\n",
      "Epoch 63 training loss: 0.017860349267721176\n",
      "Epoch 64 training loss: 0.01774909906089306\n",
      "Epoch 65 training loss: 0.017950033769011497\n",
      "Epoch 66 training loss: 0.017742974683642387\n",
      "Epoch 67 training loss: 0.017618278041481972\n",
      "Epoch 68 training loss: 0.017749140039086342\n",
      "Epoch 69 training loss: 0.017547868192195892\n",
      "Epoch 70 training loss: 0.017704833298921585\n",
      "Epoch 71 training loss: 0.017376607283949852\n",
      "Epoch 72 training loss: 0.01747717894613743\n",
      "Epoch 73 training loss: 0.01743854209780693\n",
      "Epoch 74 training loss: 0.017357533797621727\n",
      "Epoch 75 training loss: 0.0173368901014328\n",
      "Epoch 76 training loss: 0.017211036756634712\n",
      "Epoch 77 training loss: 0.0172377098351717\n",
      "Epoch 78 training loss: 0.017185280099511147\n",
      "Epoch 79 training loss: 0.017232786864042282\n",
      "Epoch 80 training loss: 0.01714184321463108\n",
      "Epoch 81 training loss: 0.016957063227891922\n",
      "Epoch 82 training loss: 0.016967248171567917\n",
      "Epoch 83 training loss: 0.016986513510346413\n",
      "Epoch 84 training loss: 0.016889620572328568\n",
      "Epoch 85 training loss: 0.016919441521167755\n",
      "Epoch 86 training loss: 0.016626089811325073\n",
      "Epoch 87 training loss: 0.01669413223862648\n",
      "Epoch 88 training loss: 0.016765782609581947\n",
      "Epoch 89 training loss: 0.01650291495025158\n",
      "Epoch 90 training loss: 0.016583943739533424\n",
      "Epoch 91 training loss: 0.016262969002127647\n",
      "Epoch 92 training loss: 0.016220485791563988\n",
      "Epoch 93 training loss: 0.016070257872343063\n",
      "Epoch 94 training loss: 0.01596401445567608\n",
      "Epoch 95 training loss: 0.015809426084160805\n",
      "Epoch 96 training loss: 0.015720020979642868\n",
      "Epoch 97 training loss: 0.015735849738121033\n",
      "Epoch 98 training loss: 0.015719745308160782\n",
      "Epoch 99 training loss: 0.015509548597037792\n",
      "Epoch 100 training loss: 0.015322119928896427\n",
      "Epoch 101 training loss: 0.01532778050750494\n",
      "Epoch 102 training loss: 0.015076628886163235\n",
      "Epoch 103 training loss: 0.0149458646774292\n",
      "Epoch 104 training loss: 0.014805941842496395\n",
      "Epoch 105 training loss: 0.0147250946611166\n",
      "Epoch 106 training loss: 0.014789283275604248\n",
      "Epoch 107 training loss: 0.014615237712860107\n",
      "Epoch 108 training loss: 0.014453504234552383\n",
      "Epoch 109 training loss: 0.014508918859064579\n",
      "Epoch 110 training loss: 0.014261085540056229\n",
      "Epoch 111 training loss: 0.01426205225288868\n",
      "Epoch 112 training loss: 0.014262132346630096\n",
      "Epoch 113 training loss: 0.014112000353634357\n",
      "Epoch 114 training loss: 0.014012403786182404\n",
      "Epoch 115 training loss: 0.01408609189093113\n",
      "Epoch 116 training loss: 0.013886873610317707\n",
      "Epoch 117 training loss: 0.01374044269323349\n",
      "Epoch 118 training loss: 0.01387003343552351\n",
      "Epoch 119 training loss: 0.013772225007414818\n",
      "Epoch 120 training loss: 0.013795035891234875\n",
      "Epoch 121 training loss: 0.013466940261423588\n",
      "Epoch 122 training loss: 0.013755028136074543\n",
      "Epoch 123 training loss: 0.013726615346968174\n",
      "Epoch 124 training loss: 0.0134730851277709\n",
      "Epoch 125 training loss: 0.013547927141189575\n",
      "Epoch 126 training loss: 0.013528102077543736\n",
      "Epoch 127 training loss: 0.013424591161310673\n",
      "Epoch 128 training loss: 0.01351673249155283\n",
      "Epoch 129 training loss: 0.013498596847057343\n",
      "Epoch 130 training loss: 0.013442899100482464\n",
      "Epoch 131 training loss: 0.013364133425056934\n",
      "Epoch 132 training loss: 0.013354748487472534\n",
      "Epoch 133 training loss: 0.013236195780336857\n",
      "Epoch 134 training loss: 0.013323521241545677\n",
      "Epoch 135 training loss: 0.013115270994603634\n",
      "Epoch 136 training loss: 0.013142894953489304\n",
      "Epoch 137 training loss: 0.013322302140295506\n",
      "Epoch 138 training loss: 0.013042558915913105\n",
      "Epoch 139 training loss: 0.013162856921553612\n",
      "Epoch 140 training loss: 0.012983868829905987\n",
      "Epoch 141 training loss: 0.013026939705014229\n",
      "Epoch 142 training loss: 0.01309625431895256\n",
      "Epoch 143 training loss: 0.013126621954143047\n",
      "Epoch 144 training loss: 0.012845205143094063\n",
      "Epoch 145 training loss: 0.012928196229040623\n",
      "Epoch 146 training loss: 0.012924396432936192\n",
      "Epoch 147 training loss: 0.012866094708442688\n",
      "Epoch 148 training loss: 0.012874449603259563\n",
      "Epoch 149 training loss: 0.012811403721570969\n",
      "Epoch 150 training loss: 0.012868626043200493\n",
      "Epoch 151 training loss: 0.012910211458802223\n",
      "Epoch 152 training loss: 0.012596533633768559\n",
      "Epoch 153 training loss: 0.012956635095179081\n",
      "Epoch 154 training loss: 0.012805708684027195\n",
      "Epoch 155 training loss: 0.012791200540959835\n",
      "Epoch 156 training loss: 0.012770901434123516\n",
      "Epoch 157 training loss: 0.012653087265789509\n",
      "Epoch 158 training loss: 0.01258409209549427\n",
      "Epoch 159 training loss: 0.012826205231249332\n",
      "Epoch 160 training loss: 0.01245005801320076\n",
      "Epoch 161 training loss: 0.01264494564384222\n",
      "Epoch 162 training loss: 0.012617778964340687\n",
      "Epoch 163 training loss: 0.012514456175267696\n",
      "Epoch 164 training loss: 0.012502624653279781\n",
      "Epoch 165 training loss: 0.012357398867607117\n",
      "Epoch 166 training loss: 0.012393324635922909\n",
      "Epoch 167 training loss: 0.012389593757689\n",
      "Epoch 168 training loss: 0.012367045506834984\n",
      "Epoch 169 training loss: 0.012400475330650806\n",
      "Epoch 170 training loss: 0.012233864516019821\n",
      "Epoch 171 training loss: 0.012260227464139462\n",
      "Epoch 172 training loss: 0.01239768136292696\n",
      "Epoch 173 training loss: 0.012634100392460823\n",
      "Epoch 174 training loss: 0.012349237687885761\n",
      "Epoch 175 training loss: 0.012171860784292221\n",
      "Epoch 176 training loss: 0.012198963202536106\n",
      "Epoch 177 training loss: 0.01197326835244894\n",
      "Epoch 178 training loss: 0.012044228613376617\n",
      "Epoch 179 training loss: 0.012370489537715912\n",
      "Epoch 180 training loss: 0.012172732502222061\n",
      "Epoch 181 training loss: 0.012168047949671745\n",
      "Epoch 182 training loss: 0.012094411067664623\n",
      "Epoch 183 training loss: 0.012061355635523796\n",
      "Epoch 184 training loss: 0.012200168333947659\n",
      "Epoch 185 training loss: 0.011829286813735962\n",
      "Epoch 186 training loss: 0.01208648830652237\n",
      "Epoch 187 training loss: 0.012231837958097458\n",
      "Epoch 188 training loss: 0.012299724854528904\n",
      "Epoch 189 training loss: 0.01174398697912693\n",
      "Epoch 190 training loss: 0.011918370611965656\n",
      "Epoch 191 training loss: 0.011898685246706009\n",
      "Epoch 192 training loss: 0.011908282525837421\n",
      "Epoch 193 training loss: 0.011898086406290531\n",
      "Epoch 194 training loss: 0.01197363343089819\n",
      "Epoch 195 training loss: 0.011655939742922783\n",
      "Epoch 196 training loss: 0.01202722080051899\n",
      "Epoch 197 training loss: 0.01174541749060154\n",
      "Epoch 198 training loss: 0.011885097250342369\n",
      "Epoch 199 training loss: 0.011889418587088585\n",
      "Epoch 200 training loss: 0.011561723425984383\n",
      "Epoch 201 training loss: 0.012064131908118725\n",
      "Epoch 202 training loss: 0.011695858091115952\n",
      "Epoch 203 training loss: 0.011639691889286041\n",
      "Epoch 204 training loss: 0.011742192320525646\n",
      "Epoch 205 training loss: 0.011739774607121944\n",
      "Epoch 206 training loss: 0.011679609306156635\n",
      "Epoch 207 training loss: 0.011578892357647419\n",
      "Epoch 208 training loss: 0.011658375151455402\n",
      "Epoch 209 training loss: 0.011785699985921383\n",
      "Epoch 210 training loss: 0.011810307390987873\n",
      "Epoch 211 training loss: 0.011679754592478275\n",
      "Epoch 212 training loss: 0.011694684624671936\n",
      "Epoch 213 training loss: 0.011764619499444962\n",
      "Epoch 214 training loss: 0.011479787528514862\n",
      "Epoch 215 training loss: 0.011653930880129337\n",
      "Epoch 216 training loss: 0.01200628187507391\n",
      "Epoch 217 training loss: 0.01153975073248148\n",
      "Epoch 218 training loss: 0.0112830251455307\n",
      "Epoch 219 training loss: 0.011388145387172699\n",
      "Epoch 220 training loss: 0.011851916089653969\n",
      "Epoch 221 training loss: 0.011348015628755093\n",
      "Epoch 222 training loss: 0.011732806451618671\n",
      "Epoch 223 training loss: 0.011590567417442799\n",
      "Epoch 224 training loss: 0.011516428552567959\n",
      "Epoch 225 training loss: 0.011593179777264595\n",
      "Epoch 226 training loss: 0.011507459916174412\n",
      "Epoch 227 training loss: 0.011482157744467258\n",
      "Epoch 228 training loss: 0.011658458039164543\n",
      "Epoch 229 training loss: 0.01136043295264244\n",
      "Epoch 230 training loss: 0.011310987174510956\n",
      "Epoch 231 training loss: 0.01154673844575882\n",
      "Epoch 232 training loss: 0.01141483522951603\n",
      "Epoch 233 training loss: 0.01157707441598177\n",
      "Epoch 234 training loss: 0.01124538667500019\n",
      "Epoch 235 training loss: 0.011882472783327103\n",
      "Epoch 236 training loss: 0.011054693721234798\n",
      "Epoch 237 training loss: 0.011390742845833302\n",
      "Epoch 238 training loss: 0.011319820769131184\n",
      "Epoch 239 training loss: 0.011219548992812634\n",
      "Epoch 240 training loss: 0.011469433084130287\n",
      "Epoch 241 training loss: 0.011268419213593006\n",
      "Epoch 242 training loss: 0.011506297625601292\n",
      "Epoch 243 training loss: 0.011308002285659313\n",
      "Epoch 244 training loss: 0.011324524879455566\n",
      "Epoch 245 training loss: 0.01146594900637865\n",
      "Epoch 246 training loss: 0.011356067843735218\n",
      "Epoch 247 training loss: 0.011158529669046402\n",
      "Epoch 248 training loss: 0.011280433274805546\n",
      "Epoch 249 training loss: 0.01135330367833376\n",
      "Epoch 250 training loss: 0.011020055040717125\n",
      "Epoch 251 training loss: 0.011329383589327335\n",
      "Epoch 252 training loss: 0.011040044017136097\n",
      "Epoch 253 training loss: 0.01134117878973484\n",
      "Epoch 254 training loss: 0.011233450844883919\n",
      "Epoch 255 training loss: 0.011138062924146652\n",
      "Epoch 256 training loss: 0.011081528849899769\n",
      "Epoch 257 training loss: 0.011081446893513203\n",
      "Epoch 258 training loss: 0.011301102116703987\n",
      "Epoch 259 training loss: 0.011142807081341743\n",
      "Epoch 260 training loss: 0.011223028413951397\n",
      "Epoch 261 training loss: 0.01106283813714981\n",
      "Epoch 262 training loss: 0.01103614829480648\n",
      "Epoch 263 training loss: 0.010996030643582344\n",
      "Epoch 264 training loss: 0.011049854569137096\n",
      "Epoch 265 training loss: 0.011121783405542374\n",
      "Epoch 266 training loss: 0.01113135740160942\n",
      "Epoch 267 training loss: 0.010935545898973942\n",
      "Epoch 268 training loss: 0.010910521261394024\n",
      "Epoch 269 training loss: 0.010923976078629494\n",
      "Epoch 270 training loss: 0.011444998905062675\n",
      "Epoch 271 training loss: 0.011054503731429577\n",
      "Epoch 272 training loss: 0.011215504258871078\n",
      "Epoch 273 training loss: 0.010934562422335148\n",
      "Epoch 274 training loss: 0.01084035262465477\n",
      "Epoch 275 training loss: 0.010942122898995876\n",
      "Epoch 276 training loss: 0.010909593664109707\n",
      "Epoch 277 training loss: 0.010754035785794258\n",
      "Epoch 278 training loss: 0.010961291380226612\n",
      "Epoch 279 training loss: 0.01077063474804163\n",
      "Epoch 280 training loss: 0.010956691578030586\n",
      "Epoch 281 training loss: 0.010772631503641605\n",
      "Epoch 282 training loss: 0.010638123378157616\n",
      "Epoch 283 training loss: 0.010591333732008934\n",
      "Epoch 284 training loss: 0.01067272387444973\n",
      "Epoch 285 training loss: 0.010974961332976818\n",
      "Epoch 286 training loss: 0.010780639946460724\n",
      "Epoch 287 training loss: 0.010791517794132233\n",
      "Epoch 288 training loss: 0.0105669517070055\n",
      "Epoch 289 training loss: 0.01073646079748869\n",
      "Epoch 290 training loss: 0.010761368088424206\n",
      "Epoch 291 training loss: 0.010698641650378704\n",
      "Epoch 292 training loss: 0.010801289230585098\n",
      "Epoch 293 training loss: 0.010589657351374626\n",
      "Epoch 294 training loss: 0.010684750974178314\n",
      "Epoch 295 training loss: 0.010689185932278633\n",
      "Epoch 296 training loss: 0.010466506704688072\n",
      "Epoch 297 training loss: 0.010655274614691734\n",
      "Epoch 298 training loss: 0.010381422005593777\n",
      "Epoch 299 training loss: 0.010593751445412636\n",
      "Epoch 300 training loss: 0.010622636415064335\n",
      "Epoch 301 training loss: 0.010421764105558395\n",
      "Epoch 302 training loss: 0.010420805774629116\n",
      "Epoch 303 training loss: 0.010652933269739151\n",
      "Epoch 304 training loss: 0.010435684584081173\n",
      "Epoch 305 training loss: 0.010253746062517166\n",
      "Epoch 306 training loss: 0.01019638404250145\n",
      "Epoch 307 training loss: 0.010276221670210361\n",
      "Epoch 308 training loss: 0.010261121205985546\n",
      "Epoch 309 training loss: 0.010358776897192001\n",
      "Epoch 310 training loss: 0.010143717750906944\n",
      "Epoch 311 training loss: 0.010404807515442371\n",
      "Epoch 312 training loss: 0.009961500763893127\n",
      "Epoch 313 training loss: 0.010207260958850384\n",
      "Epoch 314 training loss: 0.010304387658834457\n",
      "Epoch 315 training loss: 0.010022367350757122\n",
      "Epoch 316 training loss: 0.010237142443656921\n",
      "Epoch 317 training loss: 0.01025999803096056\n",
      "Epoch 318 training loss: 0.010450112633407116\n",
      "Epoch 319 training loss: 0.009985913522541523\n",
      "Epoch 320 training loss: 0.010069666430354118\n",
      "Epoch 321 training loss: 0.009971365332603455\n",
      "Epoch 322 training loss: 0.009976687841117382\n",
      "Epoch 323 training loss: 0.010095523670315742\n",
      "Epoch 324 training loss: 0.009875748306512833\n",
      "Epoch 325 training loss: 0.010242658667266369\n",
      "Epoch 326 training loss: 0.010075428523123264\n",
      "Epoch 327 training loss: 0.010135315358638763\n",
      "Epoch 328 training loss: 0.009928209707140923\n",
      "Epoch 329 training loss: 0.009937942959368229\n",
      "Epoch 330 training loss: 0.009918405674397945\n",
      "Epoch 331 training loss: 0.01000137533992529\n",
      "Epoch 332 training loss: 0.010177052579820156\n",
      "Epoch 333 training loss: 0.009866567328572273\n",
      "Epoch 334 training loss: 0.010059470310807228\n",
      "Epoch 335 training loss: 0.010027477517724037\n",
      "Epoch 336 training loss: 0.009678058326244354\n",
      "Epoch 337 training loss: 0.010004648938775063\n",
      "Epoch 338 training loss: 0.010023151524364948\n",
      "Epoch 339 training loss: 0.009657816030085087\n",
      "Epoch 340 training loss: 0.01002924982458353\n",
      "Epoch 341 training loss: 0.009686440229415894\n",
      "Epoch 342 training loss: 0.009809969924390316\n",
      "Epoch 343 training loss: 0.010121417231857777\n",
      "Epoch 344 training loss: 0.0098272655159235\n",
      "Epoch 345 training loss: 0.009696667082607746\n",
      "Epoch 346 training loss: 0.010033882223069668\n",
      "Epoch 347 training loss: 0.009900040924549103\n",
      "Epoch 348 training loss: 0.009824531152844429\n",
      "Epoch 349 training loss: 0.009669742546975613\n",
      "Epoch 350 training loss: 0.00971104484051466\n",
      "Epoch 351 training loss: 0.009999791160225868\n",
      "Epoch 352 training loss: 0.00955488346517086\n",
      "Epoch 353 training loss: 0.009670785628259182\n",
      "Epoch 354 training loss: 0.009819824248552322\n",
      "Epoch 355 training loss: 0.00966666266322136\n",
      "Epoch 356 training loss: 0.009682944975793362\n",
      "Epoch 357 training loss: 0.009882813319563866\n",
      "Epoch 358 training loss: 0.0096292644739151\n",
      "Epoch 359 training loss: 0.009934237226843834\n",
      "Epoch 360 training loss: 0.00956020224839449\n",
      "Epoch 361 training loss: 0.00965520367026329\n",
      "Epoch 362 training loss: 0.009952129796147346\n",
      "Epoch 363 training loss: 0.009937339462339878\n",
      "Epoch 364 training loss: 0.009598472155630589\n",
      "Epoch 365 training loss: 0.009525857865810394\n",
      "Epoch 366 training loss: 0.009858150035142899\n",
      "Epoch 367 training loss: 0.009870336391031742\n",
      "Epoch 368 training loss: 0.009424243122339249\n",
      "Epoch 369 training loss: 0.009602248668670654\n",
      "Epoch 370 training loss: 0.00990298017859459\n",
      "Epoch 371 training loss: 0.009403186850249767\n",
      "Epoch 372 training loss: 0.00973527505993843\n",
      "Epoch 373 training loss: 0.009549005888402462\n",
      "Epoch 374 training loss: 0.009692604653537273\n",
      "Epoch 375 training loss: 0.009650996886193752\n",
      "Epoch 376 training loss: 0.009488692507147789\n",
      "Epoch 377 training loss: 0.009469796903431416\n",
      "Epoch 378 training loss: 0.009694532491266727\n",
      "Epoch 379 training loss: 0.009678853675723076\n",
      "Epoch 380 training loss: 0.009566822089254856\n",
      "Epoch 381 training loss: 0.009565792046487331\n",
      "Epoch 382 training loss: 0.009568976238369942\n",
      "Epoch 383 training loss: 0.009527474641799927\n",
      "Epoch 384 training loss: 0.01006406731903553\n",
      "Epoch 385 training loss: 0.009586887434124947\n",
      "Epoch 386 training loss: 0.009327422827482224\n",
      "Epoch 387 training loss: 0.009741404093801975\n",
      "Epoch 388 training loss: 0.00947154313325882\n",
      "Epoch 389 training loss: 0.00936540961265564\n",
      "Epoch 390 training loss: 0.009350317530333996\n",
      "Epoch 391 training loss: 0.009850761853158474\n",
      "Epoch 392 training loss: 0.009269769303500652\n",
      "Epoch 393 training loss: 0.009540095925331116\n",
      "Epoch 394 training loss: 0.009438412263989449\n",
      "Epoch 395 training loss: 0.009274279698729515\n",
      "Epoch 396 training loss: 0.009731369093060493\n",
      "Epoch 397 training loss: 0.009290158748626709\n",
      "Epoch 398 training loss: 0.00943857990205288\n",
      "Epoch 399 training loss: 0.00940011814236641\n",
      "Epoch 400 training loss: 0.009673196822404861\n",
      "Epoch 401 training loss: 0.009381996467709541\n",
      "Epoch 402 training loss: 0.009437784552574158\n",
      "Epoch 403 training loss: 0.00949112419039011\n",
      "Epoch 404 training loss: 0.009520154446363449\n",
      "Epoch 405 training loss: 0.009454549290239811\n",
      "Epoch 406 training loss: 0.009419705718755722\n",
      "Epoch 407 training loss: 0.009524883702397346\n",
      "Epoch 408 training loss: 0.009538916870951653\n",
      "Epoch 409 training loss: 0.009261060506105423\n",
      "Epoch 410 training loss: 0.009212708100676537\n",
      "Epoch 411 training loss: 0.009389158338308334\n",
      "Epoch 412 training loss: 0.009568961337208748\n",
      "Epoch 413 training loss: 0.009404712356626987\n",
      "Epoch 414 training loss: 0.009479761123657227\n",
      "Epoch 415 training loss: 0.009104921482503414\n",
      "Epoch 416 training loss: 0.009668322280049324\n",
      "Epoch 417 training loss: 0.00929603073745966\n",
      "Epoch 418 training loss: 0.009567617438733578\n",
      "Epoch 419 training loss: 0.009306446649134159\n",
      "Epoch 420 training loss: 0.009673557244241238\n",
      "Epoch 421 training loss: 0.009112948551774025\n",
      "Epoch 422 training loss: 0.009274529293179512\n",
      "Epoch 423 training loss: 0.009748215787112713\n",
      "Epoch 424 training loss: 0.009228074923157692\n",
      "Epoch 425 training loss: 0.00903441570699215\n",
      "Epoch 426 training loss: 0.009638877585530281\n",
      "Epoch 427 training loss: 0.009023194201290607\n",
      "Epoch 428 training loss: 0.009368078783154488\n",
      "Epoch 429 training loss: 0.00941532850265503\n",
      "Epoch 430 training loss: 0.009169190190732479\n",
      "Epoch 431 training loss: 0.009141091257333755\n",
      "Epoch 432 training loss: 0.009646044112741947\n",
      "Epoch 433 training loss: 0.009081192314624786\n",
      "Epoch 434 training loss: 0.009540260769426823\n",
      "Epoch 435 training loss: 0.009164859540760517\n",
      "Epoch 436 training loss: 0.009486817754805088\n",
      "Epoch 437 training loss: 0.009112107567489147\n",
      "Epoch 438 training loss: 0.009274814277887344\n",
      "Epoch 439 training loss: 0.009296133182942867\n",
      "Epoch 440 training loss: 0.009168080985546112\n",
      "Epoch 441 training loss: 0.009260637685656548\n",
      "Epoch 442 training loss: 0.009262514300644398\n",
      "Epoch 443 training loss: 0.009085562080144882\n",
      "Epoch 444 training loss: 0.009202882647514343\n",
      "Epoch 445 training loss: 0.009413398802280426\n",
      "Epoch 446 training loss: 0.009133348241448402\n",
      "Epoch 447 training loss: 0.009137040004134178\n",
      "Epoch 448 training loss: 0.009242397733032703\n",
      "Epoch 449 training loss: 0.009239732287824154\n",
      "Epoch 450 training loss: 0.009371457621455193\n",
      "Epoch 451 training loss: 0.00934488046914339\n",
      "Epoch 452 training loss: 0.009194781072437763\n",
      "Epoch 453 training loss: 0.009371655993163586\n",
      "Epoch 454 training loss: 0.009027007035911083\n",
      "Epoch 455 training loss: 0.009318487718701363\n",
      "Epoch 456 training loss: 0.009240019135177135\n",
      "Epoch 457 training loss: 0.009474236518144608\n",
      "Epoch 458 training loss: 0.009028620086610317\n",
      "Epoch 459 training loss: 0.009033833630383015\n",
      "Epoch 460 training loss: 0.008953500539064407\n",
      "Epoch 461 training loss: 0.009265515953302383\n",
      "Epoch 462 training loss: 0.009097679518163204\n",
      "Epoch 463 training loss: 0.009329671040177345\n",
      "Epoch 464 training loss: 0.009469401091337204\n",
      "Epoch 465 training loss: 0.008978722617030144\n",
      "Epoch 466 training loss: 0.009172405116260052\n",
      "Epoch 467 training loss: 0.00906308926641941\n",
      "Epoch 468 training loss: 0.00920933485031128\n",
      "Epoch 469 training loss: 0.009196162223815918\n",
      "Epoch 470 training loss: 0.00904603861272335\n",
      "Epoch 471 training loss: 0.009224949404597282\n",
      "Epoch 472 training loss: 0.00923863798379898\n",
      "Epoch 473 training loss: 0.009064044803380966\n",
      "Epoch 474 training loss: 0.00897980760782957\n",
      "Epoch 475 training loss: 0.009276341646909714\n",
      "Epoch 476 training loss: 0.009001745842397213\n",
      "Epoch 477 training loss: 0.008977817371487617\n",
      "Epoch 478 training loss: 0.009038344956934452\n",
      "Epoch 479 training loss: 0.00916667003184557\n",
      "Epoch 480 training loss: 0.00911634974181652\n",
      "Epoch 481 training loss: 0.00910236407071352\n",
      "Epoch 482 training loss: 0.009248162619769573\n",
      "Epoch 483 training loss: 0.009188884869217873\n",
      "Epoch 484 training loss: 0.009109900332987309\n",
      "Epoch 485 training loss: 0.008822126314043999\n",
      "Epoch 486 training loss: 0.008925259113311768\n",
      "Epoch 487 training loss: 0.009184682741761208\n",
      "Epoch 488 training loss: 0.008965014480054379\n",
      "Epoch 489 training loss: 0.008863549679517746\n",
      "Epoch 490 training loss: 0.00938031729310751\n",
      "Epoch 491 training loss: 0.008836635388433933\n",
      "Epoch 492 training loss: 0.009050318971276283\n",
      "Epoch 493 training loss: 0.009250686503946781\n",
      "Epoch 494 training loss: 0.008893124759197235\n",
      "Epoch 495 training loss: 0.008961492218077183\n",
      "Epoch 496 training loss: 0.008829416707158089\n",
      "Epoch 497 training loss: 0.009320725686848164\n",
      "Epoch 498 training loss: 0.008952563628554344\n",
      "Epoch 499 training loss: 0.008839874528348446\n",
      "Epoch 500 training loss: 0.009156439453363419\n",
      "Epoch 501 training loss: 0.008898827247321606\n",
      "Epoch 502 training loss: 0.008777234703302383\n",
      "Epoch 503 training loss: 0.008794979192316532\n",
      "Epoch 504 training loss: 0.008908894844353199\n",
      "Epoch 505 training loss: 0.008988459594547749\n",
      "Epoch 506 training loss: 0.008977235294878483\n",
      "Epoch 507 training loss: 0.00879860669374466\n",
      "Epoch 508 training loss: 0.008927769958972931\n",
      "Epoch 509 training loss: 0.008836448192596436\n",
      "Epoch 510 training loss: 0.009097130037844181\n",
      "Epoch 511 training loss: 0.00904318131506443\n",
      "Epoch 512 training loss: 0.00870087742805481\n",
      "Epoch 513 training loss: 0.008634395897388458\n",
      "Epoch 514 training loss: 0.008848683908581734\n",
      "Epoch 515 training loss: 0.009062153287231922\n",
      "Epoch 516 training loss: 0.008889962919056416\n",
      "Epoch 517 training loss: 0.008752786554396152\n",
      "Epoch 518 training loss: 0.008854387328028679\n",
      "Epoch 519 training loss: 0.008867713622748852\n",
      "Epoch 520 training loss: 0.008795225992798805\n",
      "Epoch 521 training loss: 0.00894156564027071\n",
      "Epoch 522 training loss: 0.00883578322827816\n",
      "Epoch 523 training loss: 0.008985261432826519\n",
      "Epoch 524 training loss: 0.008717541582882404\n",
      "Epoch 525 training loss: 0.008579688146710396\n",
      "Epoch 526 training loss: 0.008715174160897732\n",
      "Epoch 527 training loss: 0.008909651078283787\n",
      "Epoch 528 training loss: 0.008900923654437065\n",
      "Epoch 529 training loss: 0.008715376257896423\n",
      "Epoch 530 training loss: 0.008561994880437851\n",
      "Epoch 531 training loss: 0.008716908283531666\n",
      "Epoch 532 training loss: 0.00891061034053564\n",
      "Epoch 533 training loss: 0.008749389089643955\n",
      "Epoch 534 training loss: 0.008957895450294018\n",
      "Epoch 535 training loss: 0.00877994392067194\n",
      "Epoch 536 training loss: 0.008759369142353535\n",
      "Epoch 537 training loss: 0.008641195483505726\n",
      "Epoch 538 training loss: 0.008670295588672161\n",
      "Epoch 539 training loss: 0.008944286033511162\n",
      "Epoch 540 training loss: 0.008929719217121601\n",
      "Epoch 541 training loss: 0.00873473659157753\n",
      "Epoch 542 training loss: 0.008933340199291706\n",
      "Epoch 543 training loss: 0.008694100193679333\n",
      "Epoch 544 training loss: 0.008475381880998611\n",
      "Epoch 545 training loss: 0.008677411824464798\n",
      "Epoch 546 training loss: 0.008898697793483734\n",
      "Epoch 547 training loss: 0.00868223886936903\n",
      "Epoch 548 training loss: 0.008795568719506264\n",
      "Epoch 549 training loss: 0.00891898013651371\n",
      "Epoch 550 training loss: 0.008634427562355995\n",
      "Epoch 551 training loss: 0.008677683770656586\n",
      "Epoch 552 training loss: 0.008790411055088043\n",
      "Epoch 553 training loss: 0.008700699545443058\n",
      "Epoch 554 training loss: 0.009051620028913021\n",
      "Epoch 555 training loss: 0.008593450300395489\n",
      "Epoch 556 training loss: 0.008639809675514698\n",
      "Epoch 557 training loss: 0.008811318315565586\n",
      "Epoch 558 training loss: 0.008699143305420876\n",
      "Epoch 559 training loss: 0.008663637563586235\n",
      "Epoch 560 training loss: 0.008647129870951176\n",
      "Epoch 561 training loss: 0.008563620038330555\n",
      "Epoch 562 training loss: 0.008810639381408691\n",
      "Epoch 563 training loss: 0.008913333527743816\n",
      "Epoch 564 training loss: 0.008843692019581795\n",
      "Epoch 565 training loss: 0.008675199933350086\n",
      "Epoch 566 training loss: 0.008512914180755615\n",
      "Epoch 567 training loss: 0.008758391253650188\n",
      "Epoch 568 training loss: 0.008729410357773304\n",
      "Epoch 569 training loss: 0.008661090396344662\n",
      "Epoch 570 training loss: 0.008738640695810318\n",
      "Epoch 571 training loss: 0.008451426401734352\n",
      "Epoch 572 training loss: 0.008696211501955986\n",
      "Epoch 573 training loss: 0.008737375028431416\n",
      "Epoch 574 training loss: 0.008923017419874668\n",
      "Epoch 575 training loss: 0.008602838031947613\n",
      "Epoch 576 training loss: 0.008652178570628166\n",
      "Epoch 577 training loss: 0.008894597180187702\n",
      "Epoch 578 training loss: 0.008335504680871964\n",
      "Epoch 579 training loss: 0.008541040122509003\n",
      "Epoch 580 training loss: 0.00883355550467968\n",
      "Epoch 581 training loss: 0.0084293894469738\n",
      "Epoch 582 training loss: 0.00897175632417202\n",
      "Epoch 583 training loss: 0.00840186607092619\n",
      "Epoch 584 training loss: 0.008730503730475903\n",
      "Epoch 585 training loss: 0.008630086667835712\n",
      "Epoch 586 training loss: 0.008576112799346447\n",
      "Epoch 587 training loss: 0.00872646551579237\n",
      "Epoch 588 training loss: 0.008646802045404911\n",
      "Epoch 589 training loss: 0.008578276261687279\n",
      "Epoch 590 training loss: 0.008513168431818485\n",
      "Epoch 591 training loss: 0.008587310090661049\n",
      "Epoch 592 training loss: 0.008604536764323711\n",
      "Epoch 593 training loss: 0.00893746130168438\n",
      "Epoch 594 training loss: 0.008510453626513481\n",
      "Epoch 595 training loss: 0.008675678633153439\n",
      "Epoch 596 training loss: 0.008451935835182667\n",
      "Epoch 597 training loss: 0.008521700277924538\n",
      "Epoch 598 training loss: 0.009011649526655674\n",
      "Epoch 599 training loss: 0.008504745550453663\n",
      "Epoch 600 training loss: 0.008573257364332676\n",
      "Epoch 601 training loss: 0.008619491942226887\n",
      "Epoch 602 training loss: 0.008843472227454185\n",
      "Epoch 603 training loss: 0.008994358591735363\n",
      "Epoch 604 training loss: 0.008469993248581886\n",
      "Epoch 605 training loss: 0.008445791900157928\n",
      "Epoch 606 training loss: 0.008532444015145302\n",
      "Epoch 607 training loss: 0.008463999256491661\n",
      "Epoch 608 training loss: 0.0086775878444314\n",
      "Epoch 609 training loss: 0.00856398232281208\n",
      "Epoch 610 training loss: 0.008671989664435387\n",
      "Epoch 611 training loss: 0.008435714058578014\n",
      "Epoch 612 training loss: 0.008434413000941277\n",
      "Epoch 613 training loss: 0.008898388594388962\n",
      "Epoch 614 training loss: 0.008828600868582726\n",
      "Epoch 615 training loss: 0.008280147798359394\n",
      "Epoch 616 training loss: 0.00852435827255249\n",
      "Epoch 617 training loss: 0.00848072487860918\n",
      "Epoch 618 training loss: 0.008488787338137627\n",
      "Epoch 619 training loss: 0.00874719861894846\n",
      "Epoch 620 training loss: 0.00855766050517559\n",
      "Epoch 621 training loss: 0.008398032747209072\n",
      "Epoch 622 training loss: 0.008556804619729519\n",
      "Epoch 623 training loss: 0.008449752815067768\n",
      "Epoch 624 training loss: 0.008492459543049335\n",
      "Epoch 625 training loss: 0.008899562060832977\n",
      "Epoch 626 training loss: 0.008824048563838005\n",
      "Epoch 627 training loss: 0.00858842022716999\n",
      "Epoch 628 training loss: 0.008401916362345219\n",
      "Epoch 629 training loss: 0.008345392532646656\n",
      "Epoch 630 training loss: 0.00889828335493803\n",
      "Epoch 631 training loss: 0.008557728491723537\n",
      "Epoch 632 training loss: 0.008350343443453312\n",
      "Epoch 633 training loss: 0.00859352108091116\n",
      "Epoch 634 training loss: 0.008457453921437263\n",
      "Epoch 635 training loss: 0.008814291097223759\n",
      "Epoch 636 training loss: 0.008441533893346786\n",
      "Epoch 637 training loss: 0.008450169116258621\n",
      "Epoch 638 training loss: 0.008427351713180542\n",
      "Epoch 639 training loss: 0.0085174310952425\n",
      "Epoch 640 training loss: 0.008523254655301571\n",
      "Epoch 641 training loss: 0.008497603237628937\n",
      "Epoch 642 training loss: 0.008581779897212982\n",
      "Epoch 643 training loss: 0.008732667192816734\n",
      "Epoch 644 training loss: 0.008699421770870686\n",
      "Epoch 645 training loss: 0.008363373577594757\n",
      "Epoch 646 training loss: 0.008329274132847786\n",
      "Epoch 647 training loss: 0.008902073837816715\n",
      "Epoch 648 training loss: 0.00874546729028225\n",
      "Epoch 649 training loss: 0.008514892309904099\n",
      "Epoch 650 training loss: 0.008297733031213284\n",
      "Epoch 651 training loss: 0.0088099529966712\n",
      "Epoch 652 training loss: 0.008288092911243439\n",
      "Epoch 653 training loss: 0.008332960307598114\n",
      "Epoch 654 training loss: 0.008592951111495495\n",
      "Epoch 655 training loss: 0.00888681598007679\n",
      "Epoch 656 training loss: 0.008335870690643787\n",
      "Epoch 657 training loss: 0.008519683964550495\n",
      "Epoch 658 training loss: 0.008554061874747276\n",
      "Epoch 659 training loss: 0.008631521835923195\n",
      "Epoch 660 training loss: 0.008430644869804382\n",
      "Epoch 661 training loss: 0.00824875570833683\n",
      "Epoch 662 training loss: 0.008599348366260529\n",
      "Epoch 663 training loss: 0.008515299297869205\n",
      "Epoch 664 training loss: 0.008835644461214542\n",
      "Epoch 665 training loss: 0.008942041546106339\n",
      "Epoch 666 training loss: 0.008754200302064419\n",
      "Epoch 667 training loss: 0.008416247554123402\n",
      "Epoch 668 training loss: 0.008281665854156017\n",
      "Epoch 669 training loss: 0.008340160362422466\n",
      "Epoch 670 training loss: 0.008376482874155045\n",
      "Epoch 671 training loss: 0.008862965740263462\n",
      "Epoch 672 training loss: 0.008259245194494724\n",
      "Epoch 673 training loss: 0.008938350714743137\n",
      "Epoch 674 training loss: 0.00822127889841795\n",
      "Epoch 675 training loss: 0.008435368537902832\n",
      "Epoch 676 training loss: 0.008347258903086185\n",
      "Epoch 677 training loss: 0.008762971498072147\n",
      "Epoch 678 training loss: 0.008370406925678253\n",
      "Epoch 679 training loss: 0.008297846652567387\n",
      "Epoch 680 training loss: 0.008381886407732964\n",
      "Epoch 681 training loss: 0.008609370328485966\n",
      "Epoch 682 training loss: 0.008612595498561859\n",
      "Epoch 683 training loss: 0.008434093557298183\n",
      "Epoch 684 training loss: 0.008489913307130337\n",
      "Epoch 685 training loss: 0.008396521210670471\n",
      "Epoch 686 training loss: 0.008247874677181244\n",
      "Epoch 687 training loss: 0.008861501701176167\n",
      "Epoch 688 training loss: 0.008430159650743008\n",
      "Epoch 689 training loss: 0.008324438706040382\n",
      "Epoch 690 training loss: 0.008738507516682148\n",
      "Epoch 691 training loss: 0.00825980119407177\n",
      "Epoch 692 training loss: 0.008512438274919987\n",
      "Epoch 693 training loss: 0.008478742092847824\n",
      "Epoch 694 training loss: 0.008423840627074242\n",
      "Epoch 695 training loss: 0.008339869789779186\n",
      "Epoch 696 training loss: 0.00839537288993597\n",
      "Epoch 697 training loss: 0.008555484935641289\n",
      "Epoch 698 training loss: 0.008531409315764904\n",
      "Epoch 699 training loss: 0.00833289884030819\n",
      "Epoch 700 training loss: 0.008344187401235104\n",
      "Epoch 701 training loss: 0.008707464672625065\n",
      "Epoch 702 training loss: 0.008305457420647144\n",
      "Epoch 703 training loss: 0.008581327274441719\n",
      "Epoch 704 training loss: 0.008271670900285244\n",
      "Epoch 705 training loss: 0.008621017448604107\n",
      "Epoch 706 training loss: 0.008348560892045498\n",
      "Epoch 707 training loss: 0.008338822983205318\n",
      "Epoch 708 training loss: 0.008211687207221985\n",
      "Epoch 709 training loss: 0.008209341205656528\n",
      "Epoch 710 training loss: 0.00874833669513464\n",
      "Epoch 711 training loss: 0.008205406367778778\n",
      "Epoch 712 training loss: 0.008615368977189064\n",
      "Epoch 713 training loss: 0.008283472619950771\n",
      "Epoch 714 training loss: 0.008651024661958218\n",
      "Epoch 715 training loss: 0.008192772045731544\n",
      "Epoch 716 training loss: 0.008076432161033154\n",
      "Epoch 717 training loss: 0.00862258579581976\n",
      "Epoch 718 training loss: 0.008246304467320442\n",
      "Epoch 719 training loss: 0.008493509143590927\n",
      "Epoch 720 training loss: 0.008234602399170399\n",
      "Epoch 721 training loss: 0.008709704503417015\n",
      "Epoch 722 training loss: 0.00837749894708395\n",
      "Epoch 723 training loss: 0.008326312527060509\n",
      "Epoch 724 training loss: 0.008404752239584923\n",
      "Epoch 725 training loss: 0.008679263293743134\n",
      "Epoch 726 training loss: 0.008234835229814053\n",
      "Epoch 727 training loss: 0.008328838273882866\n",
      "Epoch 728 training loss: 0.008505468256771564\n",
      "Epoch 729 training loss: 0.008255240507423878\n",
      "Epoch 730 training loss: 0.00834964495152235\n",
      "Epoch 731 training loss: 0.008417591452598572\n",
      "Epoch 732 training loss: 0.008446235209703445\n",
      "Epoch 733 training loss: 0.00825412105768919\n",
      "Epoch 734 training loss: 0.008568170480430126\n",
      "Epoch 735 training loss: 0.008329366333782673\n",
      "Epoch 736 training loss: 0.008233094587922096\n",
      "Epoch 737 training loss: 0.008374525234103203\n",
      "Epoch 738 training loss: 0.008557328954339027\n",
      "Epoch 739 training loss: 0.008348801173269749\n",
      "Epoch 740 training loss: 0.008513232693076134\n",
      "Epoch 741 training loss: 0.008199235424399376\n",
      "Epoch 742 training loss: 0.008383841253817081\n",
      "Epoch 743 training loss: 0.008428363129496574\n",
      "Epoch 744 training loss: 0.008375494740903378\n",
      "Epoch 745 training loss: 0.008556438609957695\n",
      "Epoch 746 training loss: 0.008249896578490734\n",
      "Epoch 747 training loss: 0.008369605988264084\n",
      "Epoch 748 training loss: 0.00842728465795517\n",
      "Epoch 749 training loss: 0.008390555158257484\n",
      "Epoch 750 training loss: 0.00822382140904665\n",
      "Epoch 751 training loss: 0.008525162935256958\n",
      "Epoch 752 training loss: 0.008210500702261925\n",
      "Epoch 753 training loss: 0.008203105069696903\n",
      "Epoch 754 training loss: 0.008248377591371536\n",
      "Epoch 755 training loss: 0.008400524035096169\n",
      "Epoch 756 training loss: 0.008361446671187878\n",
      "Epoch 757 training loss: 0.008429838344454765\n",
      "Epoch 758 training loss: 0.008207744918763638\n",
      "Epoch 759 training loss: 0.008533376269042492\n",
      "Epoch 760 training loss: 0.00832377839833498\n",
      "Epoch 761 training loss: 0.008343493565917015\n",
      "Epoch 762 training loss: 0.008451957255601883\n",
      "Epoch 763 training loss: 0.008307973854243755\n",
      "Epoch 764 training loss: 0.008073871023952961\n",
      "Epoch 765 training loss: 0.008419377729296684\n",
      "Epoch 766 training loss: 0.00831508170813322\n",
      "Epoch 767 training loss: 0.008404633030295372\n",
      "Epoch 768 training loss: 0.008276611566543579\n",
      "Epoch 769 training loss: 0.008289454504847527\n",
      "Epoch 770 training loss: 0.00835469551384449\n",
      "Epoch 771 training loss: 0.008271627128124237\n",
      "Epoch 772 training loss: 0.008123443461954594\n",
      "Epoch 773 training loss: 0.008311482146382332\n",
      "Epoch 774 training loss: 0.008332481607794762\n",
      "Epoch 775 training loss: 0.00850042887032032\n",
      "Epoch 776 training loss: 0.008141395635902882\n",
      "Epoch 777 training loss: 0.00825379230082035\n",
      "Epoch 778 training loss: 0.00854759942740202\n",
      "Epoch 779 training loss: 0.008038002997636795\n",
      "Epoch 780 training loss: 0.00849326141178608\n",
      "Epoch 781 training loss: 0.008361494168639183\n",
      "Epoch 782 training loss: 0.008202245458960533\n",
      "Epoch 783 training loss: 0.008329632692039013\n",
      "Epoch 784 training loss: 0.0082162544131279\n",
      "Epoch 785 training loss: 0.008536974899470806\n",
      "Epoch 786 training loss: 0.008297205902636051\n",
      "Epoch 787 training loss: 0.008304029703140259\n",
      "Epoch 788 training loss: 0.008089307695627213\n",
      "Epoch 789 training loss: 0.008323786780238152\n",
      "Epoch 790 training loss: 0.008280452340841293\n",
      "Epoch 791 training loss: 0.008332964964210987\n",
      "Epoch 792 training loss: 0.008190670982003212\n",
      "Epoch 793 training loss: 0.00806380808353424\n",
      "Epoch 794 training loss: 0.00832232553511858\n",
      "Epoch 795 training loss: 0.008089407347142696\n",
      "Epoch 796 training loss: 0.00834847241640091\n",
      "Epoch 797 training loss: 0.008181707002222538\n",
      "Epoch 798 training loss: 0.008219531737267971\n",
      "Epoch 799 training loss: 0.008350132964551449\n",
      "Epoch 800 training loss: 0.008372977375984192\n",
      "Epoch 801 training loss: 0.00838476326316595\n",
      "Epoch 802 training loss: 0.008106423541903496\n",
      "Epoch 803 training loss: 0.008327372372150421\n",
      "Epoch 804 training loss: 0.008255809545516968\n",
      "Epoch 805 training loss: 0.008268911391496658\n",
      "Epoch 806 training loss: 0.008530613034963608\n",
      "Epoch 807 training loss: 0.00815817341208458\n",
      "Epoch 808 training loss: 0.008048356510698795\n",
      "Epoch 809 training loss: 0.008414116688072681\n",
      "Epoch 810 training loss: 0.008171522058546543\n",
      "Epoch 811 training loss: 0.008117974735796452\n",
      "Epoch 812 training loss: 0.0083633316680789\n",
      "Epoch 813 training loss: 0.008549927733838558\n",
      "Epoch 814 training loss: 0.008096208795905113\n",
      "Epoch 815 training loss: 0.008069025352597237\n",
      "Epoch 816 training loss: 0.008234530687332153\n",
      "Epoch 817 training loss: 0.008362926542758942\n",
      "Epoch 818 training loss: 0.0082937590777874\n",
      "Epoch 819 training loss: 0.00838085263967514\n",
      "Epoch 820 training loss: 0.007946819998323917\n",
      "Epoch 821 training loss: 0.008629276417195797\n",
      "Epoch 822 training loss: 0.008136161603033543\n",
      "Epoch 823 training loss: 0.008451404981315136\n",
      "Epoch 824 training loss: 0.008016875945031643\n",
      "Epoch 825 training loss: 0.008135020732879639\n",
      "Epoch 826 training loss: 0.008340690284967422\n",
      "Epoch 827 training loss: 0.008241104893386364\n",
      "Epoch 828 training loss: 0.00804860983043909\n",
      "Epoch 829 training loss: 0.008165804669260979\n",
      "Epoch 830 training loss: 0.008503119461238384\n",
      "Epoch 831 training loss: 0.008041801862418652\n",
      "Epoch 832 training loss: 0.008150472305715084\n",
      "Epoch 833 training loss: 0.008238273672759533\n",
      "Epoch 834 training loss: 0.00842207483947277\n",
      "Epoch 835 training loss: 0.00794163066893816\n",
      "Epoch 836 training loss: 0.007966366596519947\n",
      "Epoch 837 training loss: 0.008485333994030952\n",
      "Epoch 838 training loss: 0.008179837837815285\n",
      "Epoch 839 training loss: 0.008123173378407955\n",
      "Epoch 840 training loss: 0.008738611824810505\n",
      "Epoch 841 training loss: 0.00811455026268959\n",
      "Epoch 842 training loss: 0.008233245462179184\n",
      "Epoch 843 training loss: 0.008221287280321121\n",
      "Epoch 844 training loss: 0.008015399798750877\n",
      "Epoch 845 training loss: 0.00825290847569704\n",
      "Epoch 846 training loss: 0.008078740909695625\n",
      "Epoch 847 training loss: 0.00810407567769289\n",
      "Epoch 848 training loss: 0.008353499695658684\n",
      "Epoch 849 training loss: 0.007969272322952747\n",
      "Epoch 850 training loss: 0.008077025413513184\n",
      "Epoch 851 training loss: 0.008188758045434952\n",
      "Epoch 852 training loss: 0.008319087326526642\n",
      "Epoch 853 training loss: 0.008403852581977844\n",
      "Epoch 854 training loss: 0.008066114038228989\n",
      "Epoch 855 training loss: 0.00811253022402525\n",
      "Epoch 856 training loss: 0.008299460634589195\n",
      "Epoch 857 training loss: 0.008071167394518852\n",
      "Epoch 858 training loss: 0.008334613405168056\n",
      "Epoch 859 training loss: 0.008091285824775696\n",
      "Epoch 860 training loss: 0.008197973482310772\n",
      "Epoch 861 training loss: 0.008229468017816544\n",
      "Epoch 862 training loss: 0.008094945922493935\n",
      "Epoch 863 training loss: 0.007987619377672672\n",
      "Epoch 864 training loss: 0.007987672463059425\n",
      "Epoch 865 training loss: 0.00832451693713665\n",
      "Epoch 866 training loss: 0.008015799336135387\n",
      "Epoch 867 training loss: 0.008071036078035831\n",
      "Epoch 868 training loss: 0.008461899124085903\n",
      "Epoch 869 training loss: 0.007908281870186329\n",
      "Epoch 870 training loss: 0.008162085898220539\n",
      "Epoch 871 training loss: 0.0081868851557374\n",
      "Epoch 872 training loss: 0.007957461290061474\n",
      "Epoch 873 training loss: 0.008269550278782845\n",
      "Epoch 874 training loss: 0.008311313576996326\n",
      "Epoch 875 training loss: 0.008181355893611908\n",
      "Epoch 876 training loss: 0.008286547847092152\n",
      "Epoch 877 training loss: 0.00796560663729906\n",
      "Epoch 878 training loss: 0.008442847989499569\n",
      "Epoch 879 training loss: 0.0078525859862566\n",
      "Epoch 880 training loss: 0.008054917678236961\n",
      "Epoch 881 training loss: 0.00786519143730402\n",
      "Epoch 882 training loss: 0.008084520697593689\n",
      "Epoch 883 training loss: 0.00812488328665495\n",
      "Epoch 884 training loss: 0.008223351091146469\n",
      "Epoch 885 training loss: 0.008452627807855606\n",
      "Epoch 886 training loss: 0.008224216289818287\n",
      "Epoch 887 training loss: 0.008170844055712223\n",
      "Epoch 888 training loss: 0.008170673623681068\n",
      "Epoch 889 training loss: 0.008139021694660187\n",
      "Epoch 890 training loss: 0.008048996329307556\n",
      "Epoch 891 training loss: 0.007927724160254002\n",
      "Epoch 892 training loss: 0.008257443085312843\n",
      "Epoch 893 training loss: 0.007967157289385796\n",
      "Epoch 894 training loss: 0.008310025557875633\n",
      "Epoch 895 training loss: 0.008067003451287746\n",
      "Epoch 896 training loss: 0.007879507727921009\n",
      "Epoch 897 training loss: 0.008204521611332893\n",
      "Epoch 898 training loss: 0.008250783197581768\n",
      "Epoch 899 training loss: 0.008380993269383907\n",
      "Epoch 900 training loss: 0.0081655103713274\n",
      "Epoch 901 training loss: 0.007930419407784939\n",
      "Epoch 902 training loss: 0.008173671551048756\n",
      "Epoch 903 training loss: 0.008150468580424786\n",
      "Epoch 904 training loss: 0.00801034364849329\n",
      "Epoch 905 training loss: 0.008282851427793503\n",
      "Epoch 906 training loss: 0.00824336800724268\n",
      "Epoch 907 training loss: 0.008055733516812325\n",
      "Epoch 908 training loss: 0.007957198657095432\n",
      "Epoch 909 training loss: 0.008485960774123669\n",
      "Epoch 910 training loss: 0.008157143369317055\n",
      "Epoch 911 training loss: 0.007952407002449036\n",
      "Epoch 912 training loss: 0.00816657766699791\n",
      "Epoch 913 training loss: 0.00822355691343546\n",
      "Epoch 914 training loss: 0.008089002221822739\n",
      "Epoch 915 training loss: 0.008222655393183231\n",
      "Epoch 916 training loss: 0.007993419654667377\n",
      "Epoch 917 training loss: 0.007968823425471783\n",
      "Epoch 918 training loss: 0.00786726363003254\n",
      "Epoch 919 training loss: 0.008179262280464172\n",
      "Epoch 920 training loss: 0.008089500479400158\n",
      "Epoch 921 training loss: 0.008252661675214767\n",
      "Epoch 922 training loss: 0.007902967743575573\n",
      "Epoch 923 training loss: 0.008088603615760803\n",
      "Epoch 924 training loss: 0.00795393530279398\n",
      "Epoch 925 training loss: 0.00843742024153471\n",
      "Epoch 926 training loss: 0.008203946053981781\n",
      "Epoch 927 training loss: 0.008110200054943562\n",
      "Epoch 928 training loss: 0.008017758838832378\n",
      "Epoch 929 training loss: 0.007927575148642063\n",
      "Epoch 930 training loss: 0.008204332552850246\n",
      "Epoch 931 training loss: 0.008019914850592613\n",
      "Epoch 932 training loss: 0.007878653705120087\n",
      "Epoch 933 training loss: 0.008142504841089249\n",
      "Epoch 934 training loss: 0.00791613943874836\n",
      "Epoch 935 training loss: 0.008241127245128155\n",
      "Epoch 936 training loss: 0.007954247295856476\n",
      "Epoch 937 training loss: 0.008061355911195278\n",
      "Epoch 938 training loss: 0.008000930771231651\n",
      "Epoch 939 training loss: 0.00848045852035284\n",
      "Epoch 940 training loss: 0.007844259962439537\n",
      "Epoch 941 training loss: 0.008175992406904697\n",
      "Epoch 942 training loss: 0.008373628370463848\n",
      "Epoch 943 training loss: 0.00788891687989235\n",
      "Epoch 944 training loss: 0.007885022088885307\n",
      "Epoch 945 training loss: 0.008033249527215958\n",
      "Epoch 946 training loss: 0.008127689361572266\n",
      "Epoch 947 training loss: 0.007993209175765514\n",
      "Epoch 948 training loss: 0.008069591596722603\n",
      "Epoch 949 training loss: 0.008113004267215729\n",
      "Epoch 950 training loss: 0.008223136886954308\n",
      "Epoch 951 training loss: 0.0080986637622118\n",
      "Epoch 952 training loss: 0.008567364886403084\n",
      "Epoch 953 training loss: 0.007925034500658512\n",
      "Epoch 954 training loss: 0.007787894457578659\n",
      "Epoch 955 training loss: 0.008008666336536407\n",
      "Epoch 956 training loss: 0.008018103428184986\n",
      "Epoch 957 training loss: 0.008013835176825523\n",
      "Epoch 958 training loss: 0.008552249521017075\n",
      "Epoch 959 training loss: 0.008167658932507038\n",
      "Epoch 960 training loss: 0.00791058037430048\n",
      "Epoch 961 training loss: 0.00810912624001503\n",
      "Epoch 962 training loss: 0.007888155989348888\n",
      "Epoch 963 training loss: 0.008090468123555183\n",
      "Epoch 964 training loss: 0.007987131364643574\n",
      "Epoch 965 training loss: 0.00822514109313488\n",
      "Epoch 966 training loss: 0.007906931452453136\n",
      "Epoch 967 training loss: 0.00786307081580162\n",
      "Epoch 968 training loss: 0.00808698683977127\n",
      "Epoch 969 training loss: 0.008557235822081566\n",
      "Epoch 970 training loss: 0.008153512142598629\n",
      "Epoch 971 training loss: 0.00804845616221428\n",
      "Epoch 972 training loss: 0.00796901248395443\n",
      "Epoch 973 training loss: 0.007923468016088009\n",
      "Epoch 974 training loss: 0.008047785609960556\n",
      "Epoch 975 training loss: 0.007918903604149818\n",
      "Epoch 976 training loss: 0.008550225757062435\n",
      "Epoch 977 training loss: 0.007977095432579517\n",
      "Epoch 978 training loss: 0.007885240018367767\n",
      "Epoch 979 training loss: 0.00797110516577959\n",
      "Epoch 980 training loss: 0.007930153049528599\n",
      "Epoch 981 training loss: 0.007779119536280632\n",
      "Epoch 982 training loss: 0.007876764051616192\n",
      "Epoch 983 training loss: 0.008316676132380962\n",
      "Epoch 984 training loss: 0.007886066101491451\n",
      "Epoch 985 training loss: 0.008085246197879314\n",
      "Epoch 986 training loss: 0.008012279868125916\n",
      "Epoch 987 training loss: 0.00818746816366911\n",
      "Epoch 988 training loss: 0.007836793549358845\n",
      "Epoch 989 training loss: 0.007961583323776722\n",
      "Epoch 990 training loss: 0.007888180203735828\n",
      "Epoch 991 training loss: 0.007990285754203796\n",
      "Epoch 992 training loss: 0.007929353043437004\n",
      "Epoch 993 training loss: 0.008198077790439129\n",
      "Epoch 994 training loss: 0.007890029810369015\n",
      "Epoch 995 training loss: 0.008147367276251316\n",
      "Epoch 996 training loss: 0.007968208752572536\n",
      "Epoch 997 training loss: 0.00807669572532177\n",
      "Epoch 998 training loss: 0.007986797019839287\n",
      "Epoch 999 training loss: 0.00827275775372982\n",
      "Epoch 1000 training loss: 0.008104848675429821\n",
      "Epoch 1001 training loss: 0.007799683604389429\n",
      "Epoch 1002 training loss: 0.008114656433463097\n",
      "Epoch 1003 training loss: 0.007744221016764641\n",
      "Epoch 1004 training loss: 0.008463077247142792\n",
      "Epoch 1005 training loss: 0.007965967990458012\n",
      "Epoch 1006 training loss: 0.00810154527425766\n",
      "Epoch 1007 training loss: 0.008131440728902817\n",
      "Epoch 1008 training loss: 0.007962842471897602\n",
      "Epoch 1009 training loss: 0.007958635687828064\n",
      "Epoch 1010 training loss: 0.007896624505519867\n",
      "Epoch 1011 training loss: 0.008353947661817074\n",
      "Epoch 1012 training loss: 0.007967730984091759\n",
      "Epoch 1013 training loss: 0.007838782854378223\n",
      "Epoch 1014 training loss: 0.008032127283513546\n",
      "Epoch 1015 training loss: 0.00811082310974598\n",
      "Epoch 1016 training loss: 0.007881744764745235\n",
      "Epoch 1017 training loss: 0.007972819730639458\n",
      "Epoch 1018 training loss: 0.008037892170250416\n",
      "Epoch 1019 training loss: 0.008198486641049385\n",
      "Epoch 1020 training loss: 0.00777027290314436\n",
      "Epoch 1021 training loss: 0.007727242540568113\n",
      "Epoch 1022 training loss: 0.007845277898013592\n",
      "Epoch 1023 training loss: 0.008046802133321762\n",
      "Epoch 1024 training loss: 0.008339457213878632\n",
      "0.6751824817518248\n",
      "[[6474 1545]\n",
      " [1926  741]]\n",
      "Accuracy: 0.6751824817518248\n",
      "Precision:  0.3241469816272966\n",
      "Recall:  0.2778402699662542\n",
      "F1:  0.2992125984251969\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06506173312664032\n",
      "Epoch 2 training loss: 0.07763776928186417\n",
      "Epoch 3 training loss: 0.07750710844993591\n",
      "Epoch 4 training loss: 0.07750604301691055\n",
      "Epoch 5 training loss: 0.0775478407740593\n",
      "Epoch 6 training loss: 0.07753785699605942\n",
      "Epoch 7 training loss: 0.07755934447050095\n",
      "Epoch 8 training loss: 0.077571801841259\n",
      "Epoch 9 training loss: 0.07756922394037247\n",
      "Epoch 10 training loss: 0.07756340503692627\n",
      "Epoch 11 training loss: 0.07754787057638168\n",
      "Epoch 12 training loss: 0.07752231508493423\n",
      "Epoch 13 training loss: 0.07755836099386215\n",
      "Epoch 14 training loss: 0.07758618146181107\n",
      "Epoch 15 training loss: 0.07756387442350388\n",
      "Epoch 16 training loss: 0.07757235318422318\n",
      "Epoch 17 training loss: 0.07750573009252548\n",
      "Epoch 18 training loss: 0.07753414660692215\n",
      "Epoch 19 training loss: 0.07756569981575012\n",
      "Epoch 20 training loss: 0.07760342955589294\n",
      "Epoch 21 training loss: 0.07752853631973267\n",
      "Epoch 22 training loss: 0.07759113609790802\n",
      "Epoch 23 training loss: 0.0775730237364769\n",
      "Epoch 24 training loss: 0.07748489081859589\n",
      "Epoch 25 training loss: 0.07759560644626617\n",
      "Epoch 26 training loss: 0.07760005444288254\n",
      "Epoch 27 training loss: 0.07755468040704727\n",
      "Epoch 28 training loss: 0.07757961750030518\n",
      "Epoch 29 training loss: 0.07765664905309677\n",
      "Epoch 30 training loss: 0.07753808051347733\n",
      "Epoch 31 training loss: 0.07755281776189804\n",
      "Epoch 32 training loss: 0.07756466418504715\n",
      "Epoch 33 training loss: 0.07744160294532776\n",
      "Epoch 34 training loss: 0.07760224491357803\n",
      "Epoch 35 training loss: 0.07759104669094086\n",
      "Epoch 36 training loss: 0.0775335431098938\n",
      "Epoch 37 training loss: 0.07760794460773468\n",
      "Epoch 38 training loss: 0.07758035510778427\n",
      "Epoch 39 training loss: 0.07764425128698349\n",
      "Epoch 40 training loss: 0.07746368646621704\n",
      "Epoch 41 training loss: 0.07755619287490845\n",
      "Epoch 42 training loss: 0.0775802955031395\n",
      "Epoch 43 training loss: 0.07760118693113327\n",
      "Epoch 44 training loss: 0.07758918404579163\n",
      "Epoch 45 training loss: 0.07755822688341141\n",
      "Epoch 46 training loss: 0.07765565812587738\n",
      "Epoch 47 training loss: 0.07766027003526688\n",
      "Epoch 48 training loss: 0.07756874710321426\n",
      "Epoch 49 training loss: 0.07753007113933563\n",
      "Epoch 50 training loss: 0.07750634849071503\n",
      "Epoch 51 training loss: 0.07748124748468399\n",
      "Epoch 52 training loss: 0.07757296413183212\n",
      "Epoch 53 training loss: 0.07763794809579849\n",
      "Epoch 54 training loss: 0.07752776890993118\n",
      "Epoch 55 training loss: 0.0775943249464035\n",
      "Epoch 56 training loss: 0.07758201658725739\n",
      "Epoch 57 training loss: 0.07751638442277908\n",
      "Epoch 58 training loss: 0.07758913934230804\n",
      "Epoch 59 training loss: 0.07758311927318573\n",
      "Epoch 60 training loss: 0.07748141139745712\n",
      "Epoch 61 training loss: 0.0774914026260376\n",
      "Epoch 62 training loss: 0.07765224575996399\n",
      "Epoch 63 training loss: 0.07758308947086334\n",
      "Epoch 64 training loss: 0.0774996429681778\n",
      "0.7880404267265582\n",
      "[[7219  800]\n",
      " [1465 1202]]\n",
      "Accuracy: 0.7880404267265582\n",
      "Precision:  0.6003996003996004\n",
      "Recall:  0.4506936632920885\n",
      "F1:  0.5148854144356393\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.66,0.60,0.72 ,0.61 ,0.80]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['blue', 'red', 'lime', 'orange', 'green'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Recall\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.95, 0.19, 0.84, 0.92, 0.44]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['blue', 'red', 'lime', 'orange', 'green'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07836276292800903\n",
      "Epoch 2 training loss: 0.07754524797201157\n",
      "Epoch 3 training loss: 0.07745422422885895\n",
      "Epoch 4 training loss: 0.07746409624814987\n",
      "Epoch 5 training loss: 0.07742595672607422\n",
      "Epoch 6 training loss: 0.07742536813020706\n",
      "Epoch 7 training loss: 0.07751432806253433\n",
      "Epoch 8 training loss: 0.0775236040353775\n",
      "Epoch 9 training loss: 0.07753631472587585\n",
      "Epoch 10 training loss: 0.0775417909026146\n",
      "Epoch 11 training loss: 0.07748011499643326\n",
      "Epoch 12 training loss: 0.0774666965007782\n",
      "Epoch 13 training loss: 0.07744150608778\n",
      "Epoch 14 training loss: 0.07743609696626663\n",
      "Epoch 15 training loss: 0.07757815718650818\n",
      "Epoch 16 training loss: 0.07755087316036224\n",
      "Epoch 17 training loss: 0.07752357423305511\n",
      "Epoch 18 training loss: 0.07744619250297546\n",
      "Epoch 19 training loss: 0.07757273316383362\n",
      "Epoch 20 training loss: 0.07744909822940826\n",
      "Epoch 21 training loss: 0.07758618891239166\n",
      "Epoch 22 training loss: 0.07743437588214874\n",
      "Epoch 23 training loss: 0.07756263762712479\n",
      "Epoch 24 training loss: 0.07752298563718796\n",
      "Epoch 25 training loss: 0.07752542942762375\n",
      "Epoch 26 training loss: 0.07756495475769043\n",
      "Epoch 27 training loss: 0.07750868797302246\n",
      "Epoch 28 training loss: 0.07753882557153702\n",
      "Epoch 29 training loss: 0.07749243080615997\n",
      "Epoch 30 training loss: 0.07750589400529861\n",
      "Epoch 31 training loss: 0.0774526298046112\n",
      "Epoch 32 training loss: 0.07752778381109238\n",
      "Epoch 33 training loss: 0.07742828130722046\n",
      "Epoch 34 training loss: 0.07756347954273224\n",
      "Epoch 35 training loss: 0.07752636820077896\n",
      "Epoch 36 training loss: 0.07749959826469421\n",
      "Epoch 37 training loss: 0.07742299139499664\n",
      "Epoch 38 training loss: 0.07745842635631561\n",
      "Epoch 39 training loss: 0.0775073692202568\n",
      "Epoch 40 training loss: 0.07751455157995224\n",
      "Epoch 41 training loss: 0.07746180891990662\n",
      "Epoch 42 training loss: 0.07754518836736679\n",
      "Epoch 43 training loss: 0.07750449329614639\n",
      "Epoch 44 training loss: 0.07747507840394974\n",
      "Epoch 45 training loss: 0.0774766057729721\n",
      "Epoch 46 training loss: 0.07754531502723694\n",
      "Epoch 47 training loss: 0.07754786312580109\n",
      "Epoch 48 training loss: 0.07752633094787598\n",
      "Epoch 49 training loss: 0.07751888781785965\n",
      "Epoch 50 training loss: 0.07747573405504227\n",
      "Epoch 51 training loss: 0.0775204673409462\n",
      "Epoch 52 training loss: 0.07747606933116913\n",
      "Epoch 53 training loss: 0.0774846151471138\n",
      "Epoch 54 training loss: 0.07765131443738937\n",
      "Epoch 55 training loss: 0.07753846049308777\n",
      "Epoch 56 training loss: 0.07741639763116837\n",
      "Epoch 57 training loss: 0.07751066237688065\n",
      "Epoch 58 training loss: 0.07752508670091629\n",
      "Epoch 59 training loss: 0.07754455506801605\n",
      "Epoch 60 training loss: 0.07749159634113312\n",
      "Epoch 61 training loss: 0.07746665179729462\n",
      "Epoch 62 training loss: 0.07758487015962601\n",
      "Epoch 63 training loss: 0.07753114402294159\n",
      "Epoch 64 training loss: 0.07749012112617493\n",
      "Epoch 65 training loss: 0.07751606404781342\n",
      "Epoch 66 training loss: 0.07754688709974289\n",
      "Epoch 67 training loss: 0.07752802968025208\n",
      "Epoch 68 training loss: 0.07753319293260574\n",
      "Epoch 69 training loss: 0.07754796743392944\n",
      "Epoch 70 training loss: 0.07742898911237717\n",
      "Epoch 71 training loss: 0.07756330072879791\n",
      "Epoch 72 training loss: 0.07753963768482208\n",
      "Epoch 73 training loss: 0.0775553435087204\n",
      "Epoch 74 training loss: 0.07745508849620819\n",
      "Epoch 75 training loss: 0.07741343230009079\n",
      "Epoch 76 training loss: 0.07757149636745453\n",
      "Epoch 77 training loss: 0.07751405984163284\n",
      "Epoch 78 training loss: 0.07746954262256622\n",
      "Epoch 79 training loss: 0.07744664698839188\n",
      "Epoch 80 training loss: 0.07746455073356628\n",
      "Epoch 81 training loss: 0.07753638178110123\n",
      "Epoch 82 training loss: 0.07749690115451813\n",
      "Epoch 83 training loss: 0.077464260160923\n",
      "Epoch 84 training loss: 0.07753986865282059\n",
      "Epoch 85 training loss: 0.07742704451084137\n",
      "Epoch 86 training loss: 0.07747697085142136\n",
      "Epoch 87 training loss: 0.0775236040353775\n",
      "Epoch 88 training loss: 0.07745207101106644\n",
      "Epoch 89 training loss: 0.0774829089641571\n",
      "Epoch 90 training loss: 0.07758104056119919\n",
      "Epoch 91 training loss: 0.0774734690785408\n",
      "Epoch 92 training loss: 0.07754381000995636\n",
      "Epoch 93 training loss: 0.07755278795957565\n",
      "Epoch 94 training loss: 0.07750848680734634\n",
      "Epoch 95 training loss: 0.07762368768453598\n",
      "Epoch 96 training loss: 0.07752431929111481\n",
      "Epoch 97 training loss: 0.0774645060300827\n",
      "Epoch 98 training loss: 0.07745029032230377\n",
      "Epoch 99 training loss: 0.07766041904687881\n",
      "Epoch 100 training loss: 0.07756766676902771\n",
      "Epoch 101 training loss: 0.07752783596515656\n",
      "Epoch 102 training loss: 0.07756087183952332\n",
      "Epoch 103 training loss: 0.0775577574968338\n",
      "Epoch 104 training loss: 0.0775250792503357\n",
      "Epoch 105 training loss: 0.0774771049618721\n",
      "Epoch 106 training loss: 0.0775931254029274\n",
      "Epoch 107 training loss: 0.07745718955993652\n",
      "Epoch 108 training loss: 0.07746520638465881\n",
      "Epoch 109 training loss: 0.07755742967128754\n",
      "Epoch 110 training loss: 0.07747315615415573\n",
      "Epoch 111 training loss: 0.07748185843229294\n",
      "Epoch 112 training loss: 0.07748344540596008\n",
      "Epoch 113 training loss: 0.07749829441308975\n",
      "Epoch 114 training loss: 0.07748587429523468\n",
      "Epoch 115 training loss: 0.07752258330583572\n",
      "Epoch 116 training loss: 0.07752960175275803\n",
      "Epoch 117 training loss: 0.07753312587738037\n",
      "Epoch 118 training loss: 0.07755168527364731\n",
      "Epoch 119 training loss: 0.077485591173172\n",
      "Epoch 120 training loss: 0.07754621654748917\n",
      "Epoch 121 training loss: 0.07759495079517365\n",
      "Epoch 122 training loss: 0.07758749276399612\n",
      "Epoch 123 training loss: 0.07742922008037567\n",
      "Epoch 124 training loss: 0.07758192718029022\n",
      "Epoch 125 training loss: 0.07751648873090744\n",
      "Epoch 126 training loss: 0.07739662379026413\n",
      "Epoch 127 training loss: 0.07750488817691803\n",
      "Epoch 128 training loss: 0.07753628492355347\n",
      "0.7885083286543141\n",
      "[[7254  765]\n",
      " [1495 1172]]\n",
      "Accuracy: 0.7885083286543141\n",
      "Precision:  0.6050593701600413\n",
      "Recall:  0.4394450693663292\n",
      "F1:  0.5091225021720244\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=128, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07671082764863968\n",
      "Epoch 2 training loss: 0.07774484157562256\n",
      "Epoch 3 training loss: 0.07758104056119919\n",
      "Epoch 4 training loss: 0.0776873528957367\n",
      "Epoch 5 training loss: 0.07762899994850159\n",
      "Epoch 6 training loss: 0.07762179523706436\n",
      "Epoch 7 training loss: 0.07758919149637222\n",
      "Epoch 8 training loss: 0.07759566605091095\n",
      "Epoch 9 training loss: 0.07760517299175262\n",
      "Epoch 10 training loss: 0.0775778740644455\n",
      "Epoch 11 training loss: 0.07756789773702621\n",
      "Epoch 12 training loss: 0.07762528210878372\n",
      "Epoch 13 training loss: 0.0775580108165741\n",
      "Epoch 14 training loss: 0.07766204327344894\n",
      "Epoch 15 training loss: 0.07756167650222778\n",
      "Epoch 16 training loss: 0.07755626738071442\n",
      "Epoch 17 training loss: 0.07752420753240585\n",
      "Epoch 18 training loss: 0.07770738750696182\n",
      "Epoch 19 training loss: 0.07764587551355362\n",
      "Epoch 20 training loss: 0.07763879001140594\n",
      "Epoch 21 training loss: 0.0775723084807396\n",
      "Epoch 22 training loss: 0.07760487496852875\n",
      "Epoch 23 training loss: 0.07760583609342575\n",
      "Epoch 24 training loss: 0.07767622172832489\n",
      "Epoch 25 training loss: 0.077566958963871\n",
      "Epoch 26 training loss: 0.07761817425489426\n",
      "Epoch 27 training loss: 0.07759112119674683\n",
      "Epoch 28 training loss: 0.07757755368947983\n",
      "Epoch 29 training loss: 0.07757992297410965\n",
      "Epoch 30 training loss: 0.07766376435756683\n",
      "Epoch 31 training loss: 0.07766436040401459\n",
      "Epoch 32 training loss: 0.07754664868116379\n",
      "Epoch 33 training loss: 0.07765118032693863\n",
      "Epoch 34 training loss: 0.07770110666751862\n",
      "Epoch 35 training loss: 0.07759861648082733\n",
      "Epoch 36 training loss: 0.07761016488075256\n",
      "Epoch 37 training loss: 0.07759099453687668\n",
      "Epoch 38 training loss: 0.07763438671827316\n",
      "Epoch 39 training loss: 0.07763101160526276\n",
      "Epoch 40 training loss: 0.07769833505153656\n",
      "Epoch 41 training loss: 0.07764451950788498\n",
      "Epoch 42 training loss: 0.0775180384516716\n",
      "Epoch 43 training loss: 0.0775904580950737\n",
      "Epoch 44 training loss: 0.07756936550140381\n",
      "Epoch 45 training loss: 0.07762428373098373\n",
      "Epoch 46 training loss: 0.07767461985349655\n",
      "Epoch 47 training loss: 0.07759710401296616\n",
      "Epoch 48 training loss: 0.07758623361587524\n",
      "Epoch 49 training loss: 0.07771499454975128\n",
      "Epoch 50 training loss: 0.07756959646940231\n",
      "Epoch 51 training loss: 0.07761362940073013\n",
      "Epoch 52 training loss: 0.07758773118257523\n",
      "Epoch 53 training loss: 0.0776667445898056\n",
      "Epoch 54 training loss: 0.07758443802595139\n",
      "Epoch 55 training loss: 0.07764260470867157\n",
      "Epoch 56 training loss: 0.07763233035802841\n",
      "Epoch 57 training loss: 0.07763491570949554\n",
      "Epoch 58 training loss: 0.07766663283109665\n",
      "Epoch 59 training loss: 0.07757079601287842\n",
      "Epoch 60 training loss: 0.07757335156202316\n",
      "Epoch 61 training loss: 0.07760395854711533\n",
      "Epoch 62 training loss: 0.07759684324264526\n",
      "Epoch 63 training loss: 0.0775836706161499\n",
      "Epoch 64 training loss: 0.0775899589061737\n",
      "Epoch 65 training loss: 0.07756321132183075\n",
      "Epoch 66 training loss: 0.07759265601634979\n",
      "Epoch 67 training loss: 0.07764234393835068\n",
      "Epoch 68 training loss: 0.07759659737348557\n",
      "Epoch 69 training loss: 0.07758046686649323\n",
      "Epoch 70 training loss: 0.07757614552974701\n",
      "Epoch 71 training loss: 0.0775943174958229\n",
      "Epoch 72 training loss: 0.07758015394210815\n",
      "Epoch 73 training loss: 0.07757732272148132\n",
      "Epoch 74 training loss: 0.07766517996788025\n",
      "Epoch 75 training loss: 0.07767260819673538\n",
      "Epoch 76 training loss: 0.07761723548173904\n",
      "Epoch 77 training loss: 0.07750751078128815\n",
      "Epoch 78 training loss: 0.07764042913913727\n",
      "Epoch 79 training loss: 0.07753411680459976\n",
      "Epoch 80 training loss: 0.07772655785083771\n",
      "Epoch 81 training loss: 0.0775831863284111\n",
      "Epoch 82 training loss: 0.07755720615386963\n",
      "Epoch 83 training loss: 0.07760614156723022\n",
      "Epoch 84 training loss: 0.0776400938630104\n",
      "Epoch 85 training loss: 0.07763523608446121\n",
      "Epoch 86 training loss: 0.07760606706142426\n",
      "Epoch 87 training loss: 0.07765348255634308\n",
      "Epoch 88 training loss: 0.07766085118055344\n",
      "Epoch 89 training loss: 0.077672079205513\n",
      "Epoch 90 training loss: 0.07752048224210739\n",
      "Epoch 91 training loss: 0.07770015299320221\n",
      "Epoch 92 training loss: 0.07758183032274246\n",
      "Epoch 93 training loss: 0.07755634188652039\n",
      "Epoch 94 training loss: 0.07761622220277786\n",
      "Epoch 95 training loss: 0.0775630846619606\n",
      "Epoch 96 training loss: 0.07767932116985321\n",
      "Epoch 97 training loss: 0.07768096029758453\n",
      "Epoch 98 training loss: 0.07752881944179535\n",
      "Epoch 99 training loss: 0.07765515148639679\n",
      "Epoch 100 training loss: 0.0775604173541069\n",
      "Epoch 101 training loss: 0.07759663462638855\n",
      "Epoch 102 training loss: 0.07768725603818893\n",
      "Epoch 103 training loss: 0.07762648910284042\n",
      "Epoch 104 training loss: 0.07758868485689163\n",
      "Epoch 105 training loss: 0.0775868147611618\n",
      "Epoch 106 training loss: 0.07766129076480865\n",
      "Epoch 107 training loss: 0.07772855460643768\n",
      "Epoch 108 training loss: 0.07758251577615738\n",
      "Epoch 109 training loss: 0.07760491967201233\n",
      "Epoch 110 training loss: 0.07764231413602829\n",
      "Epoch 111 training loss: 0.0775892361998558\n",
      "Epoch 112 training loss: 0.07758843153715134\n",
      "Epoch 113 training loss: 0.07772739231586456\n",
      "Epoch 114 training loss: 0.07762269675731659\n",
      "Epoch 115 training loss: 0.0775941014289856\n",
      "Epoch 116 training loss: 0.07764553278684616\n",
      "Epoch 117 training loss: 0.0776287391781807\n",
      "Epoch 118 training loss: 0.07769079506397247\n",
      "Epoch 119 training loss: 0.07756542414426804\n",
      "Epoch 120 training loss: 0.07771391421556473\n",
      "Epoch 121 training loss: 0.07764697819948196\n",
      "Epoch 122 training loss: 0.07759089022874832\n",
      "Epoch 123 training loss: 0.07762283086776733\n",
      "Epoch 124 training loss: 0.07759775221347809\n",
      "Epoch 125 training loss: 0.0776292085647583\n",
      "Epoch 126 training loss: 0.07754088193178177\n",
      "Epoch 127 training loss: 0.07766679674386978\n",
      "Epoch 128 training loss: 0.07755379378795624\n",
      "Epoch 129 training loss: 0.0776178166270256\n",
      "Epoch 130 training loss: 0.07763886451721191\n",
      "Epoch 131 training loss: 0.07772246748209\n",
      "Epoch 132 training loss: 0.07763322442770004\n",
      "Epoch 133 training loss: 0.07758484035730362\n",
      "Epoch 134 training loss: 0.07760639488697052\n",
      "Epoch 135 training loss: 0.07763289660215378\n",
      "Epoch 136 training loss: 0.07759334146976471\n",
      "Epoch 137 training loss: 0.07764237374067307\n",
      "Epoch 138 training loss: 0.07761350274085999\n",
      "Epoch 139 training loss: 0.07760065793991089\n",
      "Epoch 140 training loss: 0.07765910029411316\n",
      "Epoch 141 training loss: 0.07765854895114899\n",
      "Epoch 142 training loss: 0.07763496786355972\n",
      "Epoch 143 training loss: 0.07765138894319534\n",
      "Epoch 144 training loss: 0.07758627831935883\n",
      "Epoch 145 training loss: 0.07758340984582901\n",
      "Epoch 146 training loss: 0.07765757292509079\n",
      "Epoch 147 training loss: 0.07756420969963074\n",
      "Epoch 148 training loss: 0.07765162736177444\n",
      "Epoch 149 training loss: 0.07762852311134338\n",
      "Epoch 150 training loss: 0.07760800421237946\n",
      "Epoch 151 training loss: 0.0776330754160881\n",
      "Epoch 152 training loss: 0.07764632254838943\n",
      "Epoch 153 training loss: 0.0776669830083847\n",
      "Epoch 154 training loss: 0.07754931598901749\n",
      "Epoch 155 training loss: 0.07765773683786392\n",
      "Epoch 156 training loss: 0.07768759876489639\n",
      "Epoch 157 training loss: 0.07764530181884766\n",
      "Epoch 158 training loss: 0.07766154408454895\n",
      "Epoch 159 training loss: 0.07763496041297913\n",
      "Epoch 160 training loss: 0.07770293951034546\n",
      "Epoch 161 training loss: 0.07768436521291733\n",
      "Epoch 162 training loss: 0.07763361185789108\n",
      "Epoch 163 training loss: 0.07767033576965332\n",
      "Epoch 164 training loss: 0.07761845737695694\n",
      "Epoch 165 training loss: 0.07761767506599426\n",
      "Epoch 166 training loss: 0.07770571857690811\n",
      "Epoch 167 training loss: 0.07762331515550613\n",
      "Epoch 168 training loss: 0.0776299387216568\n",
      "Epoch 169 training loss: 0.07764502614736557\n",
      "Epoch 170 training loss: 0.07764612138271332\n",
      "Epoch 171 training loss: 0.07756055891513824\n",
      "Epoch 172 training loss: 0.07757668197154999\n",
      "Epoch 173 training loss: 0.07772035896778107\n",
      "Epoch 174 training loss: 0.07758448272943497\n",
      "Epoch 175 training loss: 0.07767076790332794\n",
      "Epoch 176 training loss: 0.0776175856590271\n",
      "Epoch 177 training loss: 0.07754480093717575\n",
      "Epoch 178 training loss: 0.0776127502322197\n",
      "Epoch 179 training loss: 0.07762517035007477\n",
      "Epoch 180 training loss: 0.07776308059692383\n",
      "Epoch 181 training loss: 0.07763423025608063\n",
      "Epoch 182 training loss: 0.07761142402887344\n",
      "Epoch 183 training loss: 0.07760974764823914\n",
      "Epoch 184 training loss: 0.07752551883459091\n",
      "Epoch 185 training loss: 0.07755537331104279\n",
      "Epoch 186 training loss: 0.07765010744333267\n",
      "Epoch 187 training loss: 0.07757207006216049\n",
      "Epoch 188 training loss: 0.0775916650891304\n",
      "Epoch 189 training loss: 0.07759594172239304\n",
      "Epoch 190 training loss: 0.0775829404592514\n",
      "Epoch 191 training loss: 0.07759348303079605\n",
      "Epoch 192 training loss: 0.07760407030582428\n",
      "Epoch 193 training loss: 0.07752960175275803\n",
      "Epoch 194 training loss: 0.07764121890068054\n",
      "Epoch 195 training loss: 0.07768580317497253\n",
      "Epoch 196 training loss: 0.0775526911020279\n",
      "Epoch 197 training loss: 0.07772492617368698\n",
      "Epoch 198 training loss: 0.07769017666578293\n",
      "Epoch 199 training loss: 0.07757384330034256\n",
      "Epoch 200 training loss: 0.07757104188203812\n",
      "Epoch 201 training loss: 0.0776626244187355\n",
      "Epoch 202 training loss: 0.07757481932640076\n",
      "Epoch 203 training loss: 0.07763496041297913\n",
      "Epoch 204 training loss: 0.07757257670164108\n",
      "Epoch 205 training loss: 0.07763861864805222\n",
      "Epoch 206 training loss: 0.07759913802146912\n",
      "Epoch 207 training loss: 0.0775759220123291\n",
      "Epoch 208 training loss: 0.07764729857444763\n",
      "Epoch 209 training loss: 0.07759971171617508\n",
      "Epoch 210 training loss: 0.0776003748178482\n",
      "Epoch 211 training loss: 0.07764237374067307\n",
      "Epoch 212 training loss: 0.07753093540668488\n",
      "Epoch 213 training loss: 0.07767020165920258\n",
      "Epoch 214 training loss: 0.07762041687965393\n",
      "Epoch 215 training loss: 0.07762878388166428\n",
      "Epoch 216 training loss: 0.07759271562099457\n",
      "Epoch 217 training loss: 0.07763125747442245\n",
      "Epoch 218 training loss: 0.07762091606855392\n",
      "Epoch 219 training loss: 0.07756307721138\n",
      "Epoch 220 training loss: 0.07768398523330688\n",
      "Epoch 221 training loss: 0.07766394317150116\n",
      "Epoch 222 training loss: 0.07762783765792847\n",
      "Epoch 223 training loss: 0.07762192189693451\n",
      "Epoch 224 training loss: 0.07753291726112366\n",
      "Epoch 225 training loss: 0.07767468690872192\n",
      "Epoch 226 training loss: 0.07772094011306763\n",
      "Epoch 227 training loss: 0.07759468257427216\n",
      "Epoch 228 training loss: 0.07759390771389008\n",
      "Epoch 229 training loss: 0.07761190831661224\n",
      "Epoch 230 training loss: 0.07758941501379013\n",
      "Epoch 231 training loss: 0.07767005264759064\n",
      "Epoch 232 training loss: 0.07759647816419601\n",
      "Epoch 233 training loss: 0.07762023061513901\n",
      "Epoch 234 training loss: 0.0776778981089592\n",
      "Epoch 235 training loss: 0.07763709872961044\n",
      "Epoch 236 training loss: 0.07761680334806442\n",
      "Epoch 237 training loss: 0.07755645364522934\n",
      "Epoch 238 training loss: 0.0776301696896553\n",
      "Epoch 239 training loss: 0.07768918573856354\n",
      "Epoch 240 training loss: 0.07760552316904068\n",
      "Epoch 241 training loss: 0.07761053740978241\n",
      "Epoch 242 training loss: 0.0776047334074974\n",
      "Epoch 243 training loss: 0.07758333534002304\n",
      "Epoch 244 training loss: 0.07760369777679443\n",
      "Epoch 245 training loss: 0.07760076224803925\n",
      "Epoch 246 training loss: 0.07767036557197571\n",
      "Epoch 247 training loss: 0.07763442397117615\n",
      "Epoch 248 training loss: 0.07763376086950302\n",
      "Epoch 249 training loss: 0.0775897428393364\n",
      "Epoch 250 training loss: 0.07767374068498611\n",
      "Epoch 251 training loss: 0.07754269242286682\n",
      "Epoch 252 training loss: 0.07758480310440063\n",
      "Epoch 253 training loss: 0.07766826450824738\n",
      "Epoch 254 training loss: 0.07766175270080566\n",
      "Epoch 255 training loss: 0.07765381783246994\n",
      "Epoch 256 training loss: 0.07768979668617249\n",
      "0.8001122964626615\n",
      "[[7357  662]\n",
      " [1474 1193]]\n",
      "Accuracy: 0.8001122964626615\n",
      "Precision:  0.6431266846361186\n",
      "Recall:  0.4473190851143607\n",
      "F1:  0.5276426360017691\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.07032772153615952\n",
      "Epoch 2 training loss: 0.07013164460659027\n",
      "Epoch 3 training loss: 0.06991345435380936\n",
      "Epoch 4 training loss: 0.07010791450738907\n",
      "Epoch 5 training loss: 0.07016096264123917\n",
      "Epoch 6 training loss: 0.07004042714834213\n",
      "Epoch 7 training loss: 0.07026807963848114\n",
      "Epoch 8 training loss: 0.07008042931556702\n",
      "Epoch 9 training loss: 0.07007605582475662\n",
      "Epoch 10 training loss: 0.07015755027532578\n",
      "Epoch 11 training loss: 0.07003292441368103\n",
      "Epoch 12 training loss: 0.0701369196176529\n",
      "Epoch 13 training loss: 0.07015486061573029\n",
      "Epoch 14 training loss: 0.07008297741413116\n",
      "Epoch 15 training loss: 0.0702085793018341\n",
      "Epoch 16 training loss: 0.07004798948764801\n",
      "Epoch 17 training loss: 0.07002263516187668\n",
      "Epoch 18 training loss: 0.07004742324352264\n",
      "Epoch 19 training loss: 0.07024405896663666\n",
      "Epoch 20 training loss: 0.07009511440992355\n",
      "Epoch 21 training loss: 0.07013522833585739\n",
      "Epoch 22 training loss: 0.07002323120832443\n",
      "Epoch 23 training loss: 0.07006151229143143\n",
      "Epoch 24 training loss: 0.0702165812253952\n",
      "Epoch 25 training loss: 0.07007678598165512\n",
      "Epoch 26 training loss: 0.07005652040243149\n",
      "Epoch 27 training loss: 0.07005822658538818\n",
      "Epoch 28 training loss: 0.07014302164316177\n",
      "Epoch 29 training loss: 0.07009068131446838\n",
      "Epoch 30 training loss: 0.07013352960348129\n",
      "Epoch 31 training loss: 0.070038802921772\n",
      "Epoch 32 training loss: 0.07014951854944229\n",
      "Epoch 33 training loss: 0.07018934935331345\n",
      "Epoch 34 training loss: 0.0701412707567215\n",
      "Epoch 35 training loss: 0.07001311331987381\n",
      "Epoch 36 training loss: 0.07018197327852249\n",
      "Epoch 37 training loss: 0.07011280953884125\n",
      "Epoch 38 training loss: 0.07002485543489456\n",
      "Epoch 39 training loss: 0.06997409462928772\n",
      "Epoch 40 training loss: 0.07003915309906006\n",
      "Epoch 41 training loss: 0.07016520947217941\n",
      "Epoch 42 training loss: 0.070044606924057\n",
      "Epoch 43 training loss: 0.07006324827671051\n",
      "Epoch 44 training loss: 0.07006672769784927\n",
      "Epoch 45 training loss: 0.07016246765851974\n",
      "Epoch 46 training loss: 0.07017835974693298\n",
      "Epoch 47 training loss: 0.0700092762708664\n",
      "Epoch 48 training loss: 0.07006949931383133\n",
      "Epoch 49 training loss: 0.07001554220914841\n",
      "Epoch 50 training loss: 0.070020891726017\n",
      "Epoch 51 training loss: 0.07002243399620056\n",
      "Epoch 52 training loss: 0.07005273550748825\n",
      "Epoch 53 training loss: 0.07006140053272247\n",
      "Epoch 54 training loss: 0.07006537169218063\n",
      "Epoch 55 training loss: 0.06995023787021637\n",
      "Epoch 56 training loss: 0.06996390223503113\n",
      "Epoch 57 training loss: 0.07003174722194672\n",
      "Epoch 58 training loss: 0.07006654143333435\n",
      "Epoch 59 training loss: 0.06999827176332474\n",
      "Epoch 60 training loss: 0.07009550929069519\n",
      "Epoch 61 training loss: 0.06996416300535202\n",
      "Epoch 62 training loss: 0.07025374472141266\n",
      "Epoch 63 training loss: 0.07017910480499268\n",
      "Epoch 64 training loss: 0.07017335295677185\n",
      "Epoch 65 training loss: 0.07019491493701935\n",
      "Epoch 66 training loss: 0.07011090964078903\n",
      "Epoch 67 training loss: 0.07006177306175232\n",
      "Epoch 68 training loss: 0.07005617767572403\n",
      "Epoch 69 training loss: 0.07005568593740463\n",
      "Epoch 70 training loss: 0.07001142203807831\n",
      "Epoch 71 training loss: 0.07001015543937683\n",
      "Epoch 72 training loss: 0.07013397663831711\n",
      "Epoch 73 training loss: 0.0701632872223854\n",
      "Epoch 74 training loss: 0.06998487561941147\n",
      "Epoch 75 training loss: 0.07013306021690369\n",
      "Epoch 76 training loss: 0.07019234448671341\n",
      "Epoch 77 training loss: 0.07012191414833069\n",
      "Epoch 78 training loss: 0.07012765109539032\n",
      "Epoch 79 training loss: 0.07005806267261505\n",
      "Epoch 80 training loss: 0.07000092417001724\n",
      "Epoch 81 training loss: 0.07001638412475586\n",
      "Epoch 82 training loss: 0.07009565830230713\n",
      "Epoch 83 training loss: 0.06999500095844269\n",
      "Epoch 84 training loss: 0.07008449733257294\n",
      "Epoch 85 training loss: 0.06995122879743576\n",
      "Epoch 86 training loss: 0.07005976140499115\n",
      "Epoch 87 training loss: 0.07007959485054016\n",
      "Epoch 88 training loss: 0.07007542252540588\n",
      "Epoch 89 training loss: 0.07002770155668259\n",
      "Epoch 90 training loss: 0.07012353837490082\n",
      "Epoch 91 training loss: 0.07008837163448334\n",
      "Epoch 92 training loss: 0.07004393637180328\n",
      "Epoch 93 training loss: 0.07022172957658768\n",
      "Epoch 94 training loss: 0.07002388685941696\n",
      "Epoch 95 training loss: 0.07004546374082565\n",
      "Epoch 96 training loss: 0.06999761611223221\n",
      "Epoch 97 training loss: 0.07017096877098083\n",
      "Epoch 98 training loss: 0.07014700770378113\n",
      "Epoch 99 training loss: 0.0701674148440361\n",
      "Epoch 100 training loss: 0.07007214426994324\n",
      "Epoch 101 training loss: 0.07001900672912598\n",
      "Epoch 102 training loss: 0.07003632932901382\n",
      "Epoch 103 training loss: 0.07007000595331192\n",
      "Epoch 104 training loss: 0.07006864249706268\n",
      "Epoch 105 training loss: 0.07010043412446976\n",
      "Epoch 106 training loss: 0.07004006206989288\n",
      "Epoch 107 training loss: 0.07001563161611557\n",
      "Epoch 108 training loss: 0.07004563510417938\n",
      "Epoch 109 training loss: 0.07022657990455627\n",
      "Epoch 110 training loss: 0.07007276266813278\n",
      "Epoch 111 training loss: 0.07013190537691116\n",
      "Epoch 112 training loss: 0.07008086144924164\n",
      "Epoch 113 training loss: 0.07014445960521698\n",
      "Epoch 114 training loss: 0.07003936916589737\n",
      "Epoch 115 training loss: 0.07003054767847061\n",
      "Epoch 116 training loss: 0.07018356770277023\n",
      "Epoch 117 training loss: 0.06997079402208328\n",
      "Epoch 118 training loss: 0.07005728781223297\n",
      "Epoch 119 training loss: 0.07004450261592865\n",
      "Epoch 120 training loss: 0.07006915658712387\n",
      "Epoch 121 training loss: 0.07014858722686768\n",
      "Epoch 122 training loss: 0.06993570178747177\n",
      "Epoch 123 training loss: 0.07016632705926895\n",
      "Epoch 124 training loss: 0.0700722187757492\n",
      "Epoch 125 training loss: 0.0700228363275528\n",
      "Epoch 126 training loss: 0.07020872086286545\n",
      "Epoch 127 training loss: 0.0702405571937561\n",
      "Epoch 128 training loss: 0.07002212107181549\n",
      "Epoch 129 training loss: 0.07011671364307404\n",
      "Epoch 130 training loss: 0.07012967765331268\n",
      "Epoch 131 training loss: 0.07003208994865417\n",
      "Epoch 132 training loss: 0.07009629905223846\n",
      "Epoch 133 training loss: 0.07006313651800156\n",
      "Epoch 134 training loss: 0.0699814036488533\n",
      "Epoch 135 training loss: 0.07020781934261322\n",
      "Epoch 136 training loss: 0.07011153548955917\n",
      "Epoch 137 training loss: 0.07003903388977051\n",
      "Epoch 138 training loss: 0.07007252424955368\n",
      "Epoch 139 training loss: 0.0701291561126709\n",
      "Epoch 140 training loss: 0.07015199214220047\n",
      "Epoch 141 training loss: 0.07018566131591797\n",
      "Epoch 142 training loss: 0.07003425806760788\n",
      "Epoch 143 training loss: 0.07000088691711426\n",
      "Epoch 144 training loss: 0.07013106346130371\n",
      "Epoch 145 training loss: 0.07008533179759979\n",
      "Epoch 146 training loss: 0.07004563510417938\n",
      "Epoch 147 training loss: 0.07007628679275513\n",
      "Epoch 148 training loss: 0.0700380951166153\n",
      "Epoch 149 training loss: 0.07006755471229553\n",
      "Epoch 150 training loss: 0.07020555436611176\n",
      "Epoch 151 training loss: 0.07014203816652298\n",
      "Epoch 152 training loss: 0.07002650201320648\n",
      "Epoch 153 training loss: 0.07005098462104797\n",
      "Epoch 154 training loss: 0.07008646428585052\n",
      "Epoch 155 training loss: 0.07013063132762909\n",
      "Epoch 156 training loss: 0.07014114409685135\n",
      "Epoch 157 training loss: 0.0700058788061142\n",
      "Epoch 158 training loss: 0.0700007900595665\n",
      "Epoch 159 training loss: 0.07015125453472137\n",
      "Epoch 160 training loss: 0.06996535509824753\n",
      "Epoch 161 training loss: 0.07001829147338867\n",
      "Epoch 162 training loss: 0.07020443677902222\n",
      "Epoch 163 training loss: 0.07003679871559143\n",
      "Epoch 164 training loss: 0.0699693113565445\n",
      "Epoch 165 training loss: 0.06999295204877853\n",
      "Epoch 166 training loss: 0.07010062783956528\n",
      "Epoch 167 training loss: 0.0701887309551239\n",
      "Epoch 168 training loss: 0.07006553560495377\n",
      "Epoch 169 training loss: 0.07019074261188507\n",
      "Epoch 170 training loss: 0.07005967944860458\n",
      "Epoch 171 training loss: 0.07015010714530945\n",
      "Epoch 172 training loss: 0.07021986693143845\n",
      "Epoch 173 training loss: 0.07012948393821716\n",
      "Epoch 174 training loss: 0.07003277540206909\n",
      "Epoch 175 training loss: 0.07014679908752441\n",
      "Epoch 176 training loss: 0.06998889893293381\n",
      "Epoch 177 training loss: 0.0700836256146431\n",
      "Epoch 178 training loss: 0.07014229148626328\n",
      "Epoch 179 training loss: 0.07000496238470078\n",
      "Epoch 180 training loss: 0.07008130103349686\n",
      "Epoch 181 training loss: 0.06997469067573547\n",
      "Epoch 182 training loss: 0.07001160830259323\n",
      "Epoch 183 training loss: 0.07002659142017365\n",
      "Epoch 184 training loss: 0.07007187604904175\n",
      "Epoch 185 training loss: 0.07005686312913895\n",
      "Epoch 186 training loss: 0.07002206891775131\n",
      "Epoch 187 training loss: 0.06999912858009338\n",
      "Epoch 188 training loss: 0.07019496709108353\n",
      "Epoch 189 training loss: 0.07001138478517532\n",
      "Epoch 190 training loss: 0.07009170204401016\n",
      "Epoch 191 training loss: 0.06999038904905319\n",
      "Epoch 192 training loss: 0.07000315189361572\n",
      "Epoch 193 training loss: 0.07003144174814224\n",
      "Epoch 194 training loss: 0.07009423524141312\n",
      "Epoch 195 training loss: 0.07001618295907974\n",
      "Epoch 196 training loss: 0.07014770805835724\n",
      "Epoch 197 training loss: 0.07011939585208893\n",
      "Epoch 198 training loss: 0.07003352046012878\n",
      "Epoch 199 training loss: 0.07004771381616592\n",
      "Epoch 200 training loss: 0.07009992748498917\n",
      "Epoch 201 training loss: 0.07004817575216293\n",
      "Epoch 202 training loss: 0.06994438916444778\n",
      "Epoch 203 training loss: 0.07018636167049408\n",
      "Epoch 204 training loss: 0.06996544450521469\n",
      "Epoch 205 training loss: 0.07006680220365524\n",
      "Epoch 206 training loss: 0.07007493078708649\n",
      "Epoch 207 training loss: 0.06989386677742004\n",
      "Epoch 208 training loss: 0.07018058747053146\n",
      "Epoch 209 training loss: 0.07006572932004929\n",
      "Epoch 210 training loss: 0.07010643184185028\n",
      "Epoch 211 training loss: 0.07017384469509125\n",
      "Epoch 212 training loss: 0.07010799646377563\n",
      "Epoch 213 training loss: 0.06995140016078949\n",
      "Epoch 214 training loss: 0.0701102763414383\n",
      "Epoch 215 training loss: 0.07007811963558197\n",
      "Epoch 216 training loss: 0.07003269344568253\n",
      "Epoch 217 training loss: 0.06999655812978745\n",
      "Epoch 218 training loss: 0.07013523578643799\n",
      "Epoch 219 training loss: 0.07003766298294067\n",
      "Epoch 220 training loss: 0.07019294798374176\n",
      "Epoch 221 training loss: 0.07018455862998962\n",
      "Epoch 222 training loss: 0.07000668346881866\n",
      "Epoch 223 training loss: 0.07019682228565216\n",
      "Epoch 224 training loss: 0.07004459202289581\n",
      "Epoch 225 training loss: 0.07007469236850739\n",
      "Epoch 226 training loss: 0.0701286643743515\n",
      "Epoch 227 training loss: 0.0700114369392395\n",
      "Epoch 228 training loss: 0.06999912858009338\n",
      "Epoch 229 training loss: 0.0700727179646492\n",
      "Epoch 230 training loss: 0.07018620520830154\n",
      "Epoch 231 training loss: 0.07015638798475266\n",
      "Epoch 232 training loss: 0.06994199007749557\n",
      "Epoch 233 training loss: 0.07000899314880371\n",
      "Epoch 234 training loss: 0.07010889053344727\n",
      "Epoch 235 training loss: 0.07008278369903564\n",
      "Epoch 236 training loss: 0.07006776332855225\n",
      "Epoch 237 training loss: 0.0700656846165657\n",
      "Epoch 238 training loss: 0.07016697525978088\n",
      "Epoch 239 training loss: 0.07003984600305557\n",
      "Epoch 240 training loss: 0.07009788602590561\n",
      "Epoch 241 training loss: 0.07022017985582352\n",
      "Epoch 242 training loss: 0.07007844746112823\n",
      "Epoch 243 training loss: 0.0700104609131813\n",
      "Epoch 244 training loss: 0.07003575563430786\n",
      "Epoch 245 training loss: 0.07017925381660461\n",
      "Epoch 246 training loss: 0.06999248266220093\n",
      "Epoch 247 training loss: 0.07006275653839111\n",
      "Epoch 248 training loss: 0.0700029730796814\n",
      "Epoch 249 training loss: 0.06994545459747314\n",
      "Epoch 250 training loss: 0.07008551061153412\n",
      "Epoch 251 training loss: 0.07017181068658829\n",
      "Epoch 252 training loss: 0.07008426636457443\n",
      "Epoch 253 training loss: 0.07000238448381424\n",
      "Epoch 254 training loss: 0.07011322677135468\n",
      "Epoch 255 training loss: 0.07004731893539429\n",
      "Epoch 256 training loss: 0.07000732421875\n",
      "0.751263335204941\n",
      "[[6985 1034]\n",
      " [1624 1043]]\n",
      "Accuracy: 0.751263335204941\n",
      "Precision:  0.502166586422725\n",
      "Recall:  0.3910761154855643\n",
      "F1:  0.4397133220910624\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=False\n",
    "\n",
    "autoenc = Autoencoder([40, 32, 24, 16, 8], [8, 16, 24, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07821372896432877\n",
      "Epoch 2 training loss: 0.0774301290512085\n",
      "Epoch 3 training loss: 0.07747204601764679\n",
      "Epoch 4 training loss: 0.07747378945350647\n",
      "Epoch 5 training loss: 0.0775231420993805\n",
      "Epoch 6 training loss: 0.07740684598684311\n",
      "Epoch 7 training loss: 0.07742051780223846\n",
      "Epoch 8 training loss: 0.07739140093326569\n",
      "Epoch 9 training loss: 0.07745695859193802\n",
      "Epoch 10 training loss: 0.07746768742799759\n",
      "Epoch 11 training loss: 0.07740789651870728\n",
      "Epoch 12 training loss: 0.077417753636837\n",
      "Epoch 13 training loss: 0.07751183211803436\n",
      "Epoch 14 training loss: 0.07744431495666504\n",
      "Epoch 15 training loss: 0.07744622230529785\n",
      "Epoch 16 training loss: 0.07750263810157776\n",
      "Epoch 17 training loss: 0.07744259387254715\n",
      "Epoch 18 training loss: 0.07745462656021118\n",
      "Epoch 19 training loss: 0.07743870466947556\n",
      "Epoch 20 training loss: 0.07745484262704849\n",
      "Epoch 21 training loss: 0.07742375880479813\n",
      "Epoch 22 training loss: 0.07747796922922134\n",
      "Epoch 23 training loss: 0.07749824970960617\n",
      "Epoch 24 training loss: 0.07748693227767944\n",
      "Epoch 25 training loss: 0.0774930864572525\n",
      "Epoch 26 training loss: 0.07741627097129822\n",
      "Epoch 27 training loss: 0.07749664783477783\n",
      "Epoch 28 training loss: 0.07742944359779358\n",
      "Epoch 29 training loss: 0.077445849776268\n",
      "Epoch 30 training loss: 0.07746104151010513\n",
      "Epoch 31 training loss: 0.07749255746603012\n",
      "Epoch 32 training loss: 0.0774720087647438\n",
      "Epoch 33 training loss: 0.0775042474269867\n",
      "Epoch 34 training loss: 0.07745180279016495\n",
      "Epoch 35 training loss: 0.07747648656368256\n",
      "Epoch 36 training loss: 0.07744362950325012\n",
      "Epoch 37 training loss: 0.07748481631278992\n",
      "Epoch 38 training loss: 0.07741475850343704\n",
      "Epoch 39 training loss: 0.07748692482709885\n",
      "Epoch 40 training loss: 0.07747168838977814\n",
      "Epoch 41 training loss: 0.07745791971683502\n",
      "Epoch 42 training loss: 0.07745706290006638\n",
      "Epoch 43 training loss: 0.07747597992420197\n",
      "Epoch 44 training loss: 0.07746551930904388\n",
      "Epoch 45 training loss: 0.07745984196662903\n",
      "Epoch 46 training loss: 0.07743500173091888\n",
      "Epoch 47 training loss: 0.07747801393270493\n",
      "Epoch 48 training loss: 0.07740675657987595\n",
      "Epoch 49 training loss: 0.07746816426515579\n",
      "Epoch 50 training loss: 0.07742462307214737\n",
      "Epoch 51 training loss: 0.07744874805212021\n",
      "Epoch 52 training loss: 0.07746195048093796\n",
      "Epoch 53 training loss: 0.0774989202618599\n",
      "Epoch 54 training loss: 0.07747622579336166\n",
      "Epoch 55 training loss: 0.077483631670475\n",
      "Epoch 56 training loss: 0.07751216739416122\n",
      "Epoch 57 training loss: 0.07748410850763321\n",
      "Epoch 58 training loss: 0.07746347784996033\n",
      "Epoch 59 training loss: 0.0774628147482872\n",
      "Epoch 60 training loss: 0.0774625614285469\n",
      "Epoch 61 training loss: 0.0775102823972702\n",
      "Epoch 62 training loss: 0.07749015837907791\n",
      "Epoch 63 training loss: 0.07745887339115143\n",
      "Epoch 64 training loss: 0.07746994495391846\n",
      "Epoch 65 training loss: 0.07750430703163147\n",
      "Epoch 66 training loss: 0.0774727314710617\n",
      "Epoch 67 training loss: 0.07748156785964966\n",
      "Epoch 68 training loss: 0.07744712382555008\n",
      "Epoch 69 training loss: 0.0774344727396965\n",
      "Epoch 70 training loss: 0.07744915038347244\n",
      "Epoch 71 training loss: 0.07742217183113098\n",
      "Epoch 72 training loss: 0.07739919424057007\n",
      "Epoch 73 training loss: 0.07746726274490356\n",
      "Epoch 74 training loss: 0.07747264951467514\n",
      "Epoch 75 training loss: 0.07750871777534485\n",
      "Epoch 76 training loss: 0.07747753709554672\n",
      "Epoch 77 training loss: 0.07746852934360504\n",
      "Epoch 78 training loss: 0.0774313360452652\n",
      "Epoch 79 training loss: 0.07749033719301224\n",
      "Epoch 80 training loss: 0.0773961991071701\n",
      "Epoch 81 training loss: 0.0774637758731842\n",
      "Epoch 82 training loss: 0.07737694680690765\n",
      "Epoch 83 training loss: 0.07749482244253159\n",
      "Epoch 84 training loss: 0.07744594663381577\n",
      "Epoch 85 training loss: 0.07748038321733475\n",
      "Epoch 86 training loss: 0.0774850994348526\n",
      "Epoch 87 training loss: 0.07746998965740204\n",
      "Epoch 88 training loss: 0.07747902721166611\n",
      "Epoch 89 training loss: 0.07742265611886978\n",
      "Epoch 90 training loss: 0.0774666890501976\n",
      "Epoch 91 training loss: 0.07746469229459763\n",
      "Epoch 92 training loss: 0.07741707563400269\n",
      "Epoch 93 training loss: 0.07745788246393204\n",
      "Epoch 94 training loss: 0.0774354487657547\n",
      "Epoch 95 training loss: 0.07748907804489136\n",
      "Epoch 96 training loss: 0.07747332751750946\n",
      "Epoch 97 training loss: 0.07735586911439896\n",
      "Epoch 98 training loss: 0.07749322801828384\n",
      "Epoch 99 training loss: 0.07746251672506332\n",
      "Epoch 100 training loss: 0.07745691388845444\n",
      "Epoch 101 training loss: 0.07745112478733063\n",
      "Epoch 102 training loss: 0.07746928185224533\n",
      "Epoch 103 training loss: 0.07747086137533188\n",
      "Epoch 104 training loss: 0.07748518139123917\n",
      "Epoch 105 training loss: 0.07754199951887131\n",
      "Epoch 106 training loss: 0.07755640149116516\n",
      "Epoch 107 training loss: 0.0774640366435051\n",
      "Epoch 108 training loss: 0.07746428996324539\n",
      "Epoch 109 training loss: 0.07750626653432846\n",
      "Epoch 110 training loss: 0.07746750861406326\n",
      "Epoch 111 training loss: 0.0774291381239891\n",
      "Epoch 112 training loss: 0.07747825980186462\n",
      "Epoch 113 training loss: 0.07743310928344727\n",
      "Epoch 114 training loss: 0.07740695774555206\n",
      "Epoch 115 training loss: 0.07746350020170212\n",
      "Epoch 116 training loss: 0.07749108970165253\n",
      "Epoch 117 training loss: 0.07750780135393143\n",
      "Epoch 118 training loss: 0.07743425667285919\n",
      "Epoch 119 training loss: 0.07744568586349487\n",
      "Epoch 120 training loss: 0.07744721323251724\n",
      "Epoch 121 training loss: 0.07746068388223648\n",
      "Epoch 122 training loss: 0.07747896015644073\n",
      "Epoch 123 training loss: 0.07740408927202225\n",
      "Epoch 124 training loss: 0.07749899476766586\n",
      "Epoch 125 training loss: 0.077507883310318\n",
      "Epoch 126 training loss: 0.07739807665348053\n",
      "Epoch 127 training loss: 0.07744429260492325\n",
      "Epoch 128 training loss: 0.07748968154191971\n",
      "Epoch 129 training loss: 0.07748547196388245\n",
      "Epoch 130 training loss: 0.07749123871326447\n",
      "Epoch 131 training loss: 0.07743554562330246\n",
      "Epoch 132 training loss: 0.07741477340459824\n",
      "Epoch 133 training loss: 0.07744339108467102\n",
      "Epoch 134 training loss: 0.07749395817518234\n",
      "Epoch 135 training loss: 0.07748646289110184\n",
      "Epoch 136 training loss: 0.0775308683514595\n",
      "Epoch 137 training loss: 0.07746300101280212\n",
      "Epoch 138 training loss: 0.07744716107845306\n",
      "Epoch 139 training loss: 0.07745543867349625\n",
      "Epoch 140 training loss: 0.07747106999158859\n",
      "Epoch 141 training loss: 0.07745051383972168\n",
      "Epoch 142 training loss: 0.07746978849172592\n",
      "Epoch 143 training loss: 0.077456034719944\n",
      "Epoch 144 training loss: 0.07744794338941574\n",
      "Epoch 145 training loss: 0.0774807408452034\n",
      "Epoch 146 training loss: 0.07742209732532501\n",
      "Epoch 147 training loss: 0.07745733112096786\n",
      "Epoch 148 training loss: 0.07743379473686218\n",
      "Epoch 149 training loss: 0.07741392403841019\n",
      "Epoch 150 training loss: 0.0774640142917633\n",
      "Epoch 151 training loss: 0.07747244834899902\n",
      "Epoch 152 training loss: 0.07745373249053955\n",
      "Epoch 153 training loss: 0.07748451083898544\n",
      "Epoch 154 training loss: 0.0774102658033371\n",
      "Epoch 155 training loss: 0.07750198990106583\n",
      "Epoch 156 training loss: 0.07746578752994537\n",
      "Epoch 157 training loss: 0.07745657861232758\n",
      "Epoch 158 training loss: 0.07747791707515717\n",
      "Epoch 159 training loss: 0.07744302600622177\n",
      "Epoch 160 training loss: 0.07743103802204132\n",
      "Epoch 161 training loss: 0.07743555307388306\n",
      "Epoch 162 training loss: 0.07745924592018127\n",
      "Epoch 163 training loss: 0.07748781144618988\n",
      "Epoch 164 training loss: 0.07747867703437805\n",
      "Epoch 165 training loss: 0.07753469049930573\n",
      "Epoch 166 training loss: 0.07745355367660522\n",
      "Epoch 167 training loss: 0.07746531814336777\n",
      "Epoch 168 training loss: 0.07744434475898743\n",
      "Epoch 169 training loss: 0.07748933136463165\n",
      "Epoch 170 training loss: 0.07748561352491379\n",
      "Epoch 171 training loss: 0.07745755463838577\n",
      "Epoch 172 training loss: 0.07743830233812332\n",
      "Epoch 173 training loss: 0.0774468258023262\n",
      "Epoch 174 training loss: 0.07749338448047638\n",
      "Epoch 175 training loss: 0.0773722380399704\n",
      "Epoch 176 training loss: 0.07745666801929474\n",
      "Epoch 177 training loss: 0.07743725925683975\n",
      "Epoch 178 training loss: 0.07743974030017853\n",
      "Epoch 179 training loss: 0.0774901807308197\n",
      "Epoch 180 training loss: 0.07750094681978226\n",
      "Epoch 181 training loss: 0.07740659266710281\n",
      "Epoch 182 training loss: 0.07747036218643188\n",
      "Epoch 183 training loss: 0.07746116816997528\n",
      "Epoch 184 training loss: 0.07747870683670044\n",
      "Epoch 185 training loss: 0.07743918895721436\n",
      "Epoch 186 training loss: 0.07744904607534409\n",
      "Epoch 187 training loss: 0.0774957686662674\n",
      "Epoch 188 training loss: 0.07743287831544876\n",
      "Epoch 189 training loss: 0.07750769704580307\n",
      "Epoch 190 training loss: 0.07741899788379669\n",
      "Epoch 191 training loss: 0.07744442671537399\n",
      "Epoch 192 training loss: 0.07748144119977951\n",
      "Epoch 193 training loss: 0.07739192247390747\n",
      "Epoch 194 training loss: 0.07749181985855103\n",
      "Epoch 195 training loss: 0.07746051996946335\n",
      "Epoch 196 training loss: 0.07749651372432709\n",
      "Epoch 197 training loss: 0.07749483734369278\n",
      "Epoch 198 training loss: 0.07747551798820496\n",
      "Epoch 199 training loss: 0.07748185843229294\n",
      "Epoch 200 training loss: 0.0774434432387352\n",
      "Epoch 201 training loss: 0.07755545526742935\n",
      "Epoch 202 training loss: 0.07741467654705048\n",
      "Epoch 203 training loss: 0.0774427056312561\n",
      "Epoch 204 training loss: 0.07745103538036346\n",
      "Epoch 205 training loss: 0.07738429307937622\n",
      "Epoch 206 training loss: 0.07745025306940079\n",
      "Epoch 207 training loss: 0.07749398052692413\n",
      "Epoch 208 training loss: 0.07742059230804443\n",
      "Epoch 209 training loss: 0.0774666965007782\n",
      "Epoch 210 training loss: 0.07747874408960342\n",
      "Epoch 211 training loss: 0.07745484262704849\n",
      "Epoch 212 training loss: 0.0774792730808258\n",
      "Epoch 213 training loss: 0.07748302072286606\n",
      "Epoch 214 training loss: 0.07746487855911255\n",
      "Epoch 215 training loss: 0.07742641866207123\n",
      "Epoch 216 training loss: 0.07749319076538086\n",
      "Epoch 217 training loss: 0.07744765281677246\n",
      "Epoch 218 training loss: 0.07742835581302643\n",
      "Epoch 219 training loss: 0.07747877389192581\n",
      "Epoch 220 training loss: 0.07748446613550186\n",
      "Epoch 221 training loss: 0.07743504643440247\n",
      "Epoch 222 training loss: 0.07745642960071564\n",
      "Epoch 223 training loss: 0.07742395251989365\n",
      "Epoch 224 training loss: 0.07746625691652298\n",
      "Epoch 225 training loss: 0.0774972140789032\n",
      "Epoch 226 training loss: 0.07750188559293747\n",
      "Epoch 227 training loss: 0.07748524844646454\n",
      "Epoch 228 training loss: 0.07740265876054764\n",
      "Epoch 229 training loss: 0.07743129134178162\n",
      "Epoch 230 training loss: 0.07745473086833954\n",
      "Epoch 231 training loss: 0.07744167745113373\n",
      "Epoch 232 training loss: 0.07739246636629105\n",
      "Epoch 233 training loss: 0.07748404890298843\n",
      "Epoch 234 training loss: 0.07746339589357376\n",
      "Epoch 235 training loss: 0.07747901976108551\n",
      "Epoch 236 training loss: 0.07743935286998749\n",
      "Epoch 237 training loss: 0.07743876427412033\n",
      "Epoch 238 training loss: 0.07751055061817169\n",
      "Epoch 239 training loss: 0.07753288000822067\n",
      "Epoch 240 training loss: 0.07750222831964493\n",
      "Epoch 241 training loss: 0.07745762169361115\n",
      "Epoch 242 training loss: 0.07749946415424347\n",
      "Epoch 243 training loss: 0.0774020105600357\n",
      "Epoch 244 training loss: 0.07749827951192856\n",
      "Epoch 245 training loss: 0.07745644450187683\n",
      "Epoch 246 training loss: 0.07756714522838593\n",
      "Epoch 247 training loss: 0.07754246890544891\n",
      "Epoch 248 training loss: 0.07744432985782623\n",
      "Epoch 249 training loss: 0.07746729999780655\n",
      "Epoch 250 training loss: 0.07744508981704712\n",
      "Epoch 251 training loss: 0.0774877741932869\n",
      "Epoch 252 training loss: 0.07747697085142136\n",
      "Epoch 253 training loss: 0.07743818312883377\n",
      "Epoch 254 training loss: 0.0775001272559166\n",
      "Epoch 255 training loss: 0.07746410369873047\n",
      "Epoch 256 training loss: 0.07744985073804855\n",
      "0.7330151600224593\n",
      "[[6183 1836]\n",
      " [1017 1650]]\n",
      "Accuracy: 0.7330151600224593\n",
      "Precision:  0.47332185886402756\n",
      "Recall:  0.6186726659167604\n",
      "F1:  0.5363237445148707\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07812798023223877\n",
      "Epoch 2 training loss: 0.07743661850690842\n",
      "Epoch 3 training loss: 0.07740376144647598\n",
      "Epoch 4 training loss: 0.07743975520133972\n",
      "Epoch 5 training loss: 0.07742688804864883\n",
      "Epoch 6 training loss: 0.07745155692100525\n",
      "Epoch 7 training loss: 0.0774465948343277\n",
      "Epoch 8 training loss: 0.07741744071245193\n",
      "Epoch 9 training loss: 0.07745532691478729\n",
      "Epoch 10 training loss: 0.07742851227521896\n",
      "Epoch 11 training loss: 0.07746095210313797\n",
      "Epoch 12 training loss: 0.07742488384246826\n",
      "Epoch 13 training loss: 0.07747579365968704\n",
      "Epoch 14 training loss: 0.07744032144546509\n",
      "Epoch 15 training loss: 0.07742897421121597\n",
      "Epoch 16 training loss: 0.07751155644655228\n",
      "Epoch 17 training loss: 0.07739351689815521\n",
      "Epoch 18 training loss: 0.07740240544080734\n",
      "Epoch 19 training loss: 0.07748274505138397\n",
      "Epoch 20 training loss: 0.07744436711072922\n",
      "Epoch 21 training loss: 0.0774078294634819\n",
      "Epoch 22 training loss: 0.07739578932523727\n",
      "Epoch 23 training loss: 0.07742733508348465\n",
      "Epoch 24 training loss: 0.07743795961141586\n",
      "Epoch 25 training loss: 0.07744920253753662\n",
      "Epoch 26 training loss: 0.0773923397064209\n",
      "Epoch 27 training loss: 0.07746272534132004\n",
      "Epoch 28 training loss: 0.07750817388296127\n",
      "Epoch 29 training loss: 0.07742610573768616\n",
      "Epoch 30 training loss: 0.0774325430393219\n",
      "Epoch 31 training loss: 0.07742555439472198\n",
      "Epoch 32 training loss: 0.07745441049337387\n",
      "Epoch 33 training loss: 0.07742538303136826\n",
      "Epoch 34 training loss: 0.07744311541318893\n",
      "Epoch 35 training loss: 0.07742466777563095\n",
      "Epoch 36 training loss: 0.07745049148797989\n",
      "Epoch 37 training loss: 0.07742957770824432\n",
      "Epoch 38 training loss: 0.07740917801856995\n",
      "Epoch 39 training loss: 0.07737935334444046\n",
      "Epoch 40 training loss: 0.07746833562850952\n",
      "Epoch 41 training loss: 0.07742499560117722\n",
      "Epoch 42 training loss: 0.07744681090116501\n",
      "Epoch 43 training loss: 0.07743407040834427\n",
      "Epoch 44 training loss: 0.07742394506931305\n",
      "Epoch 45 training loss: 0.07745876163244247\n",
      "Epoch 46 training loss: 0.07740865647792816\n",
      "Epoch 47 training loss: 0.07742428779602051\n",
      "Epoch 48 training loss: 0.07749349623918533\n",
      "Epoch 49 training loss: 0.07746322453022003\n",
      "Epoch 50 training loss: 0.07737366855144501\n",
      "Epoch 51 training loss: 0.07741657644510269\n",
      "Epoch 52 training loss: 0.07747690379619598\n",
      "Epoch 53 training loss: 0.07742226868867874\n",
      "Epoch 54 training loss: 0.07741229236125946\n",
      "Epoch 55 training loss: 0.07743892073631287\n",
      "Epoch 56 training loss: 0.07744564861059189\n",
      "Epoch 57 training loss: 0.07745062559843063\n",
      "Epoch 58 training loss: 0.07746175676584244\n",
      "Epoch 59 training loss: 0.0773690789937973\n",
      "Epoch 60 training loss: 0.07740002870559692\n",
      "Epoch 61 training loss: 0.07745984196662903\n",
      "Epoch 62 training loss: 0.07751363515853882\n",
      "Epoch 63 training loss: 0.07748191058635712\n",
      "Epoch 64 training loss: 0.07743840664625168\n",
      "Epoch 65 training loss: 0.07747188955545425\n",
      "Epoch 66 training loss: 0.07738712430000305\n",
      "Epoch 67 training loss: 0.07740531116724014\n",
      "Epoch 68 training loss: 0.07740769535303116\n",
      "Epoch 69 training loss: 0.07745185494422913\n",
      "Epoch 70 training loss: 0.07747133076190948\n",
      "Epoch 71 training loss: 0.07744130492210388\n",
      "Epoch 72 training loss: 0.07746735960245132\n",
      "Epoch 73 training loss: 0.07742126286029816\n",
      "Epoch 74 training loss: 0.0774296224117279\n",
      "Epoch 75 training loss: 0.07747523486614227\n",
      "Epoch 76 training loss: 0.07737012952566147\n",
      "Epoch 77 training loss: 0.07746380567550659\n",
      "Epoch 78 training loss: 0.07742446660995483\n",
      "Epoch 79 training loss: 0.0774160772562027\n",
      "Epoch 80 training loss: 0.07743265479803085\n",
      "Epoch 81 training loss: 0.07742413133382797\n",
      "Epoch 82 training loss: 0.07740895450115204\n",
      "Epoch 83 training loss: 0.07746709883213043\n",
      "Epoch 84 training loss: 0.07745310664176941\n",
      "Epoch 85 training loss: 0.07742098718881607\n",
      "Epoch 86 training loss: 0.07739657908678055\n",
      "Epoch 87 training loss: 0.0774468183517456\n",
      "Epoch 88 training loss: 0.07743831723928452\n",
      "Epoch 89 training loss: 0.07748374342918396\n",
      "Epoch 90 training loss: 0.07743804901838303\n",
      "Epoch 91 training loss: 0.0774640440940857\n",
      "Epoch 92 training loss: 0.07744201272726059\n",
      "Epoch 93 training loss: 0.07743050903081894\n",
      "Epoch 94 training loss: 0.07742185145616531\n",
      "Epoch 95 training loss: 0.07744797319173813\n",
      "Epoch 96 training loss: 0.07747352868318558\n",
      "Epoch 97 training loss: 0.07741480320692062\n",
      "Epoch 98 training loss: 0.07743187993764877\n",
      "Epoch 99 training loss: 0.0774860605597496\n",
      "Epoch 100 training loss: 0.0774078294634819\n",
      "Epoch 101 training loss: 0.07745438814163208\n",
      "Epoch 102 training loss: 0.07744579762220383\n",
      "Epoch 103 training loss: 0.07746630162000656\n",
      "Epoch 104 training loss: 0.07745155692100525\n",
      "Epoch 105 training loss: 0.0774223580956459\n",
      "Epoch 106 training loss: 0.07742784917354584\n",
      "Epoch 107 training loss: 0.07745251804590225\n",
      "Epoch 108 training loss: 0.07742860913276672\n",
      "Epoch 109 training loss: 0.07747869938611984\n",
      "Epoch 110 training loss: 0.0774160772562027\n",
      "Epoch 111 training loss: 0.0774698406457901\n",
      "Epoch 112 training loss: 0.07744206488132477\n",
      "Epoch 113 training loss: 0.07739043235778809\n",
      "Epoch 114 training loss: 0.0774000734090805\n",
      "Epoch 115 training loss: 0.07745474576950073\n",
      "Epoch 116 training loss: 0.0774509385228157\n",
      "Epoch 117 training loss: 0.07743088155984879\n",
      "Epoch 118 training loss: 0.07746271044015884\n",
      "Epoch 119 training loss: 0.07748843729496002\n",
      "Epoch 120 training loss: 0.07750355452299118\n",
      "Epoch 121 training loss: 0.07747828215360641\n",
      "Epoch 122 training loss: 0.07738235592842102\n",
      "Epoch 123 training loss: 0.07739520072937012\n",
      "Epoch 124 training loss: 0.07751726359128952\n",
      "Epoch 125 training loss: 0.07738599926233292\n",
      "Epoch 126 training loss: 0.07745524495840073\n",
      "Epoch 127 training loss: 0.07741500437259674\n",
      "Epoch 128 training loss: 0.07742521911859512\n",
      "Epoch 129 training loss: 0.0774087905883789\n",
      "Epoch 130 training loss: 0.07746502012014389\n",
      "Epoch 131 training loss: 0.07741180062294006\n",
      "Epoch 132 training loss: 0.07736284285783768\n",
      "Epoch 133 training loss: 0.07748546451330185\n",
      "Epoch 134 training loss: 0.0774245485663414\n",
      "Epoch 135 training loss: 0.07741770893335342\n",
      "Epoch 136 training loss: 0.07736793160438538\n",
      "Epoch 137 training loss: 0.07739600539207458\n",
      "Epoch 138 training loss: 0.07742960005998611\n",
      "Epoch 139 training loss: 0.07744214683771133\n",
      "Epoch 140 training loss: 0.07741190493106842\n",
      "Epoch 141 training loss: 0.07742875069379807\n",
      "Epoch 142 training loss: 0.07742192596197128\n",
      "Epoch 143 training loss: 0.07738184928894043\n",
      "Epoch 144 training loss: 0.07744675874710083\n",
      "Epoch 145 training loss: 0.07743664085865021\n",
      "Epoch 146 training loss: 0.07738455384969711\n",
      "Epoch 147 training loss: 0.07745568454265594\n",
      "Epoch 148 training loss: 0.07746700942516327\n",
      "Epoch 149 training loss: 0.07742408663034439\n",
      "Epoch 150 training loss: 0.07741950452327728\n",
      "Epoch 151 training loss: 0.07746175676584244\n",
      "Epoch 152 training loss: 0.0774260088801384\n",
      "Epoch 153 training loss: 0.0774802416563034\n",
      "Epoch 154 training loss: 0.07741723209619522\n",
      "Epoch 155 training loss: 0.07743414491415024\n",
      "Epoch 156 training loss: 0.077408067882061\n",
      "Epoch 157 training loss: 0.07744409888982773\n",
      "Epoch 158 training loss: 0.07742739468812943\n",
      "Epoch 159 training loss: 0.07744953781366348\n",
      "Epoch 160 training loss: 0.07747656852006912\n",
      "Epoch 161 training loss: 0.0774165466427803\n",
      "Epoch 162 training loss: 0.07741856575012207\n",
      "Epoch 163 training loss: 0.07741240411996841\n",
      "Epoch 164 training loss: 0.07741659134626389\n",
      "Epoch 165 training loss: 0.0774424821138382\n",
      "Epoch 166 training loss: 0.07746914029121399\n",
      "Epoch 167 training loss: 0.07744292914867401\n",
      "Epoch 168 training loss: 0.07748151570558548\n",
      "Epoch 169 training loss: 0.07744846493005753\n",
      "Epoch 170 training loss: 0.07738830894231796\n",
      "Epoch 171 training loss: 0.0774289220571518\n",
      "Epoch 172 training loss: 0.07743427157402039\n",
      "Epoch 173 training loss: 0.07743165642023087\n",
      "Epoch 174 training loss: 0.07747069746255875\n",
      "Epoch 175 training loss: 0.07738874107599258\n",
      "Epoch 176 training loss: 0.07741918414831161\n",
      "Epoch 177 training loss: 0.07743562012910843\n",
      "Epoch 178 training loss: 0.07735966891050339\n",
      "Epoch 179 training loss: 0.07743476331233978\n",
      "Epoch 180 training loss: 0.07739116251468658\n",
      "Epoch 181 training loss: 0.07742628455162048\n",
      "Epoch 182 training loss: 0.07741645723581314\n",
      "Epoch 183 training loss: 0.07747811824083328\n",
      "Epoch 184 training loss: 0.07749971002340317\n",
      "Epoch 185 training loss: 0.07743722945451736\n",
      "Epoch 186 training loss: 0.07746146619319916\n",
      "Epoch 187 training loss: 0.07741804420948029\n",
      "Epoch 188 training loss: 0.07744847238063812\n",
      "Epoch 189 training loss: 0.07746303826570511\n",
      "Epoch 190 training loss: 0.07743881642818451\n",
      "Epoch 191 training loss: 0.07744438946247101\n",
      "Epoch 192 training loss: 0.07748193293809891\n",
      "Epoch 193 training loss: 0.07738836854696274\n",
      "Epoch 194 training loss: 0.07742340117692947\n",
      "Epoch 195 training loss: 0.0774988904595375\n",
      "Epoch 196 training loss: 0.07740726321935654\n",
      "Epoch 197 training loss: 0.07745065540075302\n",
      "Epoch 198 training loss: 0.07747900485992432\n",
      "Epoch 199 training loss: 0.07747728377580643\n",
      "Epoch 200 training loss: 0.07743824273347855\n",
      "Epoch 201 training loss: 0.07746019959449768\n",
      "Epoch 202 training loss: 0.07745882123708725\n",
      "Epoch 203 training loss: 0.07747272402048111\n",
      "Epoch 204 training loss: 0.07742422074079514\n",
      "Epoch 205 training loss: 0.07743415981531143\n",
      "Epoch 206 training loss: 0.07745102047920227\n",
      "Epoch 207 training loss: 0.0774073451757431\n",
      "Epoch 208 training loss: 0.07741142809391022\n",
      "Epoch 209 training loss: 0.07740122824907303\n",
      "Epoch 210 training loss: 0.07748263329267502\n",
      "Epoch 211 training loss: 0.07749155163764954\n",
      "Epoch 212 training loss: 0.07741154730319977\n",
      "Epoch 213 training loss: 0.07739461958408356\n",
      "Epoch 214 training loss: 0.07747567445039749\n",
      "Epoch 215 training loss: 0.07741361856460571\n",
      "Epoch 216 training loss: 0.07742414623498917\n",
      "Epoch 217 training loss: 0.07741661369800568\n",
      "Epoch 218 training loss: 0.0774569883942604\n",
      "Epoch 219 training loss: 0.07744848728179932\n",
      "Epoch 220 training loss: 0.07744541019201279\n",
      "Epoch 221 training loss: 0.07745078206062317\n",
      "Epoch 222 training loss: 0.07744016498327255\n",
      "Epoch 223 training loss: 0.0774693489074707\n",
      "Epoch 224 training loss: 0.07743754237890244\n",
      "Epoch 225 training loss: 0.07743097841739655\n",
      "Epoch 226 training loss: 0.0774221420288086\n",
      "Epoch 227 training loss: 0.07746686041355133\n",
      "Epoch 228 training loss: 0.07739643007516861\n",
      "Epoch 229 training loss: 0.0774589329957962\n",
      "Epoch 230 training loss: 0.07740103453397751\n",
      "Epoch 231 training loss: 0.07740858942270279\n",
      "Epoch 232 training loss: 0.07741384953260422\n",
      "Epoch 233 training loss: 0.07745321840047836\n",
      "Epoch 234 training loss: 0.07743988186120987\n",
      "Epoch 235 training loss: 0.07742397487163544\n",
      "Epoch 236 training loss: 0.07745464146137238\n",
      "Epoch 237 training loss: 0.07745802402496338\n",
      "Epoch 238 training loss: 0.07734030485153198\n",
      "Epoch 239 training loss: 0.07746613770723343\n",
      "Epoch 240 training loss: 0.0773654356598854\n",
      "Epoch 241 training loss: 0.07748863846063614\n",
      "Epoch 242 training loss: 0.07746215164661407\n",
      "Epoch 243 training loss: 0.07744883000850677\n",
      "Epoch 244 training loss: 0.07743442058563232\n",
      "Epoch 245 training loss: 0.07742837816476822\n",
      "Epoch 246 training loss: 0.07740101963281631\n",
      "Epoch 247 training loss: 0.07742860168218613\n",
      "Epoch 248 training loss: 0.07742811739444733\n",
      "Epoch 249 training loss: 0.07739914953708649\n",
      "Epoch 250 training loss: 0.07744497060775757\n",
      "Epoch 251 training loss: 0.07748188078403473\n",
      "Epoch 252 training loss: 0.07741960138082504\n",
      "Epoch 253 training loss: 0.07741804420948029\n",
      "Epoch 254 training loss: 0.07745210081338882\n",
      "Epoch 255 training loss: 0.07737227529287338\n",
      "Epoch 256 training loss: 0.07741500437259674\n",
      "0.749766049036122\n",
      "[[6371 1648]\n",
      " [1026 1641]]\n",
      "Accuracy: 0.749766049036122\n",
      "Precision:  0.4989358467619337\n",
      "Recall:  0.6152980877390326\n",
      "F1:  0.551040967092008\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07820510119199753\n",
      "Epoch 2 training loss: 0.07742731273174286\n",
      "Epoch 3 training loss: 0.07747434079647064\n",
      "Epoch 4 training loss: 0.07741634547710419\n",
      "Epoch 5 training loss: 0.0774001032114029\n",
      "Epoch 6 training loss: 0.07741684466600418\n",
      "Epoch 7 training loss: 0.07742767035961151\n",
      "Epoch 8 training loss: 0.07743506133556366\n",
      "Epoch 9 training loss: 0.07750232517719269\n",
      "Epoch 10 training loss: 0.0774184986948967\n",
      "Epoch 11 training loss: 0.07745100557804108\n",
      "Epoch 12 training loss: 0.0774000734090805\n",
      "Epoch 13 training loss: 0.07746321707963943\n",
      "Epoch 14 training loss: 0.07743411511182785\n",
      "Epoch 15 training loss: 0.0774422287940979\n",
      "Epoch 16 training loss: 0.07743213325738907\n",
      "Epoch 17 training loss: 0.07738926261663437\n",
      "Epoch 18 training loss: 0.07744530588388443\n",
      "Epoch 19 training loss: 0.07743768393993378\n",
      "Epoch 20 training loss: 0.07742731273174286\n",
      "Epoch 21 training loss: 0.0773899033665657\n",
      "Epoch 22 training loss: 0.07744169235229492\n",
      "Epoch 23 training loss: 0.07743233442306519\n",
      "Epoch 24 training loss: 0.07745523750782013\n",
      "Epoch 25 training loss: 0.07745399326086044\n",
      "Epoch 26 training loss: 0.07738808542490005\n",
      "Epoch 27 training loss: 0.0773855522274971\n",
      "Epoch 28 training loss: 0.07735876739025116\n",
      "Epoch 29 training loss: 0.077479787170887\n",
      "Epoch 30 training loss: 0.07748821377754211\n",
      "Epoch 31 training loss: 0.07742900401353836\n",
      "Epoch 32 training loss: 0.07741423696279526\n",
      "Epoch 33 training loss: 0.07743994891643524\n",
      "Epoch 34 training loss: 0.07737038284540176\n",
      "Epoch 35 training loss: 0.07741237431764603\n",
      "Epoch 36 training loss: 0.07751559466123581\n",
      "Epoch 37 training loss: 0.07740432024002075\n",
      "Epoch 38 training loss: 0.07741332054138184\n",
      "Epoch 39 training loss: 0.07745438069105148\n",
      "Epoch 40 training loss: 0.07742320001125336\n",
      "Epoch 41 training loss: 0.07748078554868698\n",
      "Epoch 42 training loss: 0.07743661850690842\n",
      "Epoch 43 training loss: 0.07749215513467789\n",
      "Epoch 44 training loss: 0.0774574875831604\n",
      "Epoch 45 training loss: 0.07744728028774261\n",
      "Epoch 46 training loss: 0.07741936296224594\n",
      "Epoch 47 training loss: 0.07745704799890518\n",
      "Epoch 48 training loss: 0.07742255181074142\n",
      "Epoch 49 training loss: 0.077397920191288\n",
      "Epoch 50 training loss: 0.07735235244035721\n",
      "Epoch 51 training loss: 0.07738921791315079\n",
      "Epoch 52 training loss: 0.07738762348890305\n",
      "Epoch 53 training loss: 0.07744771242141724\n",
      "Epoch 54 training loss: 0.07736868411302567\n",
      "Epoch 55 training loss: 0.07743068784475327\n",
      "Epoch 56 training loss: 0.07742238789796829\n",
      "Epoch 57 training loss: 0.07743077725172043\n",
      "Epoch 58 training loss: 0.07743866741657257\n",
      "Epoch 59 training loss: 0.07746447622776031\n",
      "Epoch 60 training loss: 0.0774567723274231\n",
      "Epoch 61 training loss: 0.07738073915243149\n",
      "Epoch 62 training loss: 0.07738539576530457\n",
      "Epoch 63 training loss: 0.07743057608604431\n",
      "Epoch 64 training loss: 0.0774732157588005\n",
      "Epoch 65 training loss: 0.07742919772863388\n",
      "Epoch 66 training loss: 0.07744184136390686\n",
      "Epoch 67 training loss: 0.07742877304553986\n",
      "Epoch 68 training loss: 0.07750348746776581\n",
      "Epoch 69 training loss: 0.07738280296325684\n",
      "Epoch 70 training loss: 0.07744532823562622\n",
      "Epoch 71 training loss: 0.07740303128957748\n",
      "Epoch 72 training loss: 0.07741285115480423\n",
      "Epoch 73 training loss: 0.07748894393444061\n",
      "Epoch 74 training loss: 0.0774330347776413\n",
      "Epoch 75 training loss: 0.07743067294359207\n",
      "Epoch 76 training loss: 0.07745052129030228\n",
      "Epoch 77 training loss: 0.07744957506656647\n",
      "Epoch 78 training loss: 0.07744444906711578\n",
      "Epoch 79 training loss: 0.0774051770567894\n",
      "Epoch 80 training loss: 0.07744510471820831\n",
      "Epoch 81 training loss: 0.07740612328052521\n",
      "Epoch 82 training loss: 0.07742924243211746\n",
      "Epoch 83 training loss: 0.07757336646318436\n",
      "Epoch 84 training loss: 0.07736792415380478\n",
      "Epoch 85 training loss: 0.07742153853178024\n",
      "Epoch 86 training loss: 0.07742853462696075\n",
      "Epoch 87 training loss: 0.07741580903530121\n",
      "Epoch 88 training loss: 0.07741668075323105\n",
      "Epoch 89 training loss: 0.07750406116247177\n",
      "Epoch 90 training loss: 0.07747916877269745\n",
      "Epoch 91 training loss: 0.07740850746631622\n",
      "Epoch 92 training loss: 0.07739674299955368\n",
      "Epoch 93 training loss: 0.07744048535823822\n",
      "Epoch 94 training loss: 0.07740170508623123\n",
      "Epoch 95 training loss: 0.07742632925510406\n",
      "Epoch 96 training loss: 0.07744962722063065\n",
      "Epoch 97 training loss: 0.07742469757795334\n",
      "Epoch 98 training loss: 0.07745000720024109\n",
      "Epoch 99 training loss: 0.07739900797605515\n",
      "Epoch 100 training loss: 0.07741492241621017\n",
      "Epoch 101 training loss: 0.0774027556180954\n",
      "Epoch 102 training loss: 0.07741694897413254\n",
      "Epoch 103 training loss: 0.07737328112125397\n",
      "Epoch 104 training loss: 0.07747209817171097\n",
      "Epoch 105 training loss: 0.07746019214391708\n",
      "Epoch 106 training loss: 0.07741567492485046\n",
      "Epoch 107 training loss: 0.0774003341794014\n",
      "Epoch 108 training loss: 0.07742796093225479\n",
      "Epoch 109 training loss: 0.07739478349685669\n",
      "Epoch 110 training loss: 0.0774015560746193\n",
      "Epoch 111 training loss: 0.07742643356323242\n",
      "Epoch 112 training loss: 0.07742012292146683\n",
      "Epoch 113 training loss: 0.07739701867103577\n",
      "Epoch 114 training loss: 0.07742179185152054\n",
      "Epoch 115 training loss: 0.07743221521377563\n",
      "Epoch 116 training loss: 0.07748748362064362\n",
      "Epoch 117 training loss: 0.07745298743247986\n",
      "Epoch 118 training loss: 0.07744850218296051\n",
      "Epoch 119 training loss: 0.07749152183532715\n",
      "Epoch 120 training loss: 0.07742016017436981\n",
      "Epoch 121 training loss: 0.07739897072315216\n",
      "Epoch 122 training loss: 0.07745049148797989\n",
      "Epoch 123 training loss: 0.07745064795017242\n",
      "Epoch 124 training loss: 0.07739677280187607\n",
      "Epoch 125 training loss: 0.077447310090065\n",
      "Epoch 126 training loss: 0.07743377238512039\n",
      "Epoch 127 training loss: 0.07736174017190933\n",
      "Epoch 128 training loss: 0.07745746523141861\n",
      "Epoch 129 training loss: 0.07744526118040085\n",
      "Epoch 130 training loss: 0.07740183919668198\n",
      "Epoch 131 training loss: 0.07741434872150421\n",
      "Epoch 132 training loss: 0.0774325281381607\n",
      "Epoch 133 training loss: 0.07740558683872223\n",
      "Epoch 134 training loss: 0.07746411859989166\n",
      "Epoch 135 training loss: 0.07739145308732986\n",
      "Epoch 136 training loss: 0.07738038152456284\n",
      "Epoch 137 training loss: 0.07738006860017776\n",
      "Epoch 138 training loss: 0.07742536813020706\n",
      "Epoch 139 training loss: 0.07739362865686417\n",
      "Epoch 140 training loss: 0.07741432636976242\n",
      "Epoch 141 training loss: 0.07743701338768005\n",
      "Epoch 142 training loss: 0.07740746438503265\n",
      "Epoch 143 training loss: 0.07744122296571732\n",
      "Epoch 144 training loss: 0.07740173488855362\n",
      "Epoch 145 training loss: 0.07742122560739517\n",
      "Epoch 146 training loss: 0.07743996381759644\n",
      "Epoch 147 training loss: 0.07749509811401367\n",
      "Epoch 148 training loss: 0.07745082676410675\n",
      "Epoch 149 training loss: 0.07737855613231659\n",
      "Epoch 150 training loss: 0.07746215164661407\n",
      "Epoch 151 training loss: 0.0774436891078949\n",
      "Epoch 152 training loss: 0.07745885848999023\n",
      "Epoch 153 training loss: 0.07743802666664124\n",
      "Epoch 154 training loss: 0.07745373249053955\n",
      "Epoch 155 training loss: 0.07747890055179596\n",
      "Epoch 156 training loss: 0.07744718343019485\n",
      "Epoch 157 training loss: 0.07741285115480423\n",
      "Epoch 158 training loss: 0.0774376392364502\n",
      "Epoch 159 training loss: 0.07737496495246887\n",
      "Epoch 160 training loss: 0.07740356028079987\n",
      "Epoch 161 training loss: 0.07748392224311829\n",
      "Epoch 162 training loss: 0.0774281769990921\n",
      "Epoch 163 training loss: 0.07742192596197128\n",
      "Epoch 164 training loss: 0.07741227000951767\n",
      "Epoch 165 training loss: 0.07740402966737747\n",
      "Epoch 166 training loss: 0.07747163623571396\n",
      "Epoch 167 training loss: 0.07737860083580017\n",
      "Epoch 168 training loss: 0.07744190096855164\n",
      "Epoch 169 training loss: 0.07742905616760254\n",
      "Epoch 170 training loss: 0.0774131491780281\n",
      "Epoch 171 training loss: 0.07739119231700897\n",
      "Epoch 172 training loss: 0.07745599001646042\n",
      "Epoch 173 training loss: 0.07742186635732651\n",
      "Epoch 174 training loss: 0.07746647298336029\n",
      "Epoch 175 training loss: 0.07737014442682266\n",
      "Epoch 176 training loss: 0.07744915783405304\n",
      "Epoch 177 training loss: 0.07743239402770996\n",
      "Epoch 178 training loss: 0.07748205959796906\n",
      "Epoch 179 training loss: 0.07746542990207672\n",
      "Epoch 180 training loss: 0.0774821788072586\n",
      "Epoch 181 training loss: 0.07745592296123505\n",
      "Epoch 182 training loss: 0.07748066633939743\n",
      "Epoch 183 training loss: 0.07745208591222763\n",
      "Epoch 184 training loss: 0.07744574546813965\n",
      "Epoch 185 training loss: 0.07736950367689133\n",
      "Epoch 186 training loss: 0.0773940160870552\n",
      "Epoch 187 training loss: 0.07738503068685532\n",
      "Epoch 188 training loss: 0.0774136632680893\n",
      "Epoch 189 training loss: 0.0774226039648056\n",
      "Epoch 190 training loss: 0.07741347700357437\n",
      "Epoch 191 training loss: 0.07740890979766846\n",
      "Epoch 192 training loss: 0.0774514451622963\n",
      "Epoch 193 training loss: 0.07738316059112549\n",
      "Epoch 194 training loss: 0.07743290811777115\n",
      "Epoch 195 training loss: 0.07745641469955444\n",
      "Epoch 196 training loss: 0.07746319472789764\n",
      "Epoch 197 training loss: 0.07745571434497833\n",
      "Epoch 198 training loss: 0.07743602246046066\n",
      "Epoch 199 training loss: 0.07743863761425018\n",
      "Epoch 200 training loss: 0.07747361063957214\n",
      "Epoch 201 training loss: 0.07745525240898132\n",
      "Epoch 202 training loss: 0.0774555504322052\n",
      "Epoch 203 training loss: 0.07744631916284561\n",
      "Epoch 204 training loss: 0.07739078998565674\n",
      "Epoch 205 training loss: 0.077388696372509\n",
      "Epoch 206 training loss: 0.07741390913724899\n",
      "Epoch 207 training loss: 0.07736082375049591\n",
      "Epoch 208 training loss: 0.07742109149694443\n",
      "Epoch 209 training loss: 0.07743155211210251\n",
      "Epoch 210 training loss: 0.0774386078119278\n",
      "Epoch 211 training loss: 0.07742296159267426\n",
      "Epoch 212 training loss: 0.07745782285928726\n",
      "Epoch 213 training loss: 0.07743490487337112\n",
      "Epoch 214 training loss: 0.07743872702121735\n",
      "Epoch 215 training loss: 0.07742881774902344\n",
      "Epoch 216 training loss: 0.07736515253782272\n",
      "Epoch 217 training loss: 0.0773802325129509\n",
      "Epoch 218 training loss: 0.07746198773384094\n",
      "Epoch 219 training loss: 0.07748524844646454\n",
      "Epoch 220 training loss: 0.07749640196561813\n",
      "Epoch 221 training loss: 0.07744390517473221\n",
      "Epoch 222 training loss: 0.07744646072387695\n",
      "Epoch 223 training loss: 0.07745266705751419\n",
      "Epoch 224 training loss: 0.0774136632680893\n",
      "Epoch 225 training loss: 0.07741881161928177\n",
      "Epoch 226 training loss: 0.07740063965320587\n",
      "Epoch 227 training loss: 0.07743268460035324\n",
      "Epoch 228 training loss: 0.07746909558773041\n",
      "Epoch 229 training loss: 0.07741882652044296\n",
      "Epoch 230 training loss: 0.07745423913002014\n",
      "Epoch 231 training loss: 0.07748814672231674\n",
      "Epoch 232 training loss: 0.0774727612733841\n",
      "Epoch 233 training loss: 0.07740207761526108\n",
      "Epoch 234 training loss: 0.07741750776767731\n",
      "Epoch 235 training loss: 0.07740811258554459\n",
      "Epoch 236 training loss: 0.07747641950845718\n",
      "Epoch 237 training loss: 0.07739314436912537\n",
      "Epoch 238 training loss: 0.07741294801235199\n",
      "Epoch 239 training loss: 0.07740698009729385\n",
      "Epoch 240 training loss: 0.07741361111402512\n",
      "Epoch 241 training loss: 0.07743487507104874\n",
      "Epoch 242 training loss: 0.07740864902734756\n",
      "Epoch 243 training loss: 0.07737050205469131\n",
      "Epoch 244 training loss: 0.07739847153425217\n",
      "Epoch 245 training loss: 0.07743341475725174\n",
      "Epoch 246 training loss: 0.07743142545223236\n",
      "Epoch 247 training loss: 0.07743512094020844\n",
      "Epoch 248 training loss: 0.07745401561260223\n",
      "Epoch 249 training loss: 0.07744383811950684\n",
      "Epoch 250 training loss: 0.07737212628126144\n",
      "Epoch 251 training loss: 0.07740776985883713\n",
      "Epoch 252 training loss: 0.07743430137634277\n",
      "Epoch 253 training loss: 0.07736478000879288\n",
      "Epoch 254 training loss: 0.07745715975761414\n",
      "Epoch 255 training loss: 0.07744615525007248\n",
      "Epoch 256 training loss: 0.07740207016468048\n",
      "0.6766060606060607\n",
      "[[1142  316]\n",
      " [1018 1649]]\n",
      "Accuracy: 0.6766060606060607\n",
      "Precision:  0.8391857506361323\n",
      "Recall:  0.6182977127859017\n",
      "F1:  0.7120034542314335\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06625489145517349\n",
      "Epoch 2 training loss: 0.04909355193376541\n",
      "Epoch 3 training loss: 0.04245711490511894\n",
      "Epoch 4 training loss: 0.03914931043982506\n",
      "Epoch 5 training loss: 0.03747978061437607\n",
      "Epoch 6 training loss: 0.03643306717276573\n",
      "Epoch 7 training loss: 0.03493637219071388\n",
      "Epoch 8 training loss: 0.034098055213689804\n",
      "Epoch 9 training loss: 0.033504799008369446\n",
      "Epoch 10 training loss: 0.03302076831459999\n",
      "Epoch 11 training loss: 0.03230932727456093\n",
      "Epoch 12 training loss: 0.03206918016076088\n",
      "Epoch 13 training loss: 0.03169161081314087\n",
      "Epoch 14 training loss: 0.03128679096698761\n",
      "Epoch 15 training loss: 0.030713040381669998\n",
      "Epoch 16 training loss: 0.03057003580033779\n",
      "Epoch 17 training loss: 0.03038855269551277\n",
      "Epoch 18 training loss: 0.030225500464439392\n",
      "Epoch 19 training loss: 0.029932362958788872\n",
      "Epoch 20 training loss: 0.02969769947230816\n",
      "Epoch 21 training loss: 0.029447149485349655\n",
      "Epoch 22 training loss: 0.02919827401638031\n",
      "Epoch 23 training loss: 0.029099345207214355\n",
      "Epoch 24 training loss: 0.028855647891759872\n",
      "Epoch 25 training loss: 0.028711432591080666\n",
      "Epoch 26 training loss: 0.028599955141544342\n",
      "Epoch 27 training loss: 0.028430815786123276\n",
      "Epoch 28 training loss: 0.02842867746949196\n",
      "Epoch 29 training loss: 0.028187613934278488\n",
      "Epoch 30 training loss: 0.028057018294930458\n",
      "Epoch 31 training loss: 0.02790032885968685\n",
      "Epoch 32 training loss: 0.027849936857819557\n",
      "Epoch 33 training loss: 0.027625616639852524\n",
      "Epoch 34 training loss: 0.02761864848434925\n",
      "Epoch 35 training loss: 0.027532346546649933\n",
      "Epoch 36 training loss: 0.027616068720817566\n",
      "Epoch 37 training loss: 0.027193687856197357\n",
      "Epoch 38 training loss: 0.027319978922605515\n",
      "Epoch 39 training loss: 0.027112426236271858\n",
      "Epoch 40 training loss: 0.02696315571665764\n",
      "Epoch 41 training loss: 0.027042318135499954\n",
      "Epoch 42 training loss: 0.026838526129722595\n",
      "Epoch 43 training loss: 0.026955246925354004\n",
      "Epoch 44 training loss: 0.026909908279776573\n",
      "Epoch 45 training loss: 0.026841139420866966\n",
      "Epoch 46 training loss: 0.026516426354646683\n",
      "Epoch 47 training loss: 0.026642879471182823\n",
      "Epoch 48 training loss: 0.026324590668082237\n",
      "Epoch 49 training loss: 0.026263976469635963\n",
      "Epoch 50 training loss: 0.026249445974826813\n",
      "Epoch 51 training loss: 0.02604641765356064\n",
      "Epoch 52 training loss: 0.02612430416047573\n",
      "Epoch 53 training loss: 0.025866106152534485\n",
      "Epoch 54 training loss: 0.025845985859632492\n",
      "Epoch 55 training loss: 0.025682326406240463\n",
      "Epoch 56 training loss: 0.02536025457084179\n",
      "Epoch 57 training loss: 0.025338981300592422\n",
      "Epoch 58 training loss: 0.02513940818607807\n",
      "Epoch 59 training loss: 0.024935221299529076\n",
      "Epoch 60 training loss: 0.024671314284205437\n",
      "Epoch 61 training loss: 0.024506857618689537\n",
      "Epoch 62 training loss: 0.0242477897554636\n",
      "Epoch 63 training loss: 0.024088699370622635\n",
      "Epoch 64 training loss: 0.023625945672392845\n",
      "Epoch 65 training loss: 0.023461265489459038\n",
      "Epoch 66 training loss: 0.02332330495119095\n",
      "Epoch 67 training loss: 0.02293197438120842\n",
      "Epoch 68 training loss: 0.022817224264144897\n",
      "Epoch 69 training loss: 0.022207291796803474\n",
      "Epoch 70 training loss: 0.02219865284860134\n",
      "Epoch 71 training loss: 0.021890781819820404\n",
      "Epoch 72 training loss: 0.021742701530456543\n",
      "Epoch 73 training loss: 0.021562276408076286\n",
      "Epoch 74 training loss: 0.021385610103607178\n",
      "Epoch 75 training loss: 0.02136659063398838\n",
      "Epoch 76 training loss: 0.021052399650216103\n",
      "Epoch 77 training loss: 0.020880645141005516\n",
      "Epoch 78 training loss: 0.020940080285072327\n",
      "Epoch 79 training loss: 0.02073853835463524\n",
      "Epoch 80 training loss: 0.020687444135546684\n",
      "Epoch 81 training loss: 0.020741578191518784\n",
      "Epoch 82 training loss: 0.020395539700984955\n",
      "Epoch 83 training loss: 0.020271755754947662\n",
      "Epoch 84 training loss: 0.02035578154027462\n",
      "Epoch 85 training loss: 0.02011803537607193\n",
      "Epoch 86 training loss: 0.020023616030812263\n",
      "Epoch 87 training loss: 0.01991136185824871\n",
      "Epoch 88 training loss: 0.019974680617451668\n",
      "Epoch 89 training loss: 0.019902657717466354\n",
      "Epoch 90 training loss: 0.01937885396182537\n",
      "Epoch 91 training loss: 0.01948336698114872\n",
      "Epoch 92 training loss: 0.01942162774503231\n",
      "Epoch 93 training loss: 0.019272571429610252\n",
      "Epoch 94 training loss: 0.019369909539818764\n",
      "Epoch 95 training loss: 0.019283372908830643\n",
      "Epoch 96 training loss: 0.019233429804444313\n",
      "Epoch 97 training loss: 0.01906970515847206\n",
      "Epoch 98 training loss: 0.018963610753417015\n",
      "Epoch 99 training loss: 0.01888202130794525\n",
      "Epoch 100 training loss: 0.01865343563258648\n",
      "Epoch 101 training loss: 0.01872287131845951\n",
      "Epoch 102 training loss: 0.01871713623404503\n",
      "Epoch 103 training loss: 0.01859404891729355\n",
      "Epoch 104 training loss: 0.018709920346736908\n",
      "Epoch 105 training loss: 0.018589815124869347\n",
      "Epoch 106 training loss: 0.018433770164847374\n",
      "Epoch 107 training loss: 0.018555840477347374\n",
      "Epoch 108 training loss: 0.018234755843877792\n",
      "Epoch 109 training loss: 0.01844007708132267\n",
      "Epoch 110 training loss: 0.018145499750971794\n",
      "Epoch 111 training loss: 0.018095415085554123\n",
      "Epoch 112 training loss: 0.018095115199685097\n",
      "Epoch 113 training loss: 0.0182814784348011\n",
      "Epoch 114 training loss: 0.01810504123568535\n",
      "Epoch 115 training loss: 0.01819799281656742\n",
      "Epoch 116 training loss: 0.018134281039237976\n",
      "Epoch 117 training loss: 0.018011948093771935\n",
      "Epoch 118 training loss: 0.01785377971827984\n",
      "Epoch 119 training loss: 0.01783565990626812\n",
      "Epoch 120 training loss: 0.01792776584625244\n",
      "Epoch 121 training loss: 0.017782969400286674\n",
      "Epoch 122 training loss: 0.017611371353268623\n",
      "Epoch 123 training loss: 0.01768939569592476\n",
      "Epoch 124 training loss: 0.01762267015874386\n",
      "Epoch 125 training loss: 0.01776331663131714\n",
      "Epoch 126 training loss: 0.01750047132372856\n",
      "Epoch 127 training loss: 0.017318235710263252\n",
      "Epoch 128 training loss: 0.017360808327794075\n",
      "Epoch 129 training loss: 0.017621522769331932\n",
      "Epoch 130 training loss: 0.017387622967362404\n",
      "Epoch 131 training loss: 0.017393510788679123\n",
      "Epoch 132 training loss: 0.01744542270898819\n",
      "Epoch 133 training loss: 0.017025580629706383\n",
      "Epoch 134 training loss: 0.017030740156769753\n",
      "Epoch 135 training loss: 0.017273958772420883\n",
      "Epoch 136 training loss: 0.017209289595484734\n",
      "Epoch 137 training loss: 0.01720605045557022\n",
      "Epoch 138 training loss: 0.017168710008263588\n",
      "Epoch 139 training loss: 0.016858384013175964\n",
      "Epoch 140 training loss: 0.016813179478049278\n",
      "Epoch 141 training loss: 0.016739917919039726\n",
      "Epoch 142 training loss: 0.016970675438642502\n",
      "Epoch 143 training loss: 0.016692938283085823\n",
      "Epoch 144 training loss: 0.016780072823166847\n",
      "Epoch 145 training loss: 0.016801072284579277\n",
      "Epoch 146 training loss: 0.016828345134854317\n",
      "Epoch 147 training loss: 0.016928980126976967\n",
      "Epoch 148 training loss: 0.016836272552609444\n",
      "Epoch 149 training loss: 0.01668604649603367\n",
      "Epoch 150 training loss: 0.01664336770772934\n",
      "Epoch 151 training loss: 0.016539549455046654\n",
      "Epoch 152 training loss: 0.016560394316911697\n",
      "Epoch 153 training loss: 0.01650715060532093\n",
      "Epoch 154 training loss: 0.016589298844337463\n",
      "Epoch 155 training loss: 0.016465945169329643\n",
      "Epoch 156 training loss: 0.016458921134471893\n",
      "Epoch 157 training loss: 0.016455966979265213\n",
      "Epoch 158 training loss: 0.016433915123343468\n",
      "Epoch 159 training loss: 0.016623886302113533\n",
      "Epoch 160 training loss: 0.016405347734689713\n",
      "Epoch 161 training loss: 0.016416840255260468\n",
      "Epoch 162 training loss: 0.01631401665508747\n",
      "Epoch 163 training loss: 0.01626889407634735\n",
      "Epoch 164 training loss: 0.016602518036961555\n",
      "Epoch 165 training loss: 0.016229309141635895\n",
      "Epoch 166 training loss: 0.016477083787322044\n",
      "Epoch 167 training loss: 0.01629726029932499\n",
      "Epoch 168 training loss: 0.01626943051815033\n",
      "Epoch 169 training loss: 0.015948127955198288\n",
      "Epoch 170 training loss: 0.016117528080940247\n",
      "Epoch 171 training loss: 0.016109542921185493\n",
      "Epoch 172 training loss: 0.016357509419322014\n",
      "Epoch 173 training loss: 0.01616618037223816\n",
      "Epoch 174 training loss: 0.016037996858358383\n",
      "Epoch 175 training loss: 0.016065096482634544\n",
      "Epoch 176 training loss: 0.015989316627383232\n",
      "Epoch 177 training loss: 0.016175832599401474\n",
      "Epoch 178 training loss: 0.01587742380797863\n",
      "Epoch 179 training loss: 0.016178781166672707\n",
      "Epoch 180 training loss: 0.015889538452029228\n",
      "Epoch 181 training loss: 0.0160312931984663\n",
      "Epoch 182 training loss: 0.01588594727218151\n",
      "Epoch 183 training loss: 0.015881195664405823\n",
      "Epoch 184 training loss: 0.015840306878089905\n",
      "Epoch 185 training loss: 0.01605389639735222\n",
      "Epoch 186 training loss: 0.015793267637491226\n",
      "Epoch 187 training loss: 0.016026828438043594\n",
      "Epoch 188 training loss: 0.015839749947190285\n",
      "Epoch 189 training loss: 0.015759067609906197\n",
      "Epoch 190 training loss: 0.015535690821707249\n",
      "Epoch 191 training loss: 0.015777762979269028\n",
      "Epoch 192 training loss: 0.01567714847624302\n",
      "Epoch 193 training loss: 0.015781357884407043\n",
      "Epoch 194 training loss: 0.01580001600086689\n",
      "Epoch 195 training loss: 0.015517575666308403\n",
      "Epoch 196 training loss: 0.015692852437496185\n",
      "Epoch 197 training loss: 0.015932723879814148\n",
      "Epoch 198 training loss: 0.015628911554813385\n",
      "Epoch 199 training loss: 0.015572679229080677\n",
      "Epoch 200 training loss: 0.015677152201533318\n",
      "Epoch 201 training loss: 0.015637964010238647\n",
      "Epoch 202 training loss: 0.015426445752382278\n",
      "Epoch 203 training loss: 0.015684574842453003\n",
      "Epoch 204 training loss: 0.015476063825190067\n",
      "Epoch 205 training loss: 0.01546694990247488\n",
      "Epoch 206 training loss: 0.015629148110747337\n",
      "Epoch 207 training loss: 0.015568431466817856\n",
      "Epoch 208 training loss: 0.015556557103991508\n",
      "Epoch 209 training loss: 0.015593952499330044\n",
      "Epoch 210 training loss: 0.015547442249953747\n",
      "Epoch 211 training loss: 0.015491267666220665\n",
      "Epoch 212 training loss: 0.01567382551729679\n",
      "Epoch 213 training loss: 0.015482068993151188\n",
      "Epoch 214 training loss: 0.015459266491234303\n",
      "Epoch 215 training loss: 0.015430772677063942\n",
      "Epoch 216 training loss: 0.015466989018023014\n",
      "Epoch 217 training loss: 0.015519218519330025\n",
      "Epoch 218 training loss: 0.015249613672494888\n",
      "Epoch 219 training loss: 0.015336908400058746\n",
      "Epoch 220 training loss: 0.015540406107902527\n",
      "Epoch 221 training loss: 0.01529825758188963\n",
      "Epoch 222 training loss: 0.015517277643084526\n",
      "Epoch 223 training loss: 0.015178268775343895\n",
      "Epoch 224 training loss: 0.015360182151198387\n",
      "Epoch 225 training loss: 0.015348656103014946\n",
      "Epoch 226 training loss: 0.01520773209631443\n",
      "Epoch 227 training loss: 0.015244987793266773\n",
      "Epoch 228 training loss: 0.0152487987652421\n",
      "Epoch 229 training loss: 0.015238987281918526\n",
      "Epoch 230 training loss: 0.015494346618652344\n",
      "Epoch 231 training loss: 0.015078961849212646\n",
      "Epoch 232 training loss: 0.015356658026576042\n",
      "Epoch 233 training loss: 0.015318785794079304\n",
      "Epoch 234 training loss: 0.01512398011982441\n",
      "Epoch 235 training loss: 0.015098288655281067\n",
      "Epoch 236 training loss: 0.015524735674262047\n",
      "Epoch 237 training loss: 0.015281627885997295\n",
      "Epoch 238 training loss: 0.014933056198060513\n",
      "Epoch 239 training loss: 0.01538681797683239\n",
      "Epoch 240 training loss: 0.015145910903811455\n",
      "Epoch 241 training loss: 0.015095912851393223\n",
      "Epoch 242 training loss: 0.01506839506328106\n",
      "Epoch 243 training loss: 0.015053287148475647\n",
      "Epoch 244 training loss: 0.015319444239139557\n",
      "Epoch 245 training loss: 0.014824960380792618\n",
      "Epoch 246 training loss: 0.014945730566978455\n",
      "Epoch 247 training loss: 0.015214812941849232\n",
      "Epoch 248 training loss: 0.014963557943701744\n",
      "Epoch 249 training loss: 0.01494537852704525\n",
      "Epoch 250 training loss: 0.014922098256647587\n",
      "Epoch 251 training loss: 0.015095672570168972\n",
      "Epoch 252 training loss: 0.015136083588004112\n",
      "Epoch 253 training loss: 0.01503210049122572\n",
      "Epoch 254 training loss: 0.014876462519168854\n",
      "Epoch 255 training loss: 0.014883612282574177\n",
      "Epoch 256 training loss: 0.014965767040848732\n",
      "0.4824242424242424\n",
      "[[ 957  501]\n",
      " [1634 1033]]\n",
      "Accuracy: 0.4824242424242424\n",
      "Precision:  0.6734028683181226\n",
      "Recall:  0.38732658417697785\n",
      "F1:  0.4917876696024756\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
