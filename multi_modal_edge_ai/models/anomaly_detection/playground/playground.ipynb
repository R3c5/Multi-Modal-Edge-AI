{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-15T11:22:32.385636900Z",
     "start_time": "2023-06-15T11:22:28.946656800Z"
    }
   },
   "outputs": [],
   "source": [
    "from multi_modal_edge_ai.models.anomaly_detection.data_access.parser import parse_file_with_idle\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.local_outlier_factor import LOF\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.isolation_forest import IForest\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.one_class_svm import OCSVM\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.autoencoder import Autoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.ml_models.lstm_autoencoder import LSTMAutoencoder\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.model_validator import model_train_eval\n",
    "from multi_modal_edge_ai.models.anomaly_detection.train_and_eval.hyperparameter_config import HyperparameterConfig as Hparams\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "data = parse_file_with_idle(\"../../public_datasets/Aruba_Idle_Squashed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5905411163187048\n",
      "[[1376   82]\n",
      " [ 879   10]]\n",
      "Accuracy:  0.5905411163187048\n",
      "Precision:  0.943758573388203\n",
      "Recall:  0.61019955654102\n",
      "F1:  0.7411796391058443\n"
     ]
    }
   ],
   "source": [
    " # Basic LOF model with changed hyperparameters\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 300,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 40,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.1,\n",
    "             window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "0.5069867431028305\n",
      "[[ 111 1347]\n",
      " [  29 1304]]\n",
      "Accuracy:  0.5069867431028305\n",
      "Precision:  0.07613168724279835\n",
      "Recall:  0.7928571428571428\n",
      "F1:  0.13892365456821026\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 16\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 4000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 400,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": 0.001,\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.5206019347903977\n",
      "[[ 166 1292]\n",
      " [  46 1287]]\n",
      "Accuracy:  0.5206019347903977\n",
      "Precision:  0.11385459533607682\n",
      "Recall:  0.7830188679245284\n",
      "F1:  0.19880239520958085\n"
     ]
    }
   ],
   "source": [
    "# Basic LOF model with changed hyperparameters and Novelty set to True and batch size of 64\n",
    "# Novelty set to True means that the model will use LocalOutlierFactor for novelty detection and data is outlier-free during training\n",
    "\n",
    "lof = LOF()\n",
    "lofParams = {\n",
    "    \"n_neighbors\": 2000,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 300,\n",
    "    \"metric\": \"minkowski\",\n",
    "    \"p\": 2,\n",
    "    \"contamination\": \"auto\",\n",
    "    \"novelty\": True\n",
    "}\n",
    "\n",
    "hp = Hparams(lof_hparams=lofParams, clean_test_data_ratio = 0.15, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(lof, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iforest = IForest()\n",
    "\n",
    "iforestParams = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_samples\": \"auto\",\n",
    "    \"contamination\": 0.1,\n",
    "    \"max_features\": 1.0,\n",
    "    \"bootstrap\": False,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "hp = Hparams(i_forest_hparams=iforestParams, window_size= 8, window_slide= 1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(iforest, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[1 0 1 ... 1 1 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "0.7200546946216956\n",
      "[[6917 1831]\n",
      " [1240  982]]\n",
      "Accuracy: 0.7200546946216956\n",
      "Precision:  0.790695016003658\n",
      "Recall:  0.8479833272036288\n",
      "F1:  0.818337769890565\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.01,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2400,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.25,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05389717221260071\n",
      "Epoch 2 training loss: 0.030980540439486504\n",
      "Epoch 3 training loss: 0.023487167432904243\n",
      "Epoch 4 training loss: 0.020996732637286186\n",
      "Epoch 5 training loss: 0.019222674891352654\n",
      "Epoch 6 training loss: 0.017535774037241936\n",
      "Epoch 7 training loss: 0.016274292021989822\n",
      "Epoch 8 training loss: 0.01566317304968834\n",
      "Epoch 9 training loss: 0.015262589789927006\n",
      "Epoch 10 training loss: 0.014593065716326237\n",
      "0.2847235417062512\n",
      "[[1318 7430]\n",
      " [  99 1679]]\n",
      "Accuracy: 0.2847235417062512\n",
      "Precision:  0.1843231968382918\n",
      "Recall:  0.9443194600674916\n",
      "F1:  0.30844126021860935\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([96,64,32,16], [16,32,64,96],nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=16,  n_epochs=10, anomaly_generation_ratio = 11, clean_test_data_ratio=0.2, window_size= 8, window_slide= 1)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "[0 0 0 ... 1 0 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0.4505562422744129\n",
      "[[1458    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.4505562422744129\n",
      "Precision:  1.0\n",
      "Recall:  0.4505562422744129\n",
      "F1:  0.6212185769066894\n"
     ]
    }
   ],
   "source": [
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=32, n_epochs=16, clean_test_data_ratio=0.2,window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07760713093037244\n",
      "Epoch 2 training loss: 0.07725755977669586\n",
      "Epoch 3 training loss: 0.0772386808702628\n",
      "Epoch 4 training loss: 0.07725333552849474\n",
      "Epoch 5 training loss: 0.07720437902104449\n",
      "Epoch 6 training loss: 0.07721456329896627\n",
      "Epoch 7 training loss: 0.0772136117869106\n",
      "Epoch 8 training loss: 0.07721375107463486\n",
      "Epoch 9 training loss: 0.07718426470631153\n",
      "Epoch 10 training loss: 0.07720113484895001\n",
      "Epoch 11 training loss: 0.07719907346490616\n",
      "Epoch 12 training loss: 0.07718318898107174\n",
      "Epoch 13 training loss: 0.07717363352660002\n",
      "Epoch 14 training loss: 0.07718654360969973\n",
      "Epoch 15 training loss: 0.07717847603837319\n",
      "Epoch 16 training loss: 0.07719744948766648\n",
      "Epoch 17 training loss: 0.07721296846458041\n",
      "Epoch 18 training loss: 0.07720602415680164\n",
      "Epoch 19 training loss: 0.07716414992486968\n",
      "Epoch 20 training loss: 0.07716112782344792\n",
      "Epoch 21 training loss: 0.07718445156324819\n",
      "Epoch 22 training loss: 0.07716832268737986\n",
      "Epoch 23 training loss: 0.07718266583130569\n",
      "Epoch 24 training loss: 0.0771673936056365\n",
      "Epoch 25 training loss: 0.07717815300615251\n",
      "Epoch 26 training loss: 0.07719496329638717\n",
      "Epoch 27 training loss: 0.0771747040143289\n",
      "Epoch 28 training loss: 0.07716852902509147\n",
      "Epoch 29 training loss: 0.07714804287327305\n",
      "Epoch 30 training loss: 0.07716223987655377\n",
      "Epoch 31 training loss: 0.07717982528331041\n",
      "Epoch 32 training loss: 0.07717942436989418\n",
      "0.6146477132262052\n",
      "[[ 340 1118]\n",
      " [ 129 1649]]\n",
      "Accuracy: 0.6146477132262052\n",
      "Precision:  0.5959522949042284\n",
      "Recall:  0.9274465691788526\n",
      "F1:  0.7256325632563256\n"
     ]
    }
   ],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=16, n_epochs=32, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.7262534184138559\n",
      "[[6972 1776]\n",
      " [1227  995]]\n",
      "Accuracy:  0.7262534184138559\n",
      "Precision:  0.3590761457957416\n",
      "Recall:  0.44779477947794777\n",
      "F1:  0.3985579811736431\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.0  tol:  1e-05\n",
      "0.697538742023701\n",
      "[[6620 2128]\n",
      " [1190 1032]]\n",
      "Accuracy:  0.697538742023701\n",
      "Precision:  0.3265822784810127\n",
      "Recall:  0.46444644464446444\n",
      "F1:  0.3835005574136009\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.7302643573381951\n",
      "[[6966 1782]\n",
      " [1177 1045]]\n",
      "Accuracy:  0.7302643573381951\n",
      "Precision:  0.36964980544747084\n",
      "Recall:  0.47029702970297027\n",
      "F1:  0.4139433551198257\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.1  tol:  1e-05\n",
      "0.703281677301732\n",
      "[[6672 2076]\n",
      " [1179 1043]]\n",
      "Accuracy:  0.703281677301732\n",
      "Precision:  0.3344020519397243\n",
      "Recall:  0.4693969396939694\n",
      "F1:  0.3905635648754915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.7003646308113035\n",
      "[[6583 2165]\n",
      " [1122 1100]]\n",
      "Accuracy:  0.7003646308113035\n",
      "Precision:  0.33690658499234305\n",
      "Recall:  0.49504950495049505\n",
      "F1:  0.40094769455075635\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.2  tol:  1e-05\n",
      "0.7062898814949863\n",
      "[[6674 2074]\n",
      " [1148 1074]]\n",
      "Accuracy:  0.7062898814949863\n",
      "Precision:  0.3411689961880559\n",
      "Recall:  0.48334833483348333\n",
      "F1:  0.4\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.692616226071103\n",
      "[[6469 2279]\n",
      " [1093 1129]]\n",
      "Accuracy:  0.692616226071103\n",
      "Precision:  0.3312793427230047\n",
      "Recall:  0.508100810081008\n",
      "F1:  0.4010657193605684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  scale  coef0:  0.3  tol:  1e-05\n",
      "0.7061987237921604\n",
      "[[6658 2090]\n",
      " [1133 1089]]\n",
      "Accuracy:  0.7061987237921604\n",
      "Precision:  0.34256055363321797\n",
      "Recall:  0.4900990099009901\n",
      "F1:  0.40325865580448067\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.46298997265268915\n",
      "[[3161 5587]\n",
      " [ 304 1918]]\n",
      "Accuracy:  0.46298997265268915\n",
      "Precision:  0.25556295802798135\n",
      "Recall:  0.8631863186318632\n",
      "F1:  0.39436619718309857\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.0  tol:  1e-05\n",
      "0.47265268915223335\n",
      "[[3271 5477]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47265268915223335\n",
      "Precision:  0.2589636043837099\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39821075626755437\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.4610756608933455\n",
      "[[3173 5575]\n",
      " [ 337 1885]]\n",
      "Accuracy:  0.4610756608933455\n",
      "Precision:  0.2526809651474531\n",
      "Recall:  0.8483348334833484\n",
      "F1:  0.38938235901673207\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.1  tol:  1e-05\n",
      "0.4740200546946217\n",
      "[[3323 5425]\n",
      " [ 345 1877]]\n",
      "Accuracy:  0.4740200546946217\n",
      "Precision:  0.25705286222952617\n",
      "Recall:  0.8447344734473448\n",
      "F1:  0.3941621167576648\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.46362807657247035\n",
      "[[3200 5548]\n",
      " [ 336 1886]]\n",
      "Accuracy:  0.46362807657247035\n",
      "Precision:  0.2536992198009147\n",
      "Recall:  0.8487848784878488\n",
      "F1:  0.39063794531897267\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.2  tol:  1e-05\n",
      "0.4690975387420237\n",
      "[[3250 5498]\n",
      " [ 326 1896]]\n",
      "Accuracy:  0.4690975387420237\n",
      "Precision:  0.2564241276710847\n",
      "Recall:  0.8532853285328533\n",
      "F1:  0.39434276206322794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.47201458523245216\n",
      "[[3264 5484]\n",
      " [ 308 1914]]\n",
      "Accuracy:  0.47201458523245216\n",
      "Precision:  0.2587185725871857\n",
      "Recall:  0.8613861386138614\n",
      "F1:  0.39792099792099794\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for kernel:  rbf  degree:  3  gamma:  auto  coef0:  0.3  tol:  1e-05\n",
      "0.4545123062898815\n",
      "[[3079 5669]\n",
      " [ 315 1907]]\n",
      "Accuracy:  0.4545123062898815\n",
      "Precision:  0.25171594508975714\n",
      "Recall:  0.8582358235823583\n",
      "F1:  0.3892631149214125\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for OCSVM\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "kernels = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.0001, 0.00001]\n",
    "\n",
    "for k in kernels:\n",
    "    for d in degrees:\n",
    "        for g in gammas:\n",
    "            for c in coef0s:\n",
    "                for t in tols:\n",
    "                    ovscmParams = {\n",
    "                        \"kernel\": k,\n",
    "                        \"degree\": d if k == \"poly\" else 3,\n",
    "                        \"gamma\": g,\n",
    "                        \"coef0\": c if k == \"poly\" or k == \"sigmoid\" else 0.0,\n",
    "                        \"tol\": t,\n",
    "                        \"nu\": 0.001,\n",
    "                        \"shrinking\": True,\n",
    "                        \"cache_size\": 3200,\n",
    "                        \"verbose\": False,\n",
    "                        \"max_iter\": -1,\n",
    "                    }\n",
    "\n",
    "                    hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=11,\n",
    "                                 clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                                 one_hot=False)\n",
    "                    (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                    print(\"for kernel: \", k, \" degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                    print(avg)\n",
    "                    print(cm)\n",
    "                    (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                    print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                    print(\"Precision: \", tp / (tp + fp))\n",
    "                    print(\"Recall: \", tp / (tp + fn))\n",
    "                    print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                    print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6910205869469995\n",
      "[[6581 2167]\n",
      " [1360 1307]]\n",
      "Accuracy:  0.6910205869469995\n",
      "Precision:  0.37622337363270003\n",
      "Recall:  0.49006374203224595\n",
      "F1:  0.425663572708028\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"degree\": 3,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.0,\n",
    "    \"tol\": 0.0001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 11, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  26 1432]\n",
      " [   7 2215]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6073485056210584\n",
      "Recall:  0.9968496849684968\n",
      "F1:  0.7548134264781053\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  23 1435]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074945295404814\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7556992174208914\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  25 1433]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078270388615217\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7559564329475834\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6168478260869565\n",
      "[[  54 1404]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6168478260869565\n",
      "Precision:  0.6121546961325967\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7586442998972954\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6103260869565217\n",
      "[[  24 1434]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6077680525164114\n",
      "Recall:  1.0\n",
      "F1:  0.7560394692072133\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  17 1441]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064991807755324\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7549286199864038\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6078804347826087\n",
      "[[  16 1442]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6078804347826087\n",
      "Precision:  0.6063336063336063\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7548003398470688\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  19 1439]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069379950833106\n",
      "Recall:  1.0\n",
      "F1:  0.7553969063403025\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  3  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6108695652173913\n",
      "[[  26 1432]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6081007115489874\n",
      "Recall:  1.0\n",
      "F1:  0.7562968005445881\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  22 1436]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6073284112660651\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7555706752849124\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.647554347826087\n",
      "[[ 310 1148]\n",
      " [ 149 2073]]\n",
      "Accuracy:  0.647554347826087\n",
      "Precision:  0.6435889475318224\n",
      "Recall:  0.9329432943294329\n",
      "F1:  0.7617122910159838\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.61875\n",
      "[[  75 1383]\n",
      " [  20 2202]]\n",
      "Accuracy:  0.61875\n",
      "Precision:  0.6142259414225941\n",
      "Recall:  0.990999099909991\n",
      "F1:  0.7583950404684002\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6084239130434783\n",
      "[[  17 1441]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6084239130434783\n",
      "Precision:  0.6066066066066066\n",
      "Recall:  1.0\n",
      "F1:  0.7551401869158878\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  4  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.6108695652173913\n",
      "[[  32 1426]\n",
      " [   6 2216]]\n",
      "Accuracy:  0.6108695652173913\n",
      "Precision:  0.6084568918176826\n",
      "Recall:  0.9972997299729973\n",
      "F1:  0.7557980900409277\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.6105978260869566\n",
      "[[  25 1433]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6105978260869566\n",
      "Precision:  0.6079343365253078\n",
      "Recall:  1.0\n",
      "F1:  0.7561681129828144\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  21 1437]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.6071623838162931\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7554421768707483\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.6095108695652174\n",
      "[[  21 1437]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6095108695652174\n",
      "Precision:  0.6072697458321946\n",
      "Recall:  1.0\n",
      "F1:  0.755653800374086\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.6097826086956522\n",
      "[[  22 1436]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6097826086956522\n",
      "Precision:  0.6074357572443958\n",
      "Recall:  1.0\n",
      "F1:  0.7557823129251701\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.6111413043478261\n",
      "[[  31 1427]\n",
      " [   4 2218]]\n",
      "Accuracy:  0.6111413043478261\n",
      "Precision:  0.6085048010973937\n",
      "Recall:  0.9981998199819982\n",
      "F1:  0.7560934037838759\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.6103260869565217\n",
      "[[  26 1432]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6103260869565217\n",
      "Precision:  0.6078860898138007\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7558733401430031\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.6144021739130435\n",
      "[[  41 1417]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6144021739130435\n",
      "Precision:  0.6103931811932912\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.7578084997439836\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.6059782608695652\n",
      "[[   8 1450]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6059782608695652\n",
      "Precision:  0.605119825708061\n",
      "Recall:  1.0\n",
      "F1:  0.7539871055310485\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.6293478260869565\n",
      "[[ 732  726]\n",
      " [ 638 1584]]\n",
      "Accuracy:  0.6293478260869565\n",
      "Precision:  0.6857142857142857\n",
      "Recall:  0.7128712871287128\n",
      "F1:  0.6990291262135923\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.6157608695652174\n",
      "[[  49 1409]\n",
      " [   5 2217]]\n",
      "Accuracy:  0.6157608695652174\n",
      "Precision:  0.6114175399889685\n",
      "Recall:  0.9977497749774977\n",
      "F1:  0.7582079343365253\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.6141304347826086\n",
      "[[  40 1418]\n",
      " [   2 2220]]\n",
      "Accuracy:  0.6141304347826086\n",
      "Precision:  0.6102253985706432\n",
      "Recall:  0.9990999099909991\n",
      "F1:  0.757679180887372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.6092391304347826\n",
      "[[  20 1438]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6092391304347826\n",
      "Precision:  0.607103825136612\n",
      "Recall:  1.0\n",
      "F1:  0.7555253315198912\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.6089673913043478\n",
      "[[  20 1438]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.6089673913043478\n",
      "Precision:  0.6069964471166985\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.7553137221560959\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  5  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.6081521739130434\n",
      "[[  16 1442]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.6081521739130434\n",
      "Precision:  0.6064410480349345\n",
      "Recall:  1.0\n",
      "F1:  0.7550118926265715\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 10% of the data\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [3, 4, 5]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "0.6518787878787878\n",
      "[[  23 1435]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6518787878787878\n",
      "Precision:  0.6500853450377957\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7878250591016549\n"
     ]
    }
   ],
   "source": [
    "ocsvm = OCSVM()\n",
    "\n",
    "ovscmParams = {\n",
    "    \"kernel\": \"poly\",\n",
    "    \"degree\": 5,\n",
    "    \"gamma\": \"scale\",\n",
    "    \"coef0\": 0.3,\n",
    "    \"tol\": 0.001,\n",
    "    \"nu\": 0.001,\n",
    "    \"shrinking\": True,\n",
    "    \"cache_size\": 2800,\n",
    "    \"verbose\": False,\n",
    "    \"max_iter\": -1,\n",
    "}\n",
    "\n",
    "hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "(avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp/(2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  200\n",
      "0.6681212121212121\n",
      "[[ 217 1241]\n",
      " [ 128 2539]]\n",
      "Accuracy:  0.6681212121212121\n",
      "Precision:  0.6716931216931217\n",
      "Recall:  0.9520059992500938\n",
      "F1:  0.7876531720179929\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  250\n",
      "0.6678787878787878\n",
      "[[ 213 1245]\n",
      " [ 125 2542]]\n",
      "Accuracy:  0.6678787878787878\n",
      "Precision:  0.6712437285450225\n",
      "Recall:  0.9531308586426697\n",
      "F1:  0.7877285404400372\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  300\n",
      "0.6693333333333333\n",
      "[[ 215 1243]\n",
      " [ 121 2546]]\n",
      "Accuracy:  0.6693333333333333\n",
      "Precision:  0.6719451042491422\n",
      "Recall:  0.9546306711661042\n",
      "F1:  0.788723667905824\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  1500  leaf_size:  350\n",
      "0.6676363636363636\n",
      "[[ 210 1248]\n",
      " [ 123 2544]]\n",
      "Accuracy:  0.6676363636363636\n",
      "Precision:  0.6708860759493671\n",
      "Recall:  0.953880764904387\n",
      "F1:  0.7877380399442638\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  200\n",
      "0.6557575757575758\n",
      "[[  78 1380]\n",
      " [  40 2627]]\n",
      "Accuracy:  0.6557575757575758\n",
      "Precision:  0.6556026952832543\n",
      "Recall:  0.9850018747656543\n",
      "F1:  0.7872340425531915\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  250\n",
      "0.6564848484848484\n",
      "[[  85 1373]\n",
      " [  44 2623]]\n",
      "Accuracy:  0.6564848484848484\n",
      "Precision:  0.6564064064064065\n",
      "Recall:  0.9835020622422197\n",
      "F1:  0.7873330331682425\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  300\n",
      "0.6591515151515152\n",
      "[[  83 1375]\n",
      " [  31 2636]]\n",
      "Accuracy:  0.6591515151515152\n",
      "Precision:  0.6571927200199451\n",
      "Recall:  0.9883764529433821\n",
      "F1:  0.7894579215333932\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2000  leaf_size:  350\n",
      "0.6574545454545454\n",
      "[[  88 1370]\n",
      " [  43 2624]]\n",
      "Accuracy:  0.6574545454545454\n",
      "Precision:  0.656985478217326\n",
      "Recall:  0.9838770153730784\n",
      "F1:  0.7878696892358504\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  200\n",
      "0.6484848484848484\n",
      "[[  20 1438]\n",
      " [  12 2655]]\n",
      "Accuracy:  0.6484848484848484\n",
      "Precision:  0.6486684583435133\n",
      "Recall:  0.9955005624296963\n",
      "F1:  0.7855029585798816\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  250\n",
      "0.6477575757575758\n",
      "[[  20 1438]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6477575757575758\n",
      "Precision:  0.6484107579462103\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.784963741305313\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  300\n",
      "0.6494545454545455\n",
      "[[  25 1433]\n",
      " [  13 2654]]\n",
      "Accuracy:  0.6494545454545455\n",
      "Precision:  0.6493760704673355\n",
      "Recall:  0.9951256092988376\n",
      "F1:  0.7859046490968316\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  2500  leaf_size:  350\n",
      "0.6487272727272727\n",
      "[[  24 1434]\n",
      " [  15 2652]]\n",
      "Accuracy:  0.6487272727272727\n",
      "Precision:  0.6490455212922174\n",
      "Recall:  0.9943757030371203\n",
      "F1:  0.7854286983562861\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  200\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  250\n",
      "0.6472727272727272\n",
      "[[   4 1454]\n",
      " [   1 2666]]\n",
      "Accuracy:  0.6472727272727272\n",
      "Precision:  0.6470873786407767\n",
      "Recall:  0.9996250468691413\n",
      "F1:  0.7856195668189185\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  300\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for n_neighbors:  3000  leaf_size:  350\n",
      "0.6475151515151515\n",
      "[[   4 1454]\n",
      " [   0 2667]]\n",
      "Accuracy:  0.6475151515151515\n",
      "Precision:  0.6471730162581898\n",
      "Recall:  1.0\n",
      "F1:  0.785798467884502\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid search for LOF\n",
    "\n",
    "lof = LOF()\n",
    "\n",
    "n_neighbors = [1500,2000,2500,3000]\n",
    "leaf_sizes = [200,250,300,350]\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for l in leaf_sizes:\n",
    "        lofParams = {\n",
    "            \"n_neighbors\": n,\n",
    "            \"algorithm\": \"auto\",\n",
    "            \"leaf_size\": l,\n",
    "            \"metric\": \"minkowski\",\n",
    "            \"p\": 2,\n",
    "            \"metric_params\": None,\n",
    "            \"contamination\": \"auto\",\n",
    "            \"novelty\": True,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "\n",
    "        hp = Hparams(lof_hparams=lofParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.30,window_size= 8, window_slide= 1,one_hot=False)\n",
    "        (avg, cm) = model_train_eval(lof, data, hp)\n",
    "        print(\"for n_neighbors: \", n, \" leaf_size: \", l)\n",
    "        print(avg)\n",
    "        print(cm)\n",
    "        (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "        print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "        print(\"Precision: \", tp / (tp + fp))\n",
    "        print(\"Recall: \", tp / (tp + fn))\n",
    "        print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "        print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.001\n",
      "0.22790743091494972\n",
      "[[ 112 7907]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21937012538256492\n",
      "Recall:  1.0\n",
      "F1:  0.3598089223544652\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.0  tol:  0.0001\n",
      "0.22820037105751392\n",
      "[[ 116 7903]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22820037105751392\n",
      "Precision:  0.21937969182141445\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597926453912198\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.1  tol:  0.0001\n",
      "0.22800507762913777\n",
      "[[ 113 7906]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22800507762913777\n",
      "Precision:  0.21939178515007898\n",
      "Recall:  1.0\n",
      "F1:  0.35983805668016194\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.2  tol:  0.0001\n",
      "0.22868860462845425\n",
      "[[ 120 7899]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22868860462845425\n",
      "Precision:  0.2195435233672562\n",
      "Recall:  1.0\n",
      "F1:  0.3600421291420238\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.001\n",
      "0.22595449663118836\n",
      "[[  92 7927]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22595449663118836\n",
      "Precision:  0.21893782638683615\n",
      "Recall:  1.0\n",
      "F1:  0.35922722496160375\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.3  tol:  0.0001\n",
      "0.22663802363050484\n",
      "[[  99 7920]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22663802363050484\n",
      "Precision:  0.21908893709327548\n",
      "Recall:  1.0\n",
      "F1:  0.3594306049822064\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.001\n",
      "0.22771213748657357\n",
      "[[ 110 7909]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22771213748657357\n",
      "Precision:  0.21932681867535286\n",
      "Recall:  1.0\n",
      "F1:  0.3597506678539626\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  scale  coef0:  0.4  tol:  0.0001\n",
      "0.22790743091494972\n",
      "[[ 113 7906]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22790743091494972\n",
      "Precision:  0.21931470326849017\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3597052392906308\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.001\n",
      "0.22986036519871106\n",
      "[[ 132 7887]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22986036519871106\n",
      "Precision:  0.21980413492927095\n",
      "Recall:  1.0\n",
      "F1:  0.360392506690455\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.0  tol:  0.0001\n",
      "0.22810272434332585\n",
      "[[ 114 7905]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22810272434332585\n",
      "Precision:  0.21941344919522068\n",
      "Recall:  1.0\n",
      "F1:  0.35986719572435016\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.001\n",
      "0.22546626306024803\n",
      "[[  88 7931]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22546626306024803\n",
      "Precision:  0.2187746256895193\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3589785033133991\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.1  tol:  0.0001\n",
      "0.22585684991700028\n",
      "[[  91 7928]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22585684991700028\n",
      "Precision:  0.21891625615763546\n",
      "Recall:  1.0\n",
      "F1:  0.35919818946007115\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.001\n",
      "0.22634508348794063\n",
      "[[  96 7923]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22634508348794063\n",
      "Precision:  0.21902414982750124\n",
      "Recall:  1.0\n",
      "F1:  0.359343413924153\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.2  tol:  0.0001\n",
      "0.22605214334537643\n",
      "[[  93 7926]\n",
      " [   0 2222]]\n",
      "Accuracy:  0.22605214334537643\n",
      "Precision:  0.21895940086716595\n",
      "Recall:  1.0\n",
      "F1:  0.3592562651576395\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.001\n",
      "0.2261497900595645\n",
      "[[  95 7924]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.2261497900595645\n",
      "Precision:  0.21892557910300642\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3591816932158163\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "for degree:  2  gamma:  auto  coef0:  0.3  tol:  0.0001\n",
      "0.22761449077238552\n",
      "[[ 110 7909]\n",
      " [   1 2221]]\n",
      "Accuracy:  0.22761449077238552\n",
      "Precision:  0.2192497532082922\n",
      "Recall:  0.9995499549954996\n",
      "F1:  0.3596178756476684\n",
      "---------------------------------------------------\n",
      "clean_df: 8890 anomalous_df: 729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 30\u001B[0m\n\u001B[1;32m     14\u001B[0m ovscmParams \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkernel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdegree\u001B[39m\u001B[38;5;124m\"\u001B[39m: d,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     25\u001B[0m }\n\u001B[1;32m     27\u001B[0m hp \u001B[38;5;241m=\u001B[39m Hparams(ocsvm_hparams\u001B[38;5;241m=\u001B[39movscmParams, anomaly_generation_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     28\u001B[0m              clean_test_data_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m, window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, window_slide\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     29\u001B[0m              one_hot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m (avg, cm) \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_train_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mocsvm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor degree: \u001B[39m\u001B[38;5;124m\"\u001B[39m, d, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m gamma: \u001B[39m\u001B[38;5;124m\"\u001B[39m, g, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m coef0: \u001B[39m\u001B[38;5;124m\"\u001B[39m, c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m tol: \u001B[39m\u001B[38;5;124m\"\u001B[39m, t)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(avg)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/model_validator.py:36\u001B[0m, in \u001B[0;36mmodel_train_eval\u001B[0;34m(model, data, hparams)\u001B[0m\n\u001B[1;32m     33\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m clean_df[:split_index]\n\u001B[1;32m     34\u001B[0m training_df \u001B[38;5;241m=\u001B[39m clean_df[split_index:]\n\u001B[0;32m---> 36\u001B[0m generated_anomalies_df \u001B[38;5;241m=\u001B[39m \u001B[43msynthetic_anomaly_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manomaly_generation_ratio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m anomalous_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([anomalous_df, generated_anomalies_df])\n\u001B[1;32m     39\u001B[0m testing_df \u001B[38;5;241m=\u001B[39m testing_df\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:28\u001B[0m, in \u001B[0;36msynthetic_anomaly_generator\u001B[0;34m(anomalous_windows, anomaly_generation_ratio)\u001B[0m\n\u001B[1;32m     25\u001B[0m anomalous_windows \u001B[38;5;241m=\u001B[39m anomalous_windows\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index_window, window \u001B[38;5;129;01min\u001B[39;00m anomalous_windows\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# Get the activity, reason and type of anomaly window\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m     activity, reason, type_anomaly, window \u001B[38;5;241m=\u001B[39m \u001B[43mextract_anomaly_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     new_activity: List[pd\u001B[38;5;241m.\u001B[39mDataFrame] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     30\u001B[0m     new_duration \u001B[38;5;241m=\u001B[39m timedelta(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/multi_modal_edge_ai/models/anomaly_detection/train_and_eval/synthetic_anomaly_generator.py:63\u001B[0m, in \u001B[0;36mextract_anomaly_details\u001B[0;34m(anomalous_windows, index_window)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_anomaly_details\u001B[39m(anomalous_windows: pd\u001B[38;5;241m.\u001B[39mDataFrame, index_window: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \\\n\u001B[1;32m     55\u001B[0m         Tuple[pd\u001B[38;5;241m.\u001B[39mDataFrame, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m, pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m     56\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    This function will process the window and return the activity, reason and type of anomaly\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m    :param anomalous_windows: the windows to be processed\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;03m    :param index_window: the index of the window to be processed\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    :return: the activity, reason, type of anomaly and the window\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m     window \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43manomalous_windows\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex_window\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     reason \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     65\u001B[0m     type_anomaly \u001B[38;5;241m=\u001B[39m window[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReason\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/frame.py:790\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    781\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m    782\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m nested_data_to_arrays(\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;00m\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    788\u001B[0m         dtype,\n\u001B[1;32m    789\u001B[0m     )\n\u001B[0;32m--> 790\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    798\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m ndarray_to_mgr(\n\u001B[1;32m    799\u001B[0m         data,\n\u001B[1;32m    800\u001B[0m         index,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    804\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[1;32m    805\u001B[0m     )\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:153\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    150\u001B[0m axes \u001B[38;5;241m=\u001B[39m [columns, index]\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_block_manager_from_column_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefs\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2137\u001B[0m, in \u001B[0;36mcreate_block_manager_from_column_arrays\u001B[0;34m(arrays, axes, consolidate, refs)\u001B[0m\n\u001B[1;32m   2119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_block_manager_from_column_arrays\u001B[39m(\n\u001B[1;32m   2120\u001B[0m     arrays: \u001B[38;5;28mlist\u001B[39m[ArrayLike],\n\u001B[1;32m   2121\u001B[0m     axes: \u001B[38;5;28mlist\u001B[39m[Index],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2133\u001B[0m     \u001B[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001B[39;00m\n\u001B[1;32m   2134\u001B[0m     \u001B[38;5;66;03m#  verify_integrity=False below.\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2137\u001B[0m         blocks \u001B[38;5;241m=\u001B[39m \u001B[43m_form_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2138\u001B[0m         mgr \u001B[38;5;241m=\u001B[39m BlockManager(blocks, axes, verify_integrity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2203\u001B[0m, in \u001B[0;36m_form_blocks\u001B[0;34m(arrays, consolidate, refs)\u001B[0m\n\u001B[1;32m   2196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nbs\n\u001B[1;32m   2198\u001B[0m \u001B[38;5;66;03m# when consolidating, we can ignore refs (either stacking always copies,\u001B[39;00m\n\u001B[1;32m   2199\u001B[0m \u001B[38;5;66;03m# or the EA is already copied in the calling dict_to_mgr)\u001B[39;00m\n\u001B[1;32m   2200\u001B[0m \u001B[38;5;66;03m# TODO(CoW) check if this is also valid for rec_array_to_mgr\u001B[39;00m\n\u001B[1;32m   2201\u001B[0m \n\u001B[1;32m   2202\u001B[0m \u001B[38;5;66;03m# group by dtype\u001B[39;00m\n\u001B[0;32m-> 2203\u001B[0m grouper \u001B[38;5;241m=\u001B[39m \u001B[43mitertools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtuples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_grouping_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2205\u001B[0m nbs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   2206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (_, _, dtype), tup_block \u001B[38;5;129;01min\u001B[39;00m grouper:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [2,3,4]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.03566889837384224\n",
      "Epoch 2 training loss: 0.01830665022134781\n",
      "Epoch 3 training loss: 0.013418372720479965\n",
      "Epoch 4 training loss: 0.011778959073126316\n",
      "Epoch 5 training loss: 0.01130879856646061\n",
      "Epoch 6 training loss: 0.011056452058255672\n",
      "Epoch 7 training loss: 0.01079351082444191\n",
      "Epoch 8 training loss: 0.010838769376277924\n",
      "Epoch 9 training loss: 0.010137386620044708\n",
      "Epoch 10 training loss: 0.010338276624679565\n",
      "Epoch 11 training loss: 0.010535934008657932\n",
      "Epoch 12 training loss: 0.009792396798729897\n",
      "Epoch 13 training loss: 0.009921427816152573\n",
      "Epoch 14 training loss: 0.009429894387722015\n",
      "Epoch 15 training loss: 0.008734602481126785\n",
      "Epoch 16 training loss: 0.009123115800321102\n",
      "Epoch 17 training loss: 0.009666664525866508\n",
      "Epoch 18 training loss: 0.008979364298284054\n",
      "Epoch 19 training loss: 0.009243450127542019\n",
      "Epoch 20 training loss: 0.008844000287353992\n",
      "Epoch 21 training loss: 0.00910976342856884\n",
      "Epoch 22 training loss: 0.008840230293571949\n",
      "Epoch 23 training loss: 0.008172215893864632\n",
      "Epoch 24 training loss: 0.009498702362179756\n",
      "Epoch 25 training loss: 0.00898265652358532\n",
      "Epoch 26 training loss: 0.008497258648276329\n",
      "Epoch 27 training loss: 0.008709250018000603\n",
      "Epoch 28 training loss: 0.008624088950455189\n",
      "Epoch 29 training loss: 0.00815357081592083\n",
      "Epoch 30 training loss: 0.009594975039362907\n",
      "Epoch 31 training loss: 0.009273276664316654\n",
      "Epoch 32 training loss: 0.008483963087201118\n",
      "0.8185158722057773\n",
      "[[8019    0]\n",
      " [1778    0]]\n",
      "Accuracy: 0.8185158722057773\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/4smfbzk97dvdhsvkx0y_yb5c0000gp/T/ipykernel_98989/868071132.py:10: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(\"Precision: \", tp / (tp + fp))\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 16, 8], [8, 16, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=10, clean_test_data_ratio=0.2, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [100, 200, 400, 800]\n",
    "bootstraps = [True, False]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "    for b in bootstraps:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"bootstrap\": b,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T10:01:00.691392800Z",
     "start_time": "2023-06-15T10:00:49.584565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.04122960567474365\n",
      "Epoch 2 training loss: 0.032301533967256546\n",
      "Epoch 3 training loss: 0.029458846896886826\n",
      "Epoch 4 training loss: 0.029231302440166473\n",
      "Epoch 5 training loss: 0.026410769671201706\n",
      "Epoch 6 training loss: 0.024050837382674217\n",
      "Epoch 7 training loss: 0.02383429929614067\n",
      "Epoch 8 training loss: 0.02304689958691597\n",
      "Epoch 9 training loss: 0.023099087178707123\n",
      "Epoch 10 training loss: 0.022938374429941177\n",
      "Epoch 11 training loss: 0.021551033481955528\n",
      "Epoch 12 training loss: 0.020283041521906853\n",
      "Epoch 13 training loss: 0.019526956602931023\n",
      "Epoch 14 training loss: 0.018245210871100426\n",
      "Epoch 15 training loss: 0.01643492467701435\n",
      "Epoch 16 training loss: 0.01508433185517788\n",
      "Epoch 17 training loss: 0.014789544977247715\n",
      "Epoch 18 training loss: 0.014925693161785603\n",
      "Epoch 19 training loss: 0.013959605246782303\n",
      "Epoch 20 training loss: 0.014511987566947937\n",
      "Epoch 21 training loss: 0.014036488719284534\n",
      "Epoch 22 training loss: 0.01356128416955471\n",
      "Epoch 23 training loss: 0.013709161430597305\n",
      "Epoch 24 training loss: 0.014896780252456665\n",
      "Epoch 25 training loss: 0.013893967494368553\n",
      "Epoch 26 training loss: 0.013346045278012753\n",
      "Epoch 27 training loss: 0.013873269781470299\n",
      "Epoch 28 training loss: 0.013923469930887222\n",
      "Epoch 29 training loss: 0.013328543864190578\n",
      "Epoch 30 training loss: 0.013698993250727654\n",
      "Epoch 31 training loss: 0.013894200325012207\n",
      "Epoch 32 training loss: 0.01420495193451643\n",
      "0.7647831800262812\n",
      "[[8730   18]\n",
      " [2667    0]]\n",
      "Accuracy: 0.7647831800262812\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=False\n",
    "autoenc = Autoencoder([40, 32, 24, 16, 8], [8, 16, 24, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=32, anomaly_generation_ratio=11, clean_test_data_ratio=0.3, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.058898258954286575\n",
      "Epoch 2 training loss: 0.05155443772673607\n",
      "Epoch 3 training loss: 0.05057140812277794\n",
      "Epoch 4 training loss: 0.04858234152197838\n",
      "Epoch 5 training loss: 0.04724820703268051\n",
      "Epoch 6 training loss: 0.04657506197690964\n",
      "Epoch 7 training loss: 0.045772116631269455\n",
      "Epoch 8 training loss: 0.045489318668842316\n",
      "Epoch 9 training loss: 0.045221779495477676\n",
      "Epoch 10 training loss: 0.04497319832444191\n",
      "Epoch 11 training loss: 0.043941523879766464\n",
      "Epoch 12 training loss: 0.04381180927157402\n",
      "Epoch 13 training loss: 0.04358973726630211\n",
      "Epoch 14 training loss: 0.043723657727241516\n",
      "Epoch 15 training loss: 0.04434793069958687\n",
      "Epoch 16 training loss: 0.04259477183222771\n",
      "Epoch 17 training loss: 0.04313371703028679\n",
      "Epoch 18 training loss: 0.04417560249567032\n",
      "Epoch 19 training loss: 0.04458608478307724\n",
      "Epoch 20 training loss: 0.04394195228815079\n",
      "Epoch 21 training loss: 0.04413234442472458\n",
      "Epoch 22 training loss: 0.04372331127524376\n",
      "Epoch 23 training loss: 0.04293721169233322\n",
      "Epoch 24 training loss: 0.04323875531554222\n",
      "Epoch 25 training loss: 0.042636580765247345\n",
      "Epoch 26 training loss: 0.042571090161800385\n",
      "Epoch 27 training loss: 0.042420923709869385\n",
      "Epoch 28 training loss: 0.042121902108192444\n",
      "Epoch 29 training loss: 0.04230155050754547\n",
      "Epoch 30 training loss: 0.04195545241236687\n",
      "Epoch 31 training loss: 0.04092766344547272\n",
      "Epoch 32 training loss: 0.04151569679379463\n",
      "Epoch 33 training loss: 0.040453843772411346\n",
      "Epoch 34 training loss: 0.04354794695973396\n",
      "Epoch 35 training loss: 0.040512025356292725\n",
      "Epoch 36 training loss: 0.040154166519641876\n",
      "Epoch 37 training loss: 0.04132810980081558\n",
      "Epoch 38 training loss: 0.040305864065885544\n",
      "Epoch 39 training loss: 0.04092114418745041\n",
      "Epoch 40 training loss: 0.04208670184016228\n",
      "Epoch 41 training loss: 0.04183347523212433\n",
      "Epoch 42 training loss: 0.04109683260321617\n",
      "Epoch 43 training loss: 0.04025791957974434\n",
      "Epoch 44 training loss: 0.03995497524738312\n",
      "Epoch 45 training loss: 0.04097607359290123\n",
      "Epoch 46 training loss: 0.05953291058540344\n",
      "Epoch 47 training loss: 0.07895916700363159\n",
      "Epoch 48 training loss: 0.06983033567667007\n",
      "Epoch 49 training loss: 0.06246565654873848\n",
      "Epoch 50 training loss: 0.04813380911946297\n",
      "Epoch 51 training loss: 0.0453314334154129\n",
      "Epoch 52 training loss: 0.04376808926463127\n",
      "Epoch 53 training loss: 0.042266298085451126\n",
      "Epoch 54 training loss: 0.053543899208307266\n",
      "Epoch 55 training loss: 0.06179076060652733\n",
      "Epoch 56 training loss: 0.05984741076827049\n",
      "Epoch 57 training loss: 0.061102330684661865\n",
      "Epoch 58 training loss: 0.05008704960346222\n",
      "Epoch 59 training loss: 0.04680133983492851\n",
      "Epoch 60 training loss: 0.045489221811294556\n",
      "Epoch 61 training loss: 0.045494046062231064\n",
      "Epoch 62 training loss: 0.045764144510030746\n",
      "Epoch 63 training loss: 0.0467325896024704\n",
      "Epoch 64 training loss: 0.04569430276751518\n",
      "0.47950589556428974\n",
      "[[2888 5131]\n",
      " [ 431 2236]]\n",
      "Accuracy: 0.47950589556428974\n",
      "Precision:  0.30351567802361884\n",
      "Recall:  0.838395200599925\n",
      "F1:  0.44568467211480967\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.05829282104969025\n",
      "Epoch 2 training loss: 0.05097535252571106\n",
      "Epoch 3 training loss: 0.0498952753841877\n",
      "Epoch 4 training loss: 0.04790065810084343\n",
      "Epoch 5 training loss: 0.04763051122426987\n",
      "Epoch 6 training loss: 0.0501713901758194\n",
      "Epoch 7 training loss: 0.05156087875366211\n",
      "Epoch 8 training loss: 0.04860074445605278\n",
      "Epoch 9 training loss: 0.04710253328084946\n",
      "Epoch 10 training loss: 0.04843513295054436\n",
      "Epoch 11 training loss: 0.04762199521064758\n",
      "Epoch 12 training loss: 0.0470193512737751\n",
      "Epoch 13 training loss: 0.04710486903786659\n",
      "Epoch 14 training loss: 0.04916980862617493\n",
      "Epoch 15 training loss: 0.047472093254327774\n",
      "Epoch 16 training loss: 0.045982830226421356\n",
      "Epoch 17 training loss: 0.047718703746795654\n",
      "Epoch 18 training loss: 0.046799905598163605\n",
      "Epoch 19 training loss: 0.0502435639500618\n",
      "Epoch 20 training loss: 0.05604420229792595\n",
      "Epoch 21 training loss: 0.05407804250717163\n",
      "Epoch 22 training loss: 0.05319788679480553\n",
      "Epoch 23 training loss: 0.05286208540201187\n",
      "Epoch 24 training loss: 0.05242634192109108\n",
      "Epoch 25 training loss: 0.051848530769348145\n",
      "Epoch 26 training loss: 0.05164097994565964\n",
      "Epoch 27 training loss: 0.05050581693649292\n",
      "Epoch 28 training loss: 0.05016383156180382\n",
      "Epoch 29 training loss: 0.05007558315992355\n",
      "Epoch 30 training loss: 0.051285270601511\n",
      "Epoch 31 training loss: 0.07751145958900452\n",
      "Epoch 32 training loss: 0.0772283524274826\n",
      "Epoch 33 training loss: 0.07716844975948334\n",
      "Epoch 34 training loss: 0.07719416171312332\n",
      "Epoch 35 training loss: 0.07715099304914474\n",
      "Epoch 36 training loss: 0.07715228199958801\n",
      "Epoch 37 training loss: 0.07712981849908829\n",
      "Epoch 38 training loss: 0.0771409124135971\n",
      "Epoch 39 training loss: 0.07716117054224014\n",
      "Epoch 40 training loss: 0.07712095975875854\n",
      "Epoch 41 training loss: 0.07708992063999176\n",
      "Epoch 42 training loss: 0.07712382078170776\n",
      "Epoch 43 training loss: 0.07712587714195251\n",
      "Epoch 44 training loss: 0.07708938419818878\n",
      "Epoch 45 training loss: 0.07708832621574402\n",
      "Epoch 46 training loss: 0.07708758115768433\n",
      "Epoch 47 training loss: 0.07712376862764359\n",
      "Epoch 48 training loss: 0.07710060477256775\n",
      "Epoch 49 training loss: 0.07710567861795425\n",
      "Epoch 50 training loss: 0.07708684355020523\n",
      "Epoch 51 training loss: 0.07705388963222504\n",
      "Epoch 52 training loss: 0.07705193758010864\n",
      "Epoch 53 training loss: 0.07709895074367523\n",
      "Epoch 54 training loss: 0.07709212601184845\n",
      "Epoch 55 training loss: 0.07709264755249023\n",
      "Epoch 56 training loss: 0.07708374410867691\n",
      "Epoch 57 training loss: 0.07709158211946487\n",
      "Epoch 58 training loss: 0.07708431780338287\n",
      "Epoch 59 training loss: 0.07708100229501724\n",
      "Epoch 60 training loss: 0.0770869180560112\n",
      "Epoch 61 training loss: 0.07706017047166824\n",
      "Epoch 62 training loss: 0.07704704254865646\n",
      "Epoch 63 training loss: 0.07706429064273834\n",
      "Epoch 64 training loss: 0.07704580575227737\n",
      "Epoch 65 training loss: 0.07707887887954712\n",
      "Epoch 66 training loss: 0.07708815485239029\n",
      "Epoch 67 training loss: 0.07705943286418915\n",
      "Epoch 68 training loss: 0.07705625891685486\n",
      "Epoch 69 training loss: 0.07703600078821182\n",
      "Epoch 70 training loss: 0.07705473899841309\n",
      "Epoch 71 training loss: 0.07705537974834442\n",
      "Epoch 72 training loss: 0.07704281806945801\n",
      "Epoch 73 training loss: 0.07704969495534897\n",
      "Epoch 74 training loss: 0.07704239338636398\n",
      "Epoch 75 training loss: 0.07703730463981628\n",
      "Epoch 76 training loss: 0.07702906429767609\n",
      "Epoch 77 training loss: 0.07700634002685547\n",
      "Epoch 78 training loss: 0.07702942937612534\n",
      "Epoch 79 training loss: 0.07702913880348206\n",
      "Epoch 80 training loss: 0.07703971117734909\n",
      "Epoch 81 training loss: 0.0770302414894104\n",
      "Epoch 82 training loss: 0.07702948153018951\n",
      "Epoch 83 training loss: 0.07702755928039551\n",
      "Epoch 84 training loss: 0.07703575491905212\n",
      "Epoch 85 training loss: 0.07703883945941925\n",
      "Epoch 86 training loss: 0.07702811062335968\n",
      "Epoch 87 training loss: 0.07703310996294022\n",
      "Epoch 88 training loss: 0.07703188061714172\n",
      "Epoch 89 training loss: 0.0770219936966896\n",
      "Epoch 90 training loss: 0.07701282203197479\n",
      "Epoch 91 training loss: 0.07702331244945526\n",
      "Epoch 92 training loss: 0.07702778279781342\n",
      "Epoch 93 training loss: 0.07703300565481186\n",
      "Epoch 94 training loss: 0.07703817635774612\n",
      "Epoch 95 training loss: 0.07702020555734634\n",
      "Epoch 96 training loss: 0.07703542709350586\n",
      "Epoch 97 training loss: 0.07701947540044785\n",
      "Epoch 98 training loss: 0.07702722400426865\n",
      "Epoch 99 training loss: 0.07701949775218964\n",
      "Epoch 100 training loss: 0.07704021781682968\n",
      "Epoch 101 training loss: 0.07701585441827774\n",
      "Epoch 102 training loss: 0.07704055309295654\n",
      "Epoch 103 training loss: 0.07703545689582825\n",
      "Epoch 104 training loss: 0.07702822238206863\n",
      "Epoch 105 training loss: 0.07702390849590302\n",
      "Epoch 106 training loss: 0.07703152298927307\n",
      "Epoch 107 training loss: 0.07702185958623886\n",
      "Epoch 108 training loss: 0.07702432572841644\n",
      "Epoch 109 training loss: 0.07702384889125824\n",
      "Epoch 110 training loss: 0.07701688259840012\n",
      "Epoch 111 training loss: 0.07702643424272537\n",
      "Epoch 112 training loss: 0.07702019810676575\n",
      "Epoch 113 training loss: 0.07704506069421768\n",
      "Epoch 114 training loss: 0.07703159004449844\n",
      "Epoch 115 training loss: 0.07703684270381927\n",
      "Epoch 116 training loss: 0.07701458781957626\n",
      "Epoch 117 training loss: 0.07704203575849533\n",
      "Epoch 118 training loss: 0.07702761888504028\n",
      "Epoch 119 training loss: 0.07701722532510757\n",
      "Epoch 120 training loss: 0.07703814655542374\n",
      "Epoch 121 training loss: 0.07704107463359833\n",
      "Epoch 122 training loss: 0.0770290419459343\n",
      "Epoch 123 training loss: 0.07702631503343582\n",
      "Epoch 124 training loss: 0.07701708376407623\n",
      "Epoch 125 training loss: 0.07703052461147308\n",
      "Epoch 126 training loss: 0.07702711969614029\n",
      "Epoch 127 training loss: 0.07702083885669708\n",
      "Epoch 128 training loss: 0.07702898234128952\n",
      "Epoch 129 training loss: 0.07703835517168045\n",
      "Epoch 130 training loss: 0.07700995355844498\n",
      "Epoch 131 training loss: 0.07702820748090744\n",
      "Epoch 132 training loss: 0.07702609151601791\n",
      "Epoch 133 training loss: 0.07702130079269409\n",
      "Epoch 134 training loss: 0.07702707499265671\n",
      "Epoch 135 training loss: 0.07702576369047165\n",
      "Epoch 136 training loss: 0.07702488452196121\n",
      "Epoch 137 training loss: 0.07699967920780182\n",
      "Epoch 138 training loss: 0.07703471183776855\n",
      "Epoch 139 training loss: 0.07701798528432846\n",
      "Epoch 140 training loss: 0.07703015208244324\n",
      "Epoch 141 training loss: 0.07700847834348679\n",
      "Epoch 142 training loss: 0.07703274488449097\n",
      "Epoch 143 training loss: 0.0770258828997612\n",
      "Epoch 144 training loss: 0.07701438665390015\n",
      "Epoch 145 training loss: 0.07703520357608795\n",
      "Epoch 146 training loss: 0.07702981680631638\n",
      "Epoch 147 training loss: 0.07703104615211487\n",
      "Epoch 148 training loss: 0.07702022045850754\n",
      "Epoch 149 training loss: 0.07701580226421356\n",
      "Epoch 150 training loss: 0.07703147828578949\n",
      "Epoch 151 training loss: 0.07703186571598053\n",
      "Epoch 152 training loss: 0.07701794803142548\n",
      "Epoch 153 training loss: 0.07702413946390152\n",
      "Epoch 154 training loss: 0.07703357934951782\n",
      "Epoch 155 training loss: 0.07702314108610153\n",
      "Epoch 156 training loss: 0.07703682780265808\n",
      "Epoch 157 training loss: 0.07703470438718796\n",
      "Epoch 158 training loss: 0.07702498137950897\n",
      "Epoch 159 training loss: 0.07701396942138672\n",
      "Epoch 160 training loss: 0.07702178508043289\n",
      "Epoch 161 training loss: 0.07700908929109573\n",
      "Epoch 162 training loss: 0.07701052725315094\n",
      "Epoch 163 training loss: 0.0770343616604805\n",
      "Epoch 164 training loss: 0.07701237499713898\n",
      "Epoch 165 training loss: 0.07701359689235687\n",
      "Epoch 166 training loss: 0.07702820003032684\n",
      "Epoch 167 training loss: 0.07703246176242828\n",
      "Epoch 168 training loss: 0.07702775299549103\n",
      "Epoch 169 training loss: 0.07704504579305649\n",
      "Epoch 170 training loss: 0.07701685279607773\n",
      "Epoch 171 training loss: 0.07703884690999985\n",
      "Epoch 172 training loss: 0.07703987509012222\n",
      "Epoch 173 training loss: 0.07701688259840012\n",
      "Epoch 174 training loss: 0.07703504711389542\n",
      "Epoch 175 training loss: 0.07703150063753128\n",
      "Epoch 176 training loss: 0.07702753692865372\n",
      "Epoch 177 training loss: 0.07702451944351196\n",
      "Epoch 178 training loss: 0.07702696323394775\n",
      "Epoch 179 training loss: 0.07703723013401031\n",
      "Epoch 180 training loss: 0.07703053206205368\n",
      "Epoch 181 training loss: 0.0770321935415268\n",
      "Epoch 182 training loss: 0.07702634483575821\n",
      "Epoch 183 training loss: 0.07703079283237457\n",
      "Epoch 184 training loss: 0.07704038918018341\n",
      "Epoch 185 training loss: 0.07702688872814178\n",
      "Epoch 186 training loss: 0.07701125741004944\n",
      "Epoch 187 training loss: 0.07702894508838654\n",
      "Epoch 188 training loss: 0.07702581584453583\n",
      "Epoch 189 training loss: 0.07704629749059677\n",
      "Epoch 190 training loss: 0.07702954113483429\n",
      "Epoch 191 training loss: 0.07702025026082993\n",
      "Epoch 192 training loss: 0.07700329273939133\n",
      "Epoch 193 training loss: 0.07702518999576569\n",
      "Epoch 194 training loss: 0.07702670246362686\n",
      "Epoch 195 training loss: 0.07702674716711044\n",
      "Epoch 196 training loss: 0.07700877636671066\n",
      "Epoch 197 training loss: 0.07702724635601044\n",
      "Epoch 198 training loss: 0.07703027129173279\n",
      "Epoch 199 training loss: 0.07702340930700302\n",
      "Epoch 200 training loss: 0.07702601701021194\n",
      "Epoch 201 training loss: 0.07704586535692215\n",
      "Epoch 202 training loss: 0.07702969014644623\n",
      "Epoch 203 training loss: 0.07702009379863739\n",
      "Epoch 204 training loss: 0.0770278349518776\n",
      "Epoch 205 training loss: 0.07703326642513275\n",
      "Epoch 206 training loss: 0.07701940834522247\n",
      "Epoch 207 training loss: 0.07703382521867752\n",
      "Epoch 208 training loss: 0.07702875137329102\n",
      "Epoch 209 training loss: 0.07701776176691055\n",
      "Epoch 210 training loss: 0.07703078538179398\n",
      "Epoch 211 training loss: 0.07701732963323593\n",
      "Epoch 212 training loss: 0.07701758295297623\n",
      "Epoch 213 training loss: 0.07702015340328217\n",
      "Epoch 214 training loss: 0.0770338624715805\n",
      "Epoch 215 training loss: 0.07702308148145676\n",
      "Epoch 216 training loss: 0.07702900469303131\n",
      "Epoch 217 training loss: 0.07703094929456711\n",
      "Epoch 218 training loss: 0.0770266205072403\n",
      "Epoch 219 training loss: 0.07702970504760742\n",
      "Epoch 220 training loss: 0.07701800018548965\n",
      "Epoch 221 training loss: 0.07701873034238815\n",
      "Epoch 222 training loss: 0.07701938599348068\n",
      "Epoch 223 training loss: 0.07697942852973938\n",
      "Epoch 224 training loss: 0.07701562345027924\n",
      "Epoch 225 training loss: 0.07702654600143433\n",
      "Epoch 226 training loss: 0.0770338699221611\n",
      "Epoch 227 training loss: 0.07702809572219849\n",
      "Epoch 228 training loss: 0.0770164206624031\n",
      "Epoch 229 training loss: 0.07703149318695068\n",
      "Epoch 230 training loss: 0.07700888812541962\n",
      "Epoch 231 training loss: 0.07704063504934311\n",
      "Epoch 232 training loss: 0.07702436298131943\n",
      "Epoch 233 training loss: 0.07703782618045807\n",
      "Epoch 234 training loss: 0.07704062759876251\n",
      "Epoch 235 training loss: 0.07703305780887604\n",
      "Epoch 236 training loss: 0.07702900469303131\n",
      "Epoch 237 training loss: 0.07701142132282257\n",
      "Epoch 238 training loss: 0.07702313363552094\n",
      "Epoch 239 training loss: 0.07703787833452225\n",
      "Epoch 240 training loss: 0.07701330631971359\n",
      "Epoch 241 training loss: 0.07703802734613419\n",
      "Epoch 242 training loss: 0.07701446861028671\n",
      "Epoch 243 training loss: 0.07702381908893585\n",
      "Epoch 244 training loss: 0.07703627645969391\n",
      "Epoch 245 training loss: 0.07703565061092377\n",
      "Epoch 246 training loss: 0.0770408883690834\n",
      "Epoch 247 training loss: 0.07700607180595398\n",
      "Epoch 248 training loss: 0.07702819257974625\n",
      "Epoch 249 training loss: 0.07703020423650742\n",
      "Epoch 250 training loss: 0.07705322653055191\n",
      "Epoch 251 training loss: 0.07701022922992706\n",
      "Epoch 252 training loss: 0.07701870799064636\n",
      "Epoch 253 training loss: 0.07703284174203873\n",
      "Epoch 254 training loss: 0.07703067362308502\n",
      "Epoch 255 training loss: 0.07703249156475067\n",
      "Epoch 256 training loss: 0.07703877985477448\n",
      "0.6038787878787879\n",
      "[[1338  120]\n",
      " [1514 1153]]\n",
      "Accuracy: 0.6038787878787879\n",
      "Precision:  0.9057344854673999\n",
      "Recall:  0.432320959880015\n",
      "F1:  0.5852791878172589\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, window_size=8,\n",
    "             window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07081352174282074\n",
      "Epoch 2 training loss: 0.05177604779601097\n",
      "Epoch 3 training loss: 0.04481825605034828\n",
      "Epoch 4 training loss: 0.042032934725284576\n",
      "Epoch 5 training loss: 0.03975117206573486\n",
      "Epoch 6 training loss: 0.036157939583063126\n",
      "Epoch 7 training loss: 0.03292706608772278\n",
      "Epoch 8 training loss: 0.030977612361311913\n",
      "Epoch 9 training loss: 0.029723944142460823\n",
      "Epoch 10 training loss: 0.028918273746967316\n",
      "Epoch 11 training loss: 0.02801123820245266\n",
      "Epoch 12 training loss: 0.027473557740449905\n",
      "Epoch 13 training loss: 0.02687661163508892\n",
      "Epoch 14 training loss: 0.026449065655469894\n",
      "Epoch 15 training loss: 0.02608298324048519\n",
      "Epoch 16 training loss: 0.02545854263007641\n",
      "Epoch 17 training loss: 0.025329427793622017\n",
      "Epoch 18 training loss: 0.024881193414330482\n",
      "Epoch 19 training loss: 0.024671414867043495\n",
      "Epoch 20 training loss: 0.024505024775862694\n",
      "Epoch 21 training loss: 0.024299871176481247\n",
      "Epoch 22 training loss: 0.02397942915558815\n",
      "Epoch 23 training loss: 0.023837454617023468\n",
      "Epoch 24 training loss: 0.023697637021541595\n",
      "Epoch 25 training loss: 0.023405183106660843\n",
      "Epoch 26 training loss: 0.02320265583693981\n",
      "Epoch 27 training loss: 0.023119444027543068\n",
      "Epoch 28 training loss: 0.02296951785683632\n",
      "Epoch 29 training loss: 0.022724036127328873\n",
      "Epoch 30 training loss: 0.02265932783484459\n",
      "Epoch 31 training loss: 0.022531751543283463\n",
      "Epoch 32 training loss: 0.022266661748290062\n",
      "Epoch 33 training loss: 0.022111665457487106\n",
      "Epoch 34 training loss: 0.021976502612233162\n",
      "Epoch 35 training loss: 0.021871916949748993\n",
      "Epoch 36 training loss: 0.0215756855905056\n",
      "Epoch 37 training loss: 0.021504340693354607\n",
      "Epoch 38 training loss: 0.021440714597702026\n",
      "Epoch 39 training loss: 0.0213240385055542\n",
      "Epoch 40 training loss: 0.02123262733221054\n",
      "Epoch 41 training loss: 0.020975656807422638\n",
      "Epoch 42 training loss: 0.020951854065060616\n",
      "Epoch 43 training loss: 0.020735614001750946\n",
      "Epoch 44 training loss: 0.020570144057273865\n",
      "Epoch 45 training loss: 0.0205476526170969\n",
      "Epoch 46 training loss: 0.0204680897295475\n",
      "Epoch 47 training loss: 0.020305106416344643\n",
      "Epoch 48 training loss: 0.020412979647517204\n",
      "Epoch 49 training loss: 0.020313970744609833\n",
      "Epoch 50 training loss: 0.019998932257294655\n",
      "Epoch 51 training loss: 0.01995219476521015\n",
      "Epoch 52 training loss: 0.01997661218047142\n",
      "Epoch 53 training loss: 0.01973620429635048\n",
      "Epoch 54 training loss: 0.019737521186470985\n",
      "Epoch 55 training loss: 0.019749490544199944\n",
      "Epoch 56 training loss: 0.019465280696749687\n",
      "Epoch 57 training loss: 0.01959570124745369\n",
      "Epoch 58 training loss: 0.019428182393312454\n",
      "Epoch 59 training loss: 0.01931731030344963\n",
      "Epoch 60 training loss: 0.019147686660289764\n",
      "Epoch 61 training loss: 0.01914028264582157\n",
      "Epoch 62 training loss: 0.01894281432032585\n",
      "Epoch 63 training loss: 0.018742889165878296\n",
      "Epoch 64 training loss: 0.018763482570648193\n",
      "0.6392727272727273\n",
      "[[ 651  807]\n",
      " [ 681 1986]]\n",
      "Accuracy: 0.6392727272727273\n",
      "Precision:  0.7110633727175081\n",
      "Recall:  0.7446569178852643\n",
      "F1:  0.7274725274725274\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.1696965992450714\n",
      "Epoch 2 training loss: 0.16787873208522797\n",
      "Epoch 3 training loss: 0.1669291853904724\n",
      "Epoch 4 training loss: 0.1668277382850647\n",
      "Epoch 5 training loss: 0.16672559082508087\n",
      "Epoch 6 training loss: 0.16642449796199799\n",
      "Epoch 7 training loss: 0.16632813215255737\n",
      "Epoch 8 training loss: 0.1662880927324295\n",
      "Epoch 9 training loss: 0.1662030816078186\n",
      "Epoch 10 training loss: 0.16611048579216003\n",
      "Epoch 11 training loss: 0.1660909354686737\n",
      "Epoch 12 training loss: 0.1659357100725174\n",
      "Epoch 13 training loss: 0.16550952196121216\n",
      "Epoch 14 training loss: 0.1651533991098404\n",
      "Epoch 15 training loss: 0.16507366299629211\n",
      "Epoch 16 training loss: 0.1650806963443756\n",
      "Epoch 17 training loss: 0.1650279462337494\n",
      "Epoch 18 training loss: 0.16502952575683594\n",
      "Epoch 19 training loss: 0.16496992111206055\n",
      "Epoch 20 training loss: 0.1649792492389679\n",
      "Epoch 21 training loss: 0.1649915724992752\n",
      "Epoch 22 training loss: 0.16495685279369354\n",
      "Epoch 23 training loss: 0.16488680243492126\n",
      "Epoch 24 training loss: 0.16492563486099243\n",
      "Epoch 25 training loss: 0.16487309336662292\n",
      "Epoch 26 training loss: 0.16495220363140106\n",
      "Epoch 27 training loss: 0.1648828238248825\n",
      "Epoch 28 training loss: 0.1648811399936676\n",
      "Epoch 29 training loss: 0.16485434770584106\n",
      "Epoch 30 training loss: 0.1648762971162796\n",
      "Epoch 31 training loss: 0.16476258635520935\n",
      "Epoch 32 training loss: 0.16461719572544098\n",
      "Epoch 33 training loss: 0.16450954973697662\n",
      "Epoch 34 training loss: 0.164473295211792\n",
      "Epoch 35 training loss: 0.1645013988018036\n",
      "Epoch 36 training loss: 0.16450057923793793\n",
      "Epoch 37 training loss: 0.16447032988071442\n",
      "Epoch 38 training loss: 0.16444745659828186\n",
      "Epoch 39 training loss: 0.16443201899528503\n",
      "Epoch 40 training loss: 0.16440320014953613\n",
      "Epoch 41 training loss: 0.16439244151115417\n",
      "Epoch 42 training loss: 0.1643671691417694\n",
      "Epoch 43 training loss: 0.16440723836421967\n",
      "Epoch 44 training loss: 0.1643637865781784\n",
      "Epoch 45 training loss: 0.16430599987506866\n",
      "Epoch 46 training loss: 0.16434316337108612\n",
      "Epoch 47 training loss: 0.1643010675907135\n",
      "Epoch 48 training loss: 0.1642955094575882\n",
      "Epoch 49 training loss: 0.16434188187122345\n",
      "Epoch 50 training loss: 0.16427671909332275\n",
      "Epoch 51 training loss: 0.16429077088832855\n",
      "Epoch 52 training loss: 0.16422796249389648\n",
      "Epoch 53 training loss: 0.1642664074897766\n",
      "Epoch 54 training loss: 0.16420139372348785\n",
      "Epoch 55 training loss: 0.16420480608940125\n",
      "Epoch 56 training loss: 0.16417406499385834\n",
      "Epoch 57 training loss: 0.16414174437522888\n",
      "Epoch 58 training loss: 0.16417136788368225\n",
      "Epoch 59 training loss: 0.16415856778621674\n",
      "Epoch 60 training loss: 0.16417217254638672\n",
      "Epoch 61 training loss: 0.16416549682617188\n",
      "Epoch 62 training loss: 0.16421039402484894\n",
      "Epoch 63 training loss: 0.16415011882781982\n",
      "Epoch 64 training loss: 0.16410236060619354\n",
      "0.47393939393939394\n",
      "[[ 363 1095]\n",
      " [1075 1592]]\n",
      "Accuracy: 0.47393939393939394\n",
      "Precision:  0.5924823222925195\n",
      "Recall:  0.5969253843269592\n",
      "F1:  0.5946955547254389\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17117565870285034\n",
      "Epoch 2 training loss: 0.17089219391345978\n",
      "Epoch 3 training loss: 0.17088712751865387\n",
      "Epoch 4 training loss: 0.17088398337364197\n",
      "Epoch 5 training loss: 0.17059989273548126\n",
      "Epoch 6 training loss: 0.16920214891433716\n",
      "Epoch 7 training loss: 0.16828873753547668\n",
      "Epoch 8 training loss: 0.1681583672761917\n",
      "Epoch 9 training loss: 0.1680826097726822\n",
      "Epoch 10 training loss: 0.1680685132741928\n",
      "Epoch 11 training loss: 0.16803313791751862\n",
      "Epoch 12 training loss: 0.16800935566425323\n",
      "Epoch 13 training loss: 0.168057382106781\n",
      "Epoch 14 training loss: 0.1680016666650772\n",
      "Epoch 15 training loss: 0.16803009808063507\n",
      "Epoch 16 training loss: 0.16799914836883545\n",
      "Epoch 17 training loss: 0.16800348460674286\n",
      "Epoch 18 training loss: 0.16800744831562042\n",
      "Epoch 19 training loss: 0.16804924607276917\n",
      "Epoch 20 training loss: 0.16799481213092804\n",
      "Epoch 21 training loss: 0.16799065470695496\n",
      "Epoch 22 training loss: 0.1679990142583847\n",
      "Epoch 23 training loss: 0.1679784655570984\n",
      "Epoch 24 training loss: 0.16798534989356995\n",
      "Epoch 25 training loss: 0.16797421872615814\n",
      "Epoch 26 training loss: 0.16798251867294312\n",
      "Epoch 27 training loss: 0.16802871227264404\n",
      "Epoch 28 training loss: 0.1679912656545639\n",
      "Epoch 29 training loss: 0.16801121830940247\n",
      "Epoch 30 training loss: 0.16802790760993958\n",
      "Epoch 31 training loss: 0.1680411547422409\n",
      "Epoch 32 training loss: 0.16803418099880219\n",
      "Epoch 33 training loss: 0.16799606382846832\n",
      "Epoch 34 training loss: 0.16797992587089539\n",
      "Epoch 35 training loss: 0.16796360909938812\n",
      "Epoch 36 training loss: 0.16796182096004486\n",
      "Epoch 37 training loss: 0.16797949373722076\n",
      "Epoch 38 training loss: 0.16797710955142975\n",
      "Epoch 39 training loss: 0.1680811047554016\n",
      "Epoch 40 training loss: 0.16799592971801758\n",
      "Epoch 41 training loss: 0.16800850629806519\n",
      "Epoch 42 training loss: 0.168021097779274\n",
      "Epoch 43 training loss: 0.1679965853691101\n",
      "Epoch 44 training loss: 0.1680385172367096\n",
      "Epoch 45 training loss: 0.1679650992155075\n",
      "Epoch 46 training loss: 0.16801804304122925\n",
      "Epoch 47 training loss: 0.1679927110671997\n",
      "Epoch 48 training loss: 0.16796480119228363\n",
      "Epoch 49 training loss: 0.16796045005321503\n",
      "Epoch 50 training loss: 0.16797663271427155\n",
      "Epoch 51 training loss: 0.16806751489639282\n",
      "Epoch 52 training loss: 0.16799931228160858\n",
      "Epoch 53 training loss: 0.16803543269634247\n",
      "Epoch 54 training loss: 0.1680329442024231\n",
      "Epoch 55 training loss: 0.16805870831012726\n",
      "Epoch 56 training loss: 0.1680757999420166\n",
      "Epoch 57 training loss: 0.16799744963645935\n",
      "Epoch 58 training loss: 0.16798177361488342\n",
      "Epoch 59 training loss: 0.16800926625728607\n",
      "Epoch 60 training loss: 0.1679632067680359\n",
      "Epoch 61 training loss: 0.16794443130493164\n",
      "Epoch 62 training loss: 0.16792601346969604\n",
      "Epoch 63 training loss: 0.16794176399707794\n",
      "Epoch 64 training loss: 0.16794586181640625\n",
      "Epoch 65 training loss: 0.16797718405723572\n",
      "Epoch 66 training loss: 0.16795514523983002\n",
      "Epoch 67 training loss: 0.1679099053144455\n",
      "Epoch 68 training loss: 0.16792942583560944\n",
      "Epoch 69 training loss: 0.16796092689037323\n",
      "Epoch 70 training loss: 0.16789768636226654\n",
      "Epoch 71 training loss: 0.167667955160141\n",
      "Epoch 72 training loss: 0.16696929931640625\n",
      "Epoch 73 training loss: 0.16685323417186737\n",
      "Epoch 74 training loss: 0.1667962372303009\n",
      "Epoch 75 training loss: 0.16677238047122955\n",
      "Epoch 76 training loss: 0.16678297519683838\n",
      "Epoch 77 training loss: 0.16672664880752563\n",
      "Epoch 78 training loss: 0.16673199832439423\n",
      "Epoch 79 training loss: 0.16663001477718353\n",
      "Epoch 80 training loss: 0.1662663221359253\n",
      "Epoch 81 training loss: 0.1658490002155304\n",
      "Epoch 82 training loss: 0.16563370823860168\n",
      "Epoch 83 training loss: 0.16552095115184784\n",
      "Epoch 84 training loss: 0.1654587984085083\n",
      "Epoch 85 training loss: 0.16537614166736603\n",
      "Epoch 86 training loss: 0.16537095606327057\n",
      "Epoch 87 training loss: 0.16531004011631012\n",
      "Epoch 88 training loss: 0.16529624164104462\n",
      "Epoch 89 training loss: 0.16529308259487152\n",
      "Epoch 90 training loss: 0.16527557373046875\n",
      "Epoch 91 training loss: 0.1652599573135376\n",
      "Epoch 92 training loss: 0.1651657074689865\n",
      "Epoch 93 training loss: 0.1652131825685501\n",
      "Epoch 94 training loss: 0.16520553827285767\n",
      "Epoch 95 training loss: 0.1651611328125\n",
      "Epoch 96 training loss: 0.1651621311903\n",
      "Epoch 97 training loss: 0.16519345343112946\n",
      "Epoch 98 training loss: 0.16517382860183716\n",
      "Epoch 99 training loss: 0.16509738564491272\n",
      "Epoch 100 training loss: 0.16507796943187714\n",
      "Epoch 101 training loss: 0.1651117503643036\n",
      "Epoch 102 training loss: 0.165104478597641\n",
      "Epoch 103 training loss: 0.16516909003257751\n",
      "Epoch 104 training loss: 0.16508862376213074\n",
      "Epoch 105 training loss: 0.16511014103889465\n",
      "Epoch 106 training loss: 0.16507498919963837\n",
      "Epoch 107 training loss: 0.16505128145217896\n",
      "Epoch 108 training loss: 0.16505971550941467\n",
      "Epoch 109 training loss: 0.1649939864873886\n",
      "Epoch 110 training loss: 0.16499775648117065\n",
      "Epoch 111 training loss: 0.16497525572776794\n",
      "Epoch 112 training loss: 0.16498683393001556\n",
      "Epoch 113 training loss: 0.1649920642375946\n",
      "Epoch 114 training loss: 0.1650433987379074\n",
      "Epoch 115 training loss: 0.16500139236450195\n",
      "Epoch 116 training loss: 0.1649690866470337\n",
      "Epoch 117 training loss: 0.1650148183107376\n",
      "Epoch 118 training loss: 0.16497363150119781\n",
      "Epoch 119 training loss: 0.16497468948364258\n",
      "Epoch 120 training loss: 0.1649494469165802\n",
      "Epoch 121 training loss: 0.16495713591575623\n",
      "Epoch 122 training loss: 0.16515107452869415\n",
      "Epoch 123 training loss: 0.1649789661169052\n",
      "Epoch 124 training loss: 0.16495779156684875\n",
      "Epoch 125 training loss: 0.1649857759475708\n",
      "Epoch 126 training loss: 0.16492027044296265\n",
      "Epoch 127 training loss: 0.1649385690689087\n",
      "Epoch 128 training loss: 0.16495110094547272\n",
      "Epoch 129 training loss: 0.1649756282567978\n",
      "Epoch 130 training loss: 0.16493456065654755\n",
      "Epoch 131 training loss: 0.1649821400642395\n",
      "Epoch 132 training loss: 0.16498489677906036\n",
      "Epoch 133 training loss: 0.164930060505867\n",
      "Epoch 134 training loss: 0.16489359736442566\n",
      "Epoch 135 training loss: 0.16491061449050903\n",
      "Epoch 136 training loss: 0.1649249941110611\n",
      "Epoch 137 training loss: 0.1650056391954422\n",
      "Epoch 138 training loss: 0.16496941447257996\n",
      "Epoch 139 training loss: 0.16494052112102509\n",
      "Epoch 140 training loss: 0.16495178639888763\n",
      "Epoch 141 training loss: 0.16487109661102295\n",
      "Epoch 142 training loss: 0.1648498773574829\n",
      "Epoch 143 training loss: 0.16483591496944427\n",
      "Epoch 144 training loss: 0.16483935713768005\n",
      "Epoch 145 training loss: 0.1648600548505783\n",
      "Epoch 146 training loss: 0.16486141085624695\n",
      "Epoch 147 training loss: 0.16491375863552094\n",
      "Epoch 148 training loss: 0.16492648422718048\n",
      "Epoch 149 training loss: 0.16493812203407288\n",
      "Epoch 150 training loss: 0.16490525007247925\n",
      "Epoch 151 training loss: 0.16487710177898407\n",
      "Epoch 152 training loss: 0.16487674415111542\n",
      "Epoch 153 training loss: 0.16483594477176666\n",
      "Epoch 154 training loss: 0.16483083367347717\n",
      "Epoch 155 training loss: 0.16483162343502045\n",
      "Epoch 156 training loss: 0.16494901478290558\n",
      "Epoch 157 training loss: 0.1650901883840561\n",
      "Epoch 158 training loss: 0.16496798396110535\n",
      "Epoch 159 training loss: 0.16495105624198914\n",
      "Epoch 160 training loss: 0.16491471230983734\n",
      "Epoch 161 training loss: 0.16486495733261108\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.16493481397628784\n",
      "Epoch 164 training loss: 0.1648513674736023\n",
      "Epoch 165 training loss: 0.16486524045467377\n",
      "Epoch 166 training loss: 0.16497018933296204\n",
      "Epoch 167 training loss: 0.16491641104221344\n",
      "Epoch 168 training loss: 0.16486111283302307\n",
      "Epoch 169 training loss: 0.164909228682518\n",
      "Epoch 170 training loss: 0.1649133265018463\n",
      "Epoch 171 training loss: 0.16487690806388855\n",
      "Epoch 172 training loss: 0.16485801339149475\n",
      "Epoch 173 training loss: 0.16486766934394836\n",
      "Epoch 174 training loss: 0.16484883427619934\n",
      "Epoch 175 training loss: 0.16482563316822052\n",
      "Epoch 176 training loss: 0.1648404896259308\n",
      "Epoch 177 training loss: 0.16486871242523193\n",
      "Epoch 178 training loss: 0.1648487150669098\n",
      "Epoch 179 training loss: 0.1648281067609787\n",
      "Epoch 180 training loss: 0.16482169926166534\n",
      "Epoch 181 training loss: 0.1648256778717041\n",
      "Epoch 182 training loss: 0.16484518349170685\n",
      "Epoch 183 training loss: 0.16496612131595612\n",
      "Epoch 184 training loss: 0.1648326814174652\n",
      "Epoch 185 training loss: 0.164798766374588\n",
      "Epoch 186 training loss: 0.164801687002182\n",
      "Epoch 187 training loss: 0.16480009257793427\n",
      "Epoch 188 training loss: 0.16483540832996368\n",
      "Epoch 189 training loss: 0.1649075597524643\n",
      "Epoch 190 training loss: 0.16480576992034912\n",
      "Epoch 191 training loss: 0.1647839993238449\n",
      "Epoch 192 training loss: 0.1648356318473816\n",
      "Epoch 193 training loss: 0.16480454802513123\n",
      "Epoch 194 training loss: 0.1647886484861374\n",
      "Epoch 195 training loss: 0.16479100286960602\n",
      "Epoch 196 training loss: 0.16481254994869232\n",
      "Epoch 197 training loss: 0.1648106724023819\n",
      "Epoch 198 training loss: 0.1648610383272171\n",
      "Epoch 199 training loss: 0.16488100588321686\n",
      "Epoch 200 training loss: 0.1648060381412506\n",
      "Epoch 201 training loss: 0.16479626297950745\n",
      "Epoch 202 training loss: 0.16479703783988953\n",
      "Epoch 203 training loss: 0.1648780256509781\n",
      "Epoch 204 training loss: 0.16486670076847076\n",
      "Epoch 205 training loss: 0.16480804979801178\n",
      "Epoch 206 training loss: 0.16483813524246216\n",
      "Epoch 207 training loss: 0.1648465096950531\n",
      "Epoch 208 training loss: 0.1648363471031189\n",
      "Epoch 209 training loss: 0.164854034781456\n",
      "Epoch 210 training loss: 0.1648809313774109\n",
      "Epoch 211 training loss: 0.1649470031261444\n",
      "Epoch 212 training loss: 0.1649084836244583\n",
      "Epoch 213 training loss: 0.16496233642101288\n",
      "Epoch 214 training loss: 0.16488410532474518\n",
      "Epoch 215 training loss: 0.16484808921813965\n",
      "Epoch 216 training loss: 0.1648230254650116\n",
      "Epoch 217 training loss: 0.16481532156467438\n",
      "Epoch 218 training loss: 0.16482269763946533\n",
      "Epoch 219 training loss: 0.16485249996185303\n",
      "Epoch 220 training loss: 0.16485776007175446\n",
      "Epoch 221 training loss: 0.16479940712451935\n",
      "Epoch 222 training loss: 0.1647922694683075\n",
      "Epoch 223 training loss: 0.16481277346611023\n",
      "Epoch 224 training loss: 0.1648196280002594\n",
      "Epoch 225 training loss: 0.1648918092250824\n",
      "Epoch 226 training loss: 0.16487829387187958\n",
      "Epoch 227 training loss: 0.16488078236579895\n",
      "Epoch 228 training loss: 0.164849191904068\n",
      "Epoch 229 training loss: 0.16477710008621216\n",
      "Epoch 230 training loss: 0.16477271914482117\n",
      "Epoch 231 training loss: 0.16476401686668396\n",
      "Epoch 232 training loss: 0.1647741198539734\n",
      "Epoch 233 training loss: 0.1648848056793213\n",
      "Epoch 234 training loss: 0.16478723287582397\n",
      "Epoch 235 training loss: 0.16479089856147766\n",
      "Epoch 236 training loss: 0.16477440297603607\n",
      "Epoch 237 training loss: 0.16477873921394348\n",
      "Epoch 238 training loss: 0.164784237742424\n",
      "Epoch 239 training loss: 0.16476275026798248\n",
      "Epoch 240 training loss: 0.16477172076702118\n",
      "Epoch 241 training loss: 0.16475346684455872\n",
      "Epoch 242 training loss: 0.1647641658782959\n",
      "Epoch 243 training loss: 0.16478458046913147\n",
      "Epoch 244 training loss: 0.16474385559558868\n",
      "Epoch 245 training loss: 0.16473190486431122\n",
      "Epoch 246 training loss: 0.16472983360290527\n",
      "Epoch 247 training loss: 0.164745032787323\n",
      "Epoch 248 training loss: 0.16475629806518555\n",
      "Epoch 249 training loss: 0.16477416455745697\n",
      "Epoch 250 training loss: 0.16477340459823608\n",
      "Epoch 251 training loss: 0.16476497054100037\n",
      "Epoch 252 training loss: 0.1647583246231079\n",
      "Epoch 253 training loss: 0.1647467464208603\n",
      "Epoch 254 training loss: 0.16475403308868408\n",
      "Epoch 255 training loss: 0.16489911079406738\n",
      "Epoch 256 training loss: 0.1648680567741394\n",
      "0.48218181818181816\n",
      "[[ 392 1066]\n",
      " [1070 1597]]\n",
      "Accuracy: 0.48218181818181816\n",
      "Precision:  0.5996995869320315\n",
      "Recall:  0.5988001499812523\n",
      "F1:  0.5992495309568481\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.Tanh(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandru-sebastian-nechita/UNI/SP/multi-modal-edge-ai/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.17048297822475433\n",
      "Epoch 2 training loss: 0.16889290511608124\n",
      "Epoch 3 training loss: 0.16845721006393433\n",
      "Epoch 4 training loss: 0.16830764710903168\n",
      "Epoch 5 training loss: 0.16820263862609863\n",
      "Epoch 6 training loss: 0.16814833879470825\n",
      "Epoch 7 training loss: 0.16813574731349945\n",
      "Epoch 8 training loss: 0.16808335483074188\n",
      "Epoch 9 training loss: 0.16807159781455994\n",
      "Epoch 10 training loss: 0.16805912554264069\n",
      "Epoch 11 training loss: 0.16805769503116608\n",
      "Epoch 12 training loss: 0.16806276142597198\n",
      "Epoch 13 training loss: 0.1680343598127365\n",
      "Epoch 14 training loss: 0.1680484414100647\n",
      "Epoch 15 training loss: 0.16805490851402283\n",
      "Epoch 16 training loss: 0.16802315413951874\n",
      "Epoch 17 training loss: 0.1680458039045334\n",
      "Epoch 18 training loss: 0.16803789138793945\n",
      "Epoch 19 training loss: 0.16801948845386505\n",
      "Epoch 20 training loss: 0.16804732382297516\n",
      "Epoch 21 training loss: 0.1680135428905487\n",
      "Epoch 22 training loss: 0.16801586747169495\n",
      "Epoch 23 training loss: 0.16801030933856964\n",
      "Epoch 24 training loss: 0.16802343726158142\n",
      "Epoch 25 training loss: 0.16806061565876007\n",
      "Epoch 26 training loss: 0.1680029332637787\n",
      "Epoch 27 training loss: 0.16800610721111298\n",
      "Epoch 28 training loss: 0.16800940036773682\n",
      "Epoch 29 training loss: 0.16805070638656616\n",
      "Epoch 30 training loss: 0.16801466047763824\n",
      "Epoch 31 training loss: 0.16801109910011292\n",
      "Epoch 32 training loss: 0.16799896955490112\n",
      "Epoch 33 training loss: 0.1680048555135727\n",
      "Epoch 34 training loss: 0.16801267862319946\n",
      "Epoch 35 training loss: 0.16798287630081177\n",
      "Epoch 36 training loss: 0.16804930567741394\n",
      "Epoch 37 training loss: 0.16799293458461761\n",
      "Epoch 38 training loss: 0.16798101365566254\n",
      "Epoch 39 training loss: 0.1677250862121582\n",
      "Epoch 40 training loss: 0.166952446103096\n",
      "Epoch 41 training loss: 0.16678158938884735\n",
      "Epoch 42 training loss: 0.16661563515663147\n",
      "Epoch 43 training loss: 0.16582980751991272\n",
      "Epoch 44 training loss: 0.16569674015045166\n",
      "Epoch 45 training loss: 0.16553372144699097\n",
      "Epoch 46 training loss: 0.1655174195766449\n",
      "Epoch 47 training loss: 0.16547071933746338\n",
      "Epoch 48 training loss: 0.16548818349838257\n",
      "Epoch 49 training loss: 0.16539238393306732\n",
      "Epoch 50 training loss: 0.16536274552345276\n",
      "Epoch 51 training loss: 0.1654076725244522\n",
      "Epoch 52 training loss: 0.16531293094158173\n",
      "Epoch 53 training loss: 0.16527575254440308\n",
      "Epoch 54 training loss: 0.16525791585445404\n",
      "Epoch 55 training loss: 0.16516117751598358\n",
      "Epoch 56 training loss: 0.1651705503463745\n",
      "Epoch 57 training loss: 0.16516275703907013\n",
      "Epoch 58 training loss: 0.1651269644498825\n",
      "Epoch 59 training loss: 0.16513550281524658\n",
      "Epoch 60 training loss: 0.16511771082878113\n",
      "Epoch 61 training loss: 0.16506704688072205\n",
      "Epoch 62 training loss: 0.1650332510471344\n",
      "Epoch 63 training loss: 0.16504237055778503\n",
      "Epoch 64 training loss: 0.1650545299053192\n",
      "Epoch 65 training loss: 0.16508206725120544\n",
      "Epoch 66 training loss: 0.1649974286556244\n",
      "Epoch 67 training loss: 0.16497361660003662\n",
      "Epoch 68 training loss: 0.1649814248085022\n",
      "Epoch 69 training loss: 0.16504935920238495\n",
      "Epoch 70 training loss: 0.16501019895076752\n",
      "Epoch 71 training loss: 0.1649552583694458\n",
      "Epoch 72 training loss: 0.1650993376970291\n",
      "Epoch 73 training loss: 0.16508223116397858\n",
      "Epoch 74 training loss: 0.16517047584056854\n",
      "Epoch 75 training loss: 0.16521300375461578\n",
      "Epoch 76 training loss: 0.1650741696357727\n",
      "Epoch 77 training loss: 0.16509033739566803\n",
      "Epoch 78 training loss: 0.16508528590202332\n",
      "Epoch 79 training loss: 0.16500599682331085\n",
      "Epoch 80 training loss: 0.16493317484855652\n",
      "Epoch 81 training loss: 0.16506394743919373\n",
      "Epoch 82 training loss: 0.16497287154197693\n",
      "Epoch 83 training loss: 0.16493584215641022\n",
      "Epoch 84 training loss: 0.16493527591228485\n",
      "Epoch 85 training loss: 0.1649501770734787\n",
      "Epoch 86 training loss: 0.16490690410137177\n",
      "Epoch 87 training loss: 0.16495487093925476\n",
      "Epoch 88 training loss: 0.16495370864868164\n",
      "Epoch 89 training loss: 0.16503620147705078\n",
      "Epoch 90 training loss: 0.16493797302246094\n",
      "Epoch 91 training loss: 0.16498427093029022\n",
      "Epoch 92 training loss: 0.16493545472621918\n",
      "Epoch 93 training loss: 0.1649208962917328\n",
      "Epoch 94 training loss: 0.16495338082313538\n",
      "Epoch 95 training loss: 0.1649291068315506\n",
      "Epoch 96 training loss: 0.16492561995983124\n",
      "Epoch 97 training loss: 0.1648913323879242\n",
      "Epoch 98 training loss: 0.1649065613746643\n",
      "Epoch 99 training loss: 0.16493679583072662\n",
      "Epoch 100 training loss: 0.1649480015039444\n",
      "Epoch 101 training loss: 0.1650744378566742\n",
      "Epoch 102 training loss: 0.16502448916435242\n",
      "Epoch 103 training loss: 0.16501517593860626\n",
      "Epoch 104 training loss: 0.16496840119361877\n",
      "Epoch 105 training loss: 0.1649671494960785\n",
      "Epoch 106 training loss: 0.1649441123008728\n",
      "Epoch 107 training loss: 0.16488738358020782\n",
      "Epoch 108 training loss: 0.16491222381591797\n",
      "Epoch 109 training loss: 0.16490551829338074\n",
      "Epoch 110 training loss: 0.1649019420146942\n",
      "Epoch 111 training loss: 0.16496579349040985\n",
      "Epoch 112 training loss: 0.16494669020175934\n",
      "Epoch 113 training loss: 0.16490943729877472\n",
      "Epoch 114 training loss: 0.1649254858493805\n",
      "Epoch 115 training loss: 0.1649320423603058\n",
      "Epoch 116 training loss: 0.16492733359336853\n",
      "Epoch 117 training loss: 0.1649169772863388\n",
      "Epoch 118 training loss: 0.16515254974365234\n",
      "Epoch 119 training loss: 0.16511186957359314\n",
      "Epoch 120 training loss: 0.1650628000497818\n",
      "Epoch 121 training loss: 0.16499508917331696\n",
      "Epoch 122 training loss: 0.16491152346134186\n",
      "Epoch 123 training loss: 0.16504086554050446\n",
      "Epoch 124 training loss: 0.16507422924041748\n",
      "Epoch 125 training loss: 0.165010467171669\n",
      "Epoch 126 training loss: 0.16493843495845795\n",
      "Epoch 127 training loss: 0.16491617262363434\n",
      "Epoch 128 training loss: 0.1649228185415268\n",
      "Epoch 129 training loss: 0.1649019718170166\n",
      "Epoch 130 training loss: 0.16487954556941986\n",
      "Epoch 131 training loss: 0.16495588421821594\n",
      "Epoch 132 training loss: 0.1649574339389801\n",
      "Epoch 133 training loss: 0.16490253806114197\n",
      "Epoch 134 training loss: 0.1649157702922821\n",
      "Epoch 135 training loss: 0.16493511199951172\n",
      "Epoch 136 training loss: 0.16488772630691528\n",
      "Epoch 137 training loss: 0.16485793888568878\n",
      "Epoch 138 training loss: 0.1648665815591812\n",
      "Epoch 139 training loss: 0.16485625505447388\n",
      "Epoch 140 training loss: 0.16496773064136505\n",
      "Epoch 141 training loss: 0.165019229054451\n",
      "Epoch 142 training loss: 0.16496799886226654\n",
      "Epoch 143 training loss: 0.1649271547794342\n",
      "Epoch 144 training loss: 0.1649191528558731\n",
      "Epoch 145 training loss: 0.16488845646381378\n",
      "Epoch 146 training loss: 0.16489775478839874\n",
      "Epoch 147 training loss: 0.1648975908756256\n",
      "Epoch 148 training loss: 0.16485780477523804\n",
      "Epoch 149 training loss: 0.16485758125782013\n",
      "Epoch 150 training loss: 0.16484779119491577\n",
      "Epoch 151 training loss: 0.16487419605255127\n",
      "Epoch 152 training loss: 0.16494746506214142\n",
      "Epoch 153 training loss: 0.16492073237895966\n",
      "Epoch 154 training loss: 0.1648574322462082\n",
      "Epoch 155 training loss: 0.1648692786693573\n",
      "Epoch 156 training loss: 0.16485993564128876\n",
      "Epoch 157 training loss: 0.1649700254201889\n",
      "Epoch 158 training loss: 0.16499640047550201\n",
      "Epoch 159 training loss: 0.16493870317935944\n",
      "Epoch 160 training loss: 0.16493374109268188\n",
      "Epoch 161 training loss: 0.1649080067873001\n",
      "Epoch 162 training loss: 0.16491413116455078\n",
      "Epoch 163 training loss: 0.1648789644241333\n",
      "Epoch 164 training loss: 0.16489528119564056\n",
      "Epoch 165 training loss: 0.16487745940685272\n",
      "Epoch 166 training loss: 0.1648242473602295\n",
      "Epoch 167 training loss: 0.1648487150669098\n",
      "Epoch 168 training loss: 0.1648620218038559\n",
      "Epoch 169 training loss: 0.1648893803358078\n",
      "Epoch 170 training loss: 0.1648567169904709\n",
      "Epoch 171 training loss: 0.16489006578922272\n",
      "Epoch 172 training loss: 0.16485270857810974\n",
      "Epoch 173 training loss: 0.16487672924995422\n",
      "Epoch 174 training loss: 0.16488471627235413\n",
      "Epoch 175 training loss: 0.16484229266643524\n",
      "Epoch 176 training loss: 0.16493725776672363\n",
      "Epoch 177 training loss: 0.1649102121591568\n",
      "Epoch 178 training loss: 0.16487260162830353\n",
      "Epoch 179 training loss: 0.16490183770656586\n",
      "Epoch 180 training loss: 0.16484715044498444\n",
      "Epoch 181 training loss: 0.16485095024108887\n",
      "Epoch 182 training loss: 0.16485632956027985\n",
      "Epoch 183 training loss: 0.16485297679901123\n",
      "Epoch 184 training loss: 0.16485093533992767\n",
      "Epoch 185 training loss: 0.16483157873153687\n",
      "Epoch 186 training loss: 0.16496655344963074\n",
      "Epoch 187 training loss: 0.16495396196842194\n",
      "Epoch 188 training loss: 0.16494569182395935\n",
      "Epoch 189 training loss: 0.16491083800792694\n",
      "Epoch 190 training loss: 0.1649261862039566\n",
      "Epoch 191 training loss: 0.16509567201137543\n",
      "Epoch 192 training loss: 0.1650398075580597\n",
      "Epoch 193 training loss: 0.16508892178535461\n",
      "Epoch 194 training loss: 0.1649836301803589\n",
      "Epoch 195 training loss: 0.1649351865053177\n",
      "Epoch 196 training loss: 0.16492505371570587\n",
      "Epoch 197 training loss: 0.16490179300308228\n",
      "Epoch 198 training loss: 0.16493715345859528\n",
      "Epoch 199 training loss: 0.16490860283374786\n",
      "Epoch 200 training loss: 0.16489098966121674\n",
      "Epoch 201 training loss: 0.16490168869495392\n",
      "Epoch 202 training loss: 0.16491514444351196\n",
      "Epoch 203 training loss: 0.16485656797885895\n",
      "Epoch 204 training loss: 0.16490283608436584\n",
      "Epoch 205 training loss: 0.164883553981781\n",
      "Epoch 206 training loss: 0.16486819088459015\n",
      "Epoch 207 training loss: 0.1648823767900467\n",
      "Epoch 208 training loss: 0.16494770348072052\n",
      "Epoch 209 training loss: 0.1649188995361328\n",
      "Epoch 210 training loss: 0.16486652195453644\n",
      "Epoch 211 training loss: 0.164960578083992\n",
      "Epoch 212 training loss: 0.16485953330993652\n",
      "Epoch 213 training loss: 0.16483987867832184\n",
      "Epoch 214 training loss: 0.1648828238248825\n",
      "Epoch 215 training loss: 0.16494596004486084\n",
      "Epoch 216 training loss: 0.16491232812404633\n",
      "Epoch 217 training loss: 0.16492034494876862\n",
      "Epoch 218 training loss: 0.16492900252342224\n",
      "Epoch 219 training loss: 0.16489380598068237\n",
      "Epoch 220 training loss: 0.16489483416080475\n",
      "Epoch 221 training loss: 0.16490140557289124\n",
      "Epoch 222 training loss: 0.16484548151493073\n",
      "Epoch 223 training loss: 0.16491907835006714\n",
      "Epoch 224 training loss: 0.16493640840053558\n",
      "Epoch 225 training loss: 0.164934903383255\n",
      "Epoch 226 training loss: 0.1648758202791214\n",
      "Epoch 227 training loss: 0.16483473777770996\n",
      "Epoch 228 training loss: 0.1648281216621399\n",
      "Epoch 229 training loss: 0.16489090025424957\n",
      "Epoch 230 training loss: 0.1648705154657364\n",
      "Epoch 231 training loss: 0.1648814082145691\n",
      "Epoch 232 training loss: 0.16482363641262054\n",
      "Epoch 233 training loss: 0.1649101823568344\n",
      "Epoch 234 training loss: 0.16488051414489746\n",
      "Epoch 235 training loss: 0.16489385068416595\n",
      "Epoch 236 training loss: 0.16489002108573914\n",
      "Epoch 237 training loss: 0.16491107642650604\n",
      "Epoch 238 training loss: 0.16488583385944366\n",
      "Epoch 239 training loss: 0.1649440973997116\n",
      "Epoch 240 training loss: 0.16485491394996643\n",
      "Epoch 241 training loss: 0.1648317128419876\n",
      "Epoch 242 training loss: 0.16481254994869232\n",
      "Epoch 243 training loss: 0.1648099273443222\n",
      "Epoch 244 training loss: 0.16481496393680573\n",
      "Epoch 245 training loss: 0.16482143104076385\n",
      "Epoch 246 training loss: 0.1648457646369934\n",
      "Epoch 247 training loss: 0.16480199992656708\n",
      "Epoch 248 training loss: 0.16478675603866577\n",
      "Epoch 249 training loss: 0.16480021178722382\n",
      "Epoch 250 training loss: 0.16483859717845917\n",
      "Epoch 251 training loss: 0.16479599475860596\n",
      "Epoch 252 training loss: 0.164817675948143\n",
      "Epoch 253 training loss: 0.16481716930866241\n",
      "Epoch 254 training loss: 0.16489620506763458\n",
      "Epoch 255 training loss: 0.16483238339424133\n",
      "Epoch 256 training loss: 0.1648678183555603\n",
      "0.49066666666666664\n",
      "[[ 359 1099]\n",
      " [1002 1665]]\n",
      "Accuracy: 0.49066666666666664\n",
      "Precision:  0.6023878437047757\n",
      "Recall:  0.6242969628796401\n",
      "F1:  0.6131467501380962\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=False\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU6(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Softmax())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.01, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(5, 8, 8, 16, 16)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=False)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search for Isolation Forest\n",
    "\n",
    "iforest = IForest()\n",
    "\n",
    "n_estimators = [1000,1200,1600,2000,2400,2800,3200]\n",
    "onehots = [True, False]\n",
    "for n in n_estimators:\n",
    "        for o in onehots:\n",
    "            iforestParams = {\n",
    "                \"n_estimators\": n,\n",
    "                \"max_samples\": \"auto\",\n",
    "                \"contamination\": \"auto\",\n",
    "                \"max_features\": 1.0,\n",
    "                \"n_jobs\": -1,\n",
    "                \"random_state\": None,\n",
    "                \"verbose\": False,\n",
    "            }\n",
    "\n",
    "            hp = Hparams(i_forest_hparams=iforestParams, anomaly_generation_ratio = 0.01, clean_test_data_ratio = 0.25, window_size= 8, window_slide= 1,one_hot=o)\n",
    "            (avg, cm) = model_train_eval(iforest, data, hp)\n",
    "            print(\"for n_estimators: \", n, \" bootstrap: \", b)\n",
    "            print(avg)\n",
    "            print(cm)\n",
    "            (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "            print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "            print(\"Precision: \", tp / (tp + fp))\n",
    "            print(\"Recall: \", tp / (tp + fn))\n",
    "            print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "            print(\"---------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with poly with anomalies being 50% of the data and one_hot=False\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=10,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=False)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid search OCSVM with rbf with anomalies being 10% of the data and one_hot=True\n",
    "\n",
    "ocsvm = OCSVM()\n",
    "\n",
    "degrees = [6, 5, 4, 3]\n",
    "gammas = [\"scale\", \"auto\"]\n",
    "coef0s = [0.0, 0.1, 0.2, 0.3]\n",
    "tols = [0.001, 0.0001]\n",
    "\n",
    "for d in degrees:\n",
    "    for g in gammas:\n",
    "        for c in coef0s:\n",
    "            for t in tols:\n",
    "                ovscmParams = {\n",
    "                    \"kernel\": \"poly\",\n",
    "                    \"degree\": d,\n",
    "                    \"gamma\": g,\n",
    "                    \"coef0\": c,\n",
    "                    \"tol\": t,\n",
    "                    \"nu\": 0.001,\n",
    "                    \"shrinking\": True,\n",
    "                    \"cache_size\": 3200,\n",
    "                    \"verbose\": False,\n",
    "                    \"max_iter\": -1,\n",
    "                }\n",
    "\n",
    "                hp = Hparams(ocsvm_hparams=ovscmParams, anomaly_generation_ratio=0.01,\n",
    "                             clean_test_data_ratio=0.25, window_size=8, window_slide=1,\n",
    "                             one_hot=True)\n",
    "                (avg, cm) = model_train_eval(ocsvm, data, hp)\n",
    "                print(\"for degree: \", d, \" gamma: \", g, \" coef0: \", c, \" tol: \", t)\n",
    "                print(avg)\n",
    "                print(cm)\n",
    "                (tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "                print(\"Accuracy: \", (tp + tn) / (tp + tn + fp + fn))\n",
    "                print(\"Precision: \", tp / (tp + fp))\n",
    "                print(\"Recall: \", tp / (tp + fn))\n",
    "                print(\"F1: \", 2 * tp / (2 * tp + fp + fn))\n",
    "                print(\"---------------------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ltsm = LSTMAutoencoder(12, 8, 16, 32, 32)\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, clean_test_data_ratio=0.2, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "\n",
    "(avg, cm) = model_train_eval(ltsm, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06971529126167297\n",
      "Epoch 2 training loss: 0.04786883294582367\n",
      "Epoch 3 training loss: 0.04360324516892433\n",
      "Epoch 4 training loss: 0.04194796457886696\n",
      "Epoch 5 training loss: 0.040111180394887924\n",
      "Epoch 6 training loss: 0.03824681043624878\n",
      "Epoch 7 training loss: 0.03677377104759216\n",
      "Epoch 8 training loss: 0.03536611422896385\n",
      "Epoch 9 training loss: 0.03354225680232048\n",
      "Epoch 10 training loss: 0.03137403726577759\n",
      "Epoch 11 training loss: 0.029583103954792023\n",
      "Epoch 12 training loss: 0.028284654021263123\n",
      "Epoch 13 training loss: 0.02725023590028286\n",
      "Epoch 14 training loss: 0.026495160534977913\n",
      "Epoch 15 training loss: 0.026067033410072327\n",
      "Epoch 16 training loss: 0.02549590729176998\n",
      "Epoch 17 training loss: 0.024979382753372192\n",
      "Epoch 18 training loss: 0.024581756442785263\n",
      "Epoch 19 training loss: 0.024252163246273994\n",
      "Epoch 20 training loss: 0.023938970640301704\n",
      "Epoch 21 training loss: 0.0234003197401762\n",
      "Epoch 22 training loss: 0.023160770535469055\n",
      "Epoch 23 training loss: 0.022858845070004463\n",
      "Epoch 24 training loss: 0.02254897728562355\n",
      "Epoch 25 training loss: 0.02224528230726719\n",
      "Epoch 26 training loss: 0.022055665031075478\n",
      "Epoch 27 training loss: 0.021860601380467415\n",
      "Epoch 28 training loss: 0.021508967503905296\n",
      "Epoch 29 training loss: 0.02132820524275303\n",
      "Epoch 30 training loss: 0.021300602704286575\n",
      "Epoch 31 training loss: 0.020873505622148514\n",
      "Epoch 32 training loss: 0.02075488120317459\n",
      "Epoch 33 training loss: 0.020622408017516136\n",
      "Epoch 34 training loss: 0.020474329590797424\n",
      "Epoch 35 training loss: 0.020251190289855003\n",
      "Epoch 36 training loss: 0.02016971819102764\n",
      "Epoch 37 training loss: 0.020298535004258156\n",
      "Epoch 38 training loss: 0.019755329936742783\n",
      "Epoch 39 training loss: 0.019767414778470993\n",
      "Epoch 40 training loss: 0.019633740186691284\n",
      "Epoch 41 training loss: 0.0194877739995718\n",
      "Epoch 42 training loss: 0.01956487074494362\n",
      "Epoch 43 training loss: 0.019184475764632225\n",
      "Epoch 44 training loss: 0.019263993948698044\n",
      "Epoch 45 training loss: 0.01911487616598606\n",
      "Epoch 46 training loss: 0.019197430461645126\n",
      "Epoch 47 training loss: 0.01920635998249054\n",
      "Epoch 48 training loss: 0.018751351162791252\n",
      "Epoch 49 training loss: 0.01861399970948696\n",
      "Epoch 50 training loss: 0.018730437383055687\n",
      "Epoch 51 training loss: 0.0187191441655159\n",
      "Epoch 52 training loss: 0.01851801574230194\n",
      "Epoch 53 training loss: 0.01863197237253189\n",
      "Epoch 54 training loss: 0.018345829099416733\n",
      "Epoch 55 training loss: 0.018362069502472878\n",
      "Epoch 56 training loss: 0.018251044675707817\n",
      "Epoch 57 training loss: 0.01821674406528473\n",
      "Epoch 58 training loss: 0.018235333263874054\n",
      "Epoch 59 training loss: 0.018105700612068176\n",
      "Epoch 60 training loss: 0.018001500517129898\n",
      "Epoch 61 training loss: 0.01796252653002739\n",
      "Epoch 62 training loss: 0.018044082447886467\n",
      "Epoch 63 training loss: 0.017860349267721176\n",
      "Epoch 64 training loss: 0.01774909906089306\n",
      "Epoch 65 training loss: 0.017950033769011497\n",
      "Epoch 66 training loss: 0.017742974683642387\n",
      "Epoch 67 training loss: 0.017618278041481972\n",
      "Epoch 68 training loss: 0.017749140039086342\n",
      "Epoch 69 training loss: 0.017547868192195892\n",
      "Epoch 70 training loss: 0.017704833298921585\n",
      "Epoch 71 training loss: 0.017376607283949852\n",
      "Epoch 72 training loss: 0.01747717894613743\n",
      "Epoch 73 training loss: 0.01743854209780693\n",
      "Epoch 74 training loss: 0.017357533797621727\n",
      "Epoch 75 training loss: 0.0173368901014328\n",
      "Epoch 76 training loss: 0.017211036756634712\n",
      "Epoch 77 training loss: 0.0172377098351717\n",
      "Epoch 78 training loss: 0.017185280099511147\n",
      "Epoch 79 training loss: 0.017232786864042282\n",
      "Epoch 80 training loss: 0.01714184321463108\n",
      "Epoch 81 training loss: 0.016957063227891922\n",
      "Epoch 82 training loss: 0.016967248171567917\n",
      "Epoch 83 training loss: 0.016986513510346413\n",
      "Epoch 84 training loss: 0.016889620572328568\n",
      "Epoch 85 training loss: 0.016919441521167755\n",
      "Epoch 86 training loss: 0.016626089811325073\n",
      "Epoch 87 training loss: 0.01669413223862648\n",
      "Epoch 88 training loss: 0.016765782609581947\n",
      "Epoch 89 training loss: 0.01650291495025158\n",
      "Epoch 90 training loss: 0.016583943739533424\n",
      "Epoch 91 training loss: 0.016262969002127647\n",
      "Epoch 92 training loss: 0.016220485791563988\n",
      "Epoch 93 training loss: 0.016070257872343063\n",
      "Epoch 94 training loss: 0.01596401445567608\n",
      "Epoch 95 training loss: 0.015809426084160805\n",
      "Epoch 96 training loss: 0.015720020979642868\n",
      "Epoch 97 training loss: 0.015735849738121033\n",
      "Epoch 98 training loss: 0.015719745308160782\n",
      "Epoch 99 training loss: 0.015509548597037792\n",
      "Epoch 100 training loss: 0.015322119928896427\n",
      "Epoch 101 training loss: 0.01532778050750494\n",
      "Epoch 102 training loss: 0.015076628886163235\n",
      "Epoch 103 training loss: 0.0149458646774292\n",
      "Epoch 104 training loss: 0.014805941842496395\n",
      "Epoch 105 training loss: 0.0147250946611166\n",
      "Epoch 106 training loss: 0.014789283275604248\n",
      "Epoch 107 training loss: 0.014615237712860107\n",
      "Epoch 108 training loss: 0.014453504234552383\n",
      "Epoch 109 training loss: 0.014508918859064579\n",
      "Epoch 110 training loss: 0.014261085540056229\n",
      "Epoch 111 training loss: 0.01426205225288868\n",
      "Epoch 112 training loss: 0.014262132346630096\n",
      "Epoch 113 training loss: 0.014112000353634357\n",
      "Epoch 114 training loss: 0.014012403786182404\n",
      "Epoch 115 training loss: 0.01408609189093113\n",
      "Epoch 116 training loss: 0.013886873610317707\n",
      "Epoch 117 training loss: 0.01374044269323349\n",
      "Epoch 118 training loss: 0.01387003343552351\n",
      "Epoch 119 training loss: 0.013772225007414818\n",
      "Epoch 120 training loss: 0.013795035891234875\n",
      "Epoch 121 training loss: 0.013466940261423588\n",
      "Epoch 122 training loss: 0.013755028136074543\n",
      "Epoch 123 training loss: 0.013726615346968174\n",
      "Epoch 124 training loss: 0.0134730851277709\n",
      "Epoch 125 training loss: 0.013547927141189575\n",
      "Epoch 126 training loss: 0.013528102077543736\n",
      "Epoch 127 training loss: 0.013424591161310673\n",
      "Epoch 128 training loss: 0.01351673249155283\n",
      "Epoch 129 training loss: 0.013498596847057343\n",
      "Epoch 130 training loss: 0.013442899100482464\n",
      "Epoch 131 training loss: 0.013364133425056934\n",
      "Epoch 132 training loss: 0.013354748487472534\n",
      "Epoch 133 training loss: 0.013236195780336857\n",
      "Epoch 134 training loss: 0.013323521241545677\n",
      "Epoch 135 training loss: 0.013115270994603634\n",
      "Epoch 136 training loss: 0.013142894953489304\n",
      "Epoch 137 training loss: 0.013322302140295506\n",
      "Epoch 138 training loss: 0.013042558915913105\n",
      "Epoch 139 training loss: 0.013162856921553612\n",
      "Epoch 140 training loss: 0.012983868829905987\n",
      "Epoch 141 training loss: 0.013026939705014229\n",
      "Epoch 142 training loss: 0.01309625431895256\n",
      "Epoch 143 training loss: 0.013126621954143047\n",
      "Epoch 144 training loss: 0.012845205143094063\n",
      "Epoch 145 training loss: 0.012928196229040623\n",
      "Epoch 146 training loss: 0.012924396432936192\n",
      "Epoch 147 training loss: 0.012866094708442688\n",
      "Epoch 148 training loss: 0.012874449603259563\n",
      "Epoch 149 training loss: 0.012811403721570969\n",
      "Epoch 150 training loss: 0.012868626043200493\n",
      "Epoch 151 training loss: 0.012910211458802223\n",
      "Epoch 152 training loss: 0.012596533633768559\n",
      "Epoch 153 training loss: 0.012956635095179081\n",
      "Epoch 154 training loss: 0.012805708684027195\n",
      "Epoch 155 training loss: 0.012791200540959835\n",
      "Epoch 156 training loss: 0.012770901434123516\n",
      "Epoch 157 training loss: 0.012653087265789509\n",
      "Epoch 158 training loss: 0.01258409209549427\n",
      "Epoch 159 training loss: 0.012826205231249332\n",
      "Epoch 160 training loss: 0.01245005801320076\n",
      "Epoch 161 training loss: 0.01264494564384222\n",
      "Epoch 162 training loss: 0.012617778964340687\n",
      "Epoch 163 training loss: 0.012514456175267696\n",
      "Epoch 164 training loss: 0.012502624653279781\n",
      "Epoch 165 training loss: 0.012357398867607117\n",
      "Epoch 166 training loss: 0.012393324635922909\n",
      "Epoch 167 training loss: 0.012389593757689\n",
      "Epoch 168 training loss: 0.012367045506834984\n",
      "Epoch 169 training loss: 0.012400475330650806\n",
      "Epoch 170 training loss: 0.012233864516019821\n",
      "Epoch 171 training loss: 0.012260227464139462\n",
      "Epoch 172 training loss: 0.01239768136292696\n",
      "Epoch 173 training loss: 0.012634100392460823\n",
      "Epoch 174 training loss: 0.012349237687885761\n",
      "Epoch 175 training loss: 0.012171860784292221\n",
      "Epoch 176 training loss: 0.012198963202536106\n",
      "Epoch 177 training loss: 0.01197326835244894\n",
      "Epoch 178 training loss: 0.012044228613376617\n",
      "Epoch 179 training loss: 0.012370489537715912\n",
      "Epoch 180 training loss: 0.012172732502222061\n",
      "Epoch 181 training loss: 0.012168047949671745\n",
      "Epoch 182 training loss: 0.012094411067664623\n",
      "Epoch 183 training loss: 0.012061355635523796\n",
      "Epoch 184 training loss: 0.012200168333947659\n",
      "Epoch 185 training loss: 0.011829286813735962\n",
      "Epoch 186 training loss: 0.01208648830652237\n",
      "Epoch 187 training loss: 0.012231837958097458\n",
      "Epoch 188 training loss: 0.012299724854528904\n",
      "Epoch 189 training loss: 0.01174398697912693\n",
      "Epoch 190 training loss: 0.011918370611965656\n",
      "Epoch 191 training loss: 0.011898685246706009\n",
      "Epoch 192 training loss: 0.011908282525837421\n",
      "Epoch 193 training loss: 0.011898086406290531\n",
      "Epoch 194 training loss: 0.01197363343089819\n",
      "Epoch 195 training loss: 0.011655939742922783\n",
      "Epoch 196 training loss: 0.01202722080051899\n",
      "Epoch 197 training loss: 0.01174541749060154\n",
      "Epoch 198 training loss: 0.011885097250342369\n",
      "Epoch 199 training loss: 0.011889418587088585\n",
      "Epoch 200 training loss: 0.011561723425984383\n",
      "Epoch 201 training loss: 0.012064131908118725\n",
      "Epoch 202 training loss: 0.011695858091115952\n",
      "Epoch 203 training loss: 0.011639691889286041\n",
      "Epoch 204 training loss: 0.011742192320525646\n",
      "Epoch 205 training loss: 0.011739774607121944\n",
      "Epoch 206 training loss: 0.011679609306156635\n",
      "Epoch 207 training loss: 0.011578892357647419\n",
      "Epoch 208 training loss: 0.011658375151455402\n",
      "Epoch 209 training loss: 0.011785699985921383\n",
      "Epoch 210 training loss: 0.011810307390987873\n",
      "Epoch 211 training loss: 0.011679754592478275\n",
      "Epoch 212 training loss: 0.011694684624671936\n",
      "Epoch 213 training loss: 0.011764619499444962\n",
      "Epoch 214 training loss: 0.011479787528514862\n",
      "Epoch 215 training loss: 0.011653930880129337\n",
      "Epoch 216 training loss: 0.01200628187507391\n",
      "Epoch 217 training loss: 0.01153975073248148\n",
      "Epoch 218 training loss: 0.0112830251455307\n",
      "Epoch 219 training loss: 0.011388145387172699\n",
      "Epoch 220 training loss: 0.011851916089653969\n",
      "Epoch 221 training loss: 0.011348015628755093\n",
      "Epoch 222 training loss: 0.011732806451618671\n",
      "Epoch 223 training loss: 0.011590567417442799\n",
      "Epoch 224 training loss: 0.011516428552567959\n",
      "Epoch 225 training loss: 0.011593179777264595\n",
      "Epoch 226 training loss: 0.011507459916174412\n",
      "Epoch 227 training loss: 0.011482157744467258\n",
      "Epoch 228 training loss: 0.011658458039164543\n",
      "Epoch 229 training loss: 0.01136043295264244\n",
      "Epoch 230 training loss: 0.011310987174510956\n",
      "Epoch 231 training loss: 0.01154673844575882\n",
      "Epoch 232 training loss: 0.01141483522951603\n",
      "Epoch 233 training loss: 0.01157707441598177\n",
      "Epoch 234 training loss: 0.01124538667500019\n",
      "Epoch 235 training loss: 0.011882472783327103\n",
      "Epoch 236 training loss: 0.011054693721234798\n",
      "Epoch 237 training loss: 0.011390742845833302\n",
      "Epoch 238 training loss: 0.011319820769131184\n",
      "Epoch 239 training loss: 0.011219548992812634\n",
      "Epoch 240 training loss: 0.011469433084130287\n",
      "Epoch 241 training loss: 0.011268419213593006\n",
      "Epoch 242 training loss: 0.011506297625601292\n",
      "Epoch 243 training loss: 0.011308002285659313\n",
      "Epoch 244 training loss: 0.011324524879455566\n",
      "Epoch 245 training loss: 0.01146594900637865\n",
      "Epoch 246 training loss: 0.011356067843735218\n",
      "Epoch 247 training loss: 0.011158529669046402\n",
      "Epoch 248 training loss: 0.011280433274805546\n",
      "Epoch 249 training loss: 0.01135330367833376\n",
      "Epoch 250 training loss: 0.011020055040717125\n",
      "Epoch 251 training loss: 0.011329383589327335\n",
      "Epoch 252 training loss: 0.011040044017136097\n",
      "Epoch 253 training loss: 0.01134117878973484\n",
      "Epoch 254 training loss: 0.011233450844883919\n",
      "Epoch 255 training loss: 0.011138062924146652\n",
      "Epoch 256 training loss: 0.011081528849899769\n",
      "Epoch 257 training loss: 0.011081446893513203\n",
      "Epoch 258 training loss: 0.011301102116703987\n",
      "Epoch 259 training loss: 0.011142807081341743\n",
      "Epoch 260 training loss: 0.011223028413951397\n",
      "Epoch 261 training loss: 0.01106283813714981\n",
      "Epoch 262 training loss: 0.01103614829480648\n",
      "Epoch 263 training loss: 0.010996030643582344\n",
      "Epoch 264 training loss: 0.011049854569137096\n",
      "Epoch 265 training loss: 0.011121783405542374\n",
      "Epoch 266 training loss: 0.01113135740160942\n",
      "Epoch 267 training loss: 0.010935545898973942\n",
      "Epoch 268 training loss: 0.010910521261394024\n",
      "Epoch 269 training loss: 0.010923976078629494\n",
      "Epoch 270 training loss: 0.011444998905062675\n",
      "Epoch 271 training loss: 0.011054503731429577\n",
      "Epoch 272 training loss: 0.011215504258871078\n",
      "Epoch 273 training loss: 0.010934562422335148\n",
      "Epoch 274 training loss: 0.01084035262465477\n",
      "Epoch 275 training loss: 0.010942122898995876\n",
      "Epoch 276 training loss: 0.010909593664109707\n",
      "Epoch 277 training loss: 0.010754035785794258\n",
      "Epoch 278 training loss: 0.010961291380226612\n",
      "Epoch 279 training loss: 0.01077063474804163\n",
      "Epoch 280 training loss: 0.010956691578030586\n",
      "Epoch 281 training loss: 0.010772631503641605\n",
      "Epoch 282 training loss: 0.010638123378157616\n",
      "Epoch 283 training loss: 0.010591333732008934\n",
      "Epoch 284 training loss: 0.01067272387444973\n",
      "Epoch 285 training loss: 0.010974961332976818\n",
      "Epoch 286 training loss: 0.010780639946460724\n",
      "Epoch 287 training loss: 0.010791517794132233\n",
      "Epoch 288 training loss: 0.0105669517070055\n",
      "Epoch 289 training loss: 0.01073646079748869\n",
      "Epoch 290 training loss: 0.010761368088424206\n",
      "Epoch 291 training loss: 0.010698641650378704\n",
      "Epoch 292 training loss: 0.010801289230585098\n",
      "Epoch 293 training loss: 0.010589657351374626\n",
      "Epoch 294 training loss: 0.010684750974178314\n",
      "Epoch 295 training loss: 0.010689185932278633\n",
      "Epoch 296 training loss: 0.010466506704688072\n",
      "Epoch 297 training loss: 0.010655274614691734\n",
      "Epoch 298 training loss: 0.010381422005593777\n",
      "Epoch 299 training loss: 0.010593751445412636\n",
      "Epoch 300 training loss: 0.010622636415064335\n",
      "Epoch 301 training loss: 0.010421764105558395\n",
      "Epoch 302 training loss: 0.010420805774629116\n",
      "Epoch 303 training loss: 0.010652933269739151\n",
      "Epoch 304 training loss: 0.010435684584081173\n",
      "Epoch 305 training loss: 0.010253746062517166\n",
      "Epoch 306 training loss: 0.01019638404250145\n",
      "Epoch 307 training loss: 0.010276221670210361\n",
      "Epoch 308 training loss: 0.010261121205985546\n",
      "Epoch 309 training loss: 0.010358776897192001\n",
      "Epoch 310 training loss: 0.010143717750906944\n",
      "Epoch 311 training loss: 0.010404807515442371\n",
      "Epoch 312 training loss: 0.009961500763893127\n",
      "Epoch 313 training loss: 0.010207260958850384\n",
      "Epoch 314 training loss: 0.010304387658834457\n",
      "Epoch 315 training loss: 0.010022367350757122\n",
      "Epoch 316 training loss: 0.010237142443656921\n",
      "Epoch 317 training loss: 0.01025999803096056\n",
      "Epoch 318 training loss: 0.010450112633407116\n",
      "Epoch 319 training loss: 0.009985913522541523\n",
      "Epoch 320 training loss: 0.010069666430354118\n",
      "Epoch 321 training loss: 0.009971365332603455\n",
      "Epoch 322 training loss: 0.009976687841117382\n",
      "Epoch 323 training loss: 0.010095523670315742\n",
      "Epoch 324 training loss: 0.009875748306512833\n",
      "Epoch 325 training loss: 0.010242658667266369\n",
      "Epoch 326 training loss: 0.010075428523123264\n",
      "Epoch 327 training loss: 0.010135315358638763\n",
      "Epoch 328 training loss: 0.009928209707140923\n",
      "Epoch 329 training loss: 0.009937942959368229\n",
      "Epoch 330 training loss: 0.009918405674397945\n",
      "Epoch 331 training loss: 0.01000137533992529\n",
      "Epoch 332 training loss: 0.010177052579820156\n",
      "Epoch 333 training loss: 0.009866567328572273\n",
      "Epoch 334 training loss: 0.010059470310807228\n",
      "Epoch 335 training loss: 0.010027477517724037\n",
      "Epoch 336 training loss: 0.009678058326244354\n",
      "Epoch 337 training loss: 0.010004648938775063\n",
      "Epoch 338 training loss: 0.010023151524364948\n",
      "Epoch 339 training loss: 0.009657816030085087\n",
      "Epoch 340 training loss: 0.01002924982458353\n",
      "Epoch 341 training loss: 0.009686440229415894\n",
      "Epoch 342 training loss: 0.009809969924390316\n",
      "Epoch 343 training loss: 0.010121417231857777\n",
      "Epoch 344 training loss: 0.0098272655159235\n",
      "Epoch 345 training loss: 0.009696667082607746\n",
      "Epoch 346 training loss: 0.010033882223069668\n",
      "Epoch 347 training loss: 0.009900040924549103\n",
      "Epoch 348 training loss: 0.009824531152844429\n",
      "Epoch 349 training loss: 0.009669742546975613\n",
      "Epoch 350 training loss: 0.00971104484051466\n",
      "Epoch 351 training loss: 0.009999791160225868\n",
      "Epoch 352 training loss: 0.00955488346517086\n",
      "Epoch 353 training loss: 0.009670785628259182\n",
      "Epoch 354 training loss: 0.009819824248552322\n",
      "Epoch 355 training loss: 0.00966666266322136\n",
      "Epoch 356 training loss: 0.009682944975793362\n",
      "Epoch 357 training loss: 0.009882813319563866\n",
      "Epoch 358 training loss: 0.0096292644739151\n",
      "Epoch 359 training loss: 0.009934237226843834\n",
      "Epoch 360 training loss: 0.00956020224839449\n",
      "Epoch 361 training loss: 0.00965520367026329\n",
      "Epoch 362 training loss: 0.009952129796147346\n",
      "Epoch 363 training loss: 0.009937339462339878\n",
      "Epoch 364 training loss: 0.009598472155630589\n",
      "Epoch 365 training loss: 0.009525857865810394\n",
      "Epoch 366 training loss: 0.009858150035142899\n",
      "Epoch 367 training loss: 0.009870336391031742\n",
      "Epoch 368 training loss: 0.009424243122339249\n",
      "Epoch 369 training loss: 0.009602248668670654\n",
      "Epoch 370 training loss: 0.00990298017859459\n",
      "Epoch 371 training loss: 0.009403186850249767\n",
      "Epoch 372 training loss: 0.00973527505993843\n",
      "Epoch 373 training loss: 0.009549005888402462\n",
      "Epoch 374 training loss: 0.009692604653537273\n",
      "Epoch 375 training loss: 0.009650996886193752\n",
      "Epoch 376 training loss: 0.009488692507147789\n",
      "Epoch 377 training loss: 0.009469796903431416\n",
      "Epoch 378 training loss: 0.009694532491266727\n",
      "Epoch 379 training loss: 0.009678853675723076\n",
      "Epoch 380 training loss: 0.009566822089254856\n",
      "Epoch 381 training loss: 0.009565792046487331\n",
      "Epoch 382 training loss: 0.009568976238369942\n",
      "Epoch 383 training loss: 0.009527474641799927\n",
      "Epoch 384 training loss: 0.01006406731903553\n",
      "Epoch 385 training loss: 0.009586887434124947\n",
      "Epoch 386 training loss: 0.009327422827482224\n",
      "Epoch 387 training loss: 0.009741404093801975\n",
      "Epoch 388 training loss: 0.00947154313325882\n",
      "Epoch 389 training loss: 0.00936540961265564\n",
      "Epoch 390 training loss: 0.009350317530333996\n",
      "Epoch 391 training loss: 0.009850761853158474\n",
      "Epoch 392 training loss: 0.009269769303500652\n",
      "Epoch 393 training loss: 0.009540095925331116\n",
      "Epoch 394 training loss: 0.009438412263989449\n",
      "Epoch 395 training loss: 0.009274279698729515\n",
      "Epoch 396 training loss: 0.009731369093060493\n",
      "Epoch 397 training loss: 0.009290158748626709\n",
      "Epoch 398 training loss: 0.00943857990205288\n",
      "Epoch 399 training loss: 0.00940011814236641\n",
      "Epoch 400 training loss: 0.009673196822404861\n",
      "Epoch 401 training loss: 0.009381996467709541\n",
      "Epoch 402 training loss: 0.009437784552574158\n",
      "Epoch 403 training loss: 0.00949112419039011\n",
      "Epoch 404 training loss: 0.009520154446363449\n",
      "Epoch 405 training loss: 0.009454549290239811\n",
      "Epoch 406 training loss: 0.009419705718755722\n",
      "Epoch 407 training loss: 0.009524883702397346\n",
      "Epoch 408 training loss: 0.009538916870951653\n",
      "Epoch 409 training loss: 0.009261060506105423\n",
      "Epoch 410 training loss: 0.009212708100676537\n",
      "Epoch 411 training loss: 0.009389158338308334\n",
      "Epoch 412 training loss: 0.009568961337208748\n",
      "Epoch 413 training loss: 0.009404712356626987\n",
      "Epoch 414 training loss: 0.009479761123657227\n",
      "Epoch 415 training loss: 0.009104921482503414\n",
      "Epoch 416 training loss: 0.009668322280049324\n",
      "Epoch 417 training loss: 0.00929603073745966\n",
      "Epoch 418 training loss: 0.009567617438733578\n",
      "Epoch 419 training loss: 0.009306446649134159\n",
      "Epoch 420 training loss: 0.009673557244241238\n",
      "Epoch 421 training loss: 0.009112948551774025\n",
      "Epoch 422 training loss: 0.009274529293179512\n",
      "Epoch 423 training loss: 0.009748215787112713\n",
      "Epoch 424 training loss: 0.009228074923157692\n",
      "Epoch 425 training loss: 0.00903441570699215\n",
      "Epoch 426 training loss: 0.009638877585530281\n",
      "Epoch 427 training loss: 0.009023194201290607\n",
      "Epoch 428 training loss: 0.009368078783154488\n",
      "Epoch 429 training loss: 0.00941532850265503\n",
      "Epoch 430 training loss: 0.009169190190732479\n",
      "Epoch 431 training loss: 0.009141091257333755\n",
      "Epoch 432 training loss: 0.009646044112741947\n",
      "Epoch 433 training loss: 0.009081192314624786\n",
      "Epoch 434 training loss: 0.009540260769426823\n",
      "Epoch 435 training loss: 0.009164859540760517\n",
      "Epoch 436 training loss: 0.009486817754805088\n",
      "Epoch 437 training loss: 0.009112107567489147\n",
      "Epoch 438 training loss: 0.009274814277887344\n",
      "Epoch 439 training loss: 0.009296133182942867\n",
      "Epoch 440 training loss: 0.009168080985546112\n",
      "Epoch 441 training loss: 0.009260637685656548\n",
      "Epoch 442 training loss: 0.009262514300644398\n",
      "Epoch 443 training loss: 0.009085562080144882\n",
      "Epoch 444 training loss: 0.009202882647514343\n",
      "Epoch 445 training loss: 0.009413398802280426\n",
      "Epoch 446 training loss: 0.009133348241448402\n",
      "Epoch 447 training loss: 0.009137040004134178\n",
      "Epoch 448 training loss: 0.009242397733032703\n",
      "Epoch 449 training loss: 0.009239732287824154\n",
      "Epoch 450 training loss: 0.009371457621455193\n",
      "Epoch 451 training loss: 0.00934488046914339\n",
      "Epoch 452 training loss: 0.009194781072437763\n",
      "Epoch 453 training loss: 0.009371655993163586\n",
      "Epoch 454 training loss: 0.009027007035911083\n",
      "Epoch 455 training loss: 0.009318487718701363\n",
      "Epoch 456 training loss: 0.009240019135177135\n",
      "Epoch 457 training loss: 0.009474236518144608\n",
      "Epoch 458 training loss: 0.009028620086610317\n",
      "Epoch 459 training loss: 0.009033833630383015\n",
      "Epoch 460 training loss: 0.008953500539064407\n",
      "Epoch 461 training loss: 0.009265515953302383\n",
      "Epoch 462 training loss: 0.009097679518163204\n",
      "Epoch 463 training loss: 0.009329671040177345\n",
      "Epoch 464 training loss: 0.009469401091337204\n",
      "Epoch 465 training loss: 0.008978722617030144\n",
      "Epoch 466 training loss: 0.009172405116260052\n",
      "Epoch 467 training loss: 0.00906308926641941\n",
      "Epoch 468 training loss: 0.00920933485031128\n",
      "Epoch 469 training loss: 0.009196162223815918\n",
      "Epoch 470 training loss: 0.00904603861272335\n",
      "Epoch 471 training loss: 0.009224949404597282\n",
      "Epoch 472 training loss: 0.00923863798379898\n",
      "Epoch 473 training loss: 0.009064044803380966\n",
      "Epoch 474 training loss: 0.00897980760782957\n",
      "Epoch 475 training loss: 0.009276341646909714\n",
      "Epoch 476 training loss: 0.009001745842397213\n",
      "Epoch 477 training loss: 0.008977817371487617\n",
      "Epoch 478 training loss: 0.009038344956934452\n",
      "Epoch 479 training loss: 0.00916667003184557\n",
      "Epoch 480 training loss: 0.00911634974181652\n",
      "Epoch 481 training loss: 0.00910236407071352\n",
      "Epoch 482 training loss: 0.009248162619769573\n",
      "Epoch 483 training loss: 0.009188884869217873\n",
      "Epoch 484 training loss: 0.009109900332987309\n",
      "Epoch 485 training loss: 0.008822126314043999\n",
      "Epoch 486 training loss: 0.008925259113311768\n",
      "Epoch 487 training loss: 0.009184682741761208\n",
      "Epoch 488 training loss: 0.008965014480054379\n",
      "Epoch 489 training loss: 0.008863549679517746\n",
      "Epoch 490 training loss: 0.00938031729310751\n",
      "Epoch 491 training loss: 0.008836635388433933\n",
      "Epoch 492 training loss: 0.009050318971276283\n",
      "Epoch 493 training loss: 0.009250686503946781\n",
      "Epoch 494 training loss: 0.008893124759197235\n",
      "Epoch 495 training loss: 0.008961492218077183\n",
      "Epoch 496 training loss: 0.008829416707158089\n",
      "Epoch 497 training loss: 0.009320725686848164\n",
      "Epoch 498 training loss: 0.008952563628554344\n",
      "Epoch 499 training loss: 0.008839874528348446\n",
      "Epoch 500 training loss: 0.009156439453363419\n",
      "Epoch 501 training loss: 0.008898827247321606\n",
      "Epoch 502 training loss: 0.008777234703302383\n",
      "Epoch 503 training loss: 0.008794979192316532\n",
      "Epoch 504 training loss: 0.008908894844353199\n",
      "Epoch 505 training loss: 0.008988459594547749\n",
      "Epoch 506 training loss: 0.008977235294878483\n",
      "Epoch 507 training loss: 0.00879860669374466\n",
      "Epoch 508 training loss: 0.008927769958972931\n",
      "Epoch 509 training loss: 0.008836448192596436\n",
      "Epoch 510 training loss: 0.009097130037844181\n",
      "Epoch 511 training loss: 0.00904318131506443\n",
      "Epoch 512 training loss: 0.00870087742805481\n",
      "Epoch 513 training loss: 0.008634395897388458\n",
      "Epoch 514 training loss: 0.008848683908581734\n",
      "Epoch 515 training loss: 0.009062153287231922\n",
      "Epoch 516 training loss: 0.008889962919056416\n",
      "Epoch 517 training loss: 0.008752786554396152\n",
      "Epoch 518 training loss: 0.008854387328028679\n",
      "Epoch 519 training loss: 0.008867713622748852\n",
      "Epoch 520 training loss: 0.008795225992798805\n",
      "Epoch 521 training loss: 0.00894156564027071\n",
      "Epoch 522 training loss: 0.00883578322827816\n",
      "Epoch 523 training loss: 0.008985261432826519\n",
      "Epoch 524 training loss: 0.008717541582882404\n",
      "Epoch 525 training loss: 0.008579688146710396\n",
      "Epoch 526 training loss: 0.008715174160897732\n",
      "Epoch 527 training loss: 0.008909651078283787\n",
      "Epoch 528 training loss: 0.008900923654437065\n",
      "Epoch 529 training loss: 0.008715376257896423\n",
      "Epoch 530 training loss: 0.008561994880437851\n",
      "Epoch 531 training loss: 0.008716908283531666\n",
      "Epoch 532 training loss: 0.00891061034053564\n",
      "Epoch 533 training loss: 0.008749389089643955\n",
      "Epoch 534 training loss: 0.008957895450294018\n",
      "Epoch 535 training loss: 0.00877994392067194\n",
      "Epoch 536 training loss: 0.008759369142353535\n",
      "Epoch 537 training loss: 0.008641195483505726\n",
      "Epoch 538 training loss: 0.008670295588672161\n",
      "Epoch 539 training loss: 0.008944286033511162\n",
      "Epoch 540 training loss: 0.008929719217121601\n",
      "Epoch 541 training loss: 0.00873473659157753\n",
      "Epoch 542 training loss: 0.008933340199291706\n",
      "Epoch 543 training loss: 0.008694100193679333\n",
      "Epoch 544 training loss: 0.008475381880998611\n",
      "Epoch 545 training loss: 0.008677411824464798\n",
      "Epoch 546 training loss: 0.008898697793483734\n",
      "Epoch 547 training loss: 0.00868223886936903\n",
      "Epoch 548 training loss: 0.008795568719506264\n",
      "Epoch 549 training loss: 0.00891898013651371\n",
      "Epoch 550 training loss: 0.008634427562355995\n",
      "Epoch 551 training loss: 0.008677683770656586\n",
      "Epoch 552 training loss: 0.008790411055088043\n",
      "Epoch 553 training loss: 0.008700699545443058\n",
      "Epoch 554 training loss: 0.009051620028913021\n",
      "Epoch 555 training loss: 0.008593450300395489\n",
      "Epoch 556 training loss: 0.008639809675514698\n",
      "Epoch 557 training loss: 0.008811318315565586\n",
      "Epoch 558 training loss: 0.008699143305420876\n",
      "Epoch 559 training loss: 0.008663637563586235\n",
      "Epoch 560 training loss: 0.008647129870951176\n",
      "Epoch 561 training loss: 0.008563620038330555\n",
      "Epoch 562 training loss: 0.008810639381408691\n",
      "Epoch 563 training loss: 0.008913333527743816\n",
      "Epoch 564 training loss: 0.008843692019581795\n",
      "Epoch 565 training loss: 0.008675199933350086\n",
      "Epoch 566 training loss: 0.008512914180755615\n",
      "Epoch 567 training loss: 0.008758391253650188\n",
      "Epoch 568 training loss: 0.008729410357773304\n",
      "Epoch 569 training loss: 0.008661090396344662\n",
      "Epoch 570 training loss: 0.008738640695810318\n",
      "Epoch 571 training loss: 0.008451426401734352\n",
      "Epoch 572 training loss: 0.008696211501955986\n",
      "Epoch 573 training loss: 0.008737375028431416\n",
      "Epoch 574 training loss: 0.008923017419874668\n",
      "Epoch 575 training loss: 0.008602838031947613\n",
      "Epoch 576 training loss: 0.008652178570628166\n",
      "Epoch 577 training loss: 0.008894597180187702\n",
      "Epoch 578 training loss: 0.008335504680871964\n",
      "Epoch 579 training loss: 0.008541040122509003\n",
      "Epoch 580 training loss: 0.00883355550467968\n",
      "Epoch 581 training loss: 0.0084293894469738\n",
      "Epoch 582 training loss: 0.00897175632417202\n",
      "Epoch 583 training loss: 0.00840186607092619\n",
      "Epoch 584 training loss: 0.008730503730475903\n",
      "Epoch 585 training loss: 0.008630086667835712\n",
      "Epoch 586 training loss: 0.008576112799346447\n",
      "Epoch 587 training loss: 0.00872646551579237\n",
      "Epoch 588 training loss: 0.008646802045404911\n",
      "Epoch 589 training loss: 0.008578276261687279\n",
      "Epoch 590 training loss: 0.008513168431818485\n",
      "Epoch 591 training loss: 0.008587310090661049\n",
      "Epoch 592 training loss: 0.008604536764323711\n",
      "Epoch 593 training loss: 0.00893746130168438\n",
      "Epoch 594 training loss: 0.008510453626513481\n",
      "Epoch 595 training loss: 0.008675678633153439\n",
      "Epoch 596 training loss: 0.008451935835182667\n",
      "Epoch 597 training loss: 0.008521700277924538\n",
      "Epoch 598 training loss: 0.009011649526655674\n",
      "Epoch 599 training loss: 0.008504745550453663\n",
      "Epoch 600 training loss: 0.008573257364332676\n",
      "Epoch 601 training loss: 0.008619491942226887\n",
      "Epoch 602 training loss: 0.008843472227454185\n",
      "Epoch 603 training loss: 0.008994358591735363\n",
      "Epoch 604 training loss: 0.008469993248581886\n",
      "Epoch 605 training loss: 0.008445791900157928\n",
      "Epoch 606 training loss: 0.008532444015145302\n",
      "Epoch 607 training loss: 0.008463999256491661\n",
      "Epoch 608 training loss: 0.0086775878444314\n",
      "Epoch 609 training loss: 0.00856398232281208\n",
      "Epoch 610 training loss: 0.008671989664435387\n",
      "Epoch 611 training loss: 0.008435714058578014\n",
      "Epoch 612 training loss: 0.008434413000941277\n",
      "Epoch 613 training loss: 0.008898388594388962\n",
      "Epoch 614 training loss: 0.008828600868582726\n",
      "Epoch 615 training loss: 0.008280147798359394\n",
      "Epoch 616 training loss: 0.00852435827255249\n",
      "Epoch 617 training loss: 0.00848072487860918\n",
      "Epoch 618 training loss: 0.008488787338137627\n",
      "Epoch 619 training loss: 0.00874719861894846\n",
      "Epoch 620 training loss: 0.00855766050517559\n",
      "Epoch 621 training loss: 0.008398032747209072\n",
      "Epoch 622 training loss: 0.008556804619729519\n",
      "Epoch 623 training loss: 0.008449752815067768\n",
      "Epoch 624 training loss: 0.008492459543049335\n",
      "Epoch 625 training loss: 0.008899562060832977\n",
      "Epoch 626 training loss: 0.008824048563838005\n",
      "Epoch 627 training loss: 0.00858842022716999\n",
      "Epoch 628 training loss: 0.008401916362345219\n",
      "Epoch 629 training loss: 0.008345392532646656\n",
      "Epoch 630 training loss: 0.00889828335493803\n",
      "Epoch 631 training loss: 0.008557728491723537\n",
      "Epoch 632 training loss: 0.008350343443453312\n",
      "Epoch 633 training loss: 0.00859352108091116\n",
      "Epoch 634 training loss: 0.008457453921437263\n",
      "Epoch 635 training loss: 0.008814291097223759\n",
      "Epoch 636 training loss: 0.008441533893346786\n",
      "Epoch 637 training loss: 0.008450169116258621\n",
      "Epoch 638 training loss: 0.008427351713180542\n",
      "Epoch 639 training loss: 0.0085174310952425\n",
      "Epoch 640 training loss: 0.008523254655301571\n",
      "Epoch 641 training loss: 0.008497603237628937\n",
      "Epoch 642 training loss: 0.008581779897212982\n",
      "Epoch 643 training loss: 0.008732667192816734\n",
      "Epoch 644 training loss: 0.008699421770870686\n",
      "Epoch 645 training loss: 0.008363373577594757\n",
      "Epoch 646 training loss: 0.008329274132847786\n",
      "Epoch 647 training loss: 0.008902073837816715\n",
      "Epoch 648 training loss: 0.00874546729028225\n",
      "Epoch 649 training loss: 0.008514892309904099\n",
      "Epoch 650 training loss: 0.008297733031213284\n",
      "Epoch 651 training loss: 0.0088099529966712\n",
      "Epoch 652 training loss: 0.008288092911243439\n",
      "Epoch 653 training loss: 0.008332960307598114\n",
      "Epoch 654 training loss: 0.008592951111495495\n",
      "Epoch 655 training loss: 0.00888681598007679\n",
      "Epoch 656 training loss: 0.008335870690643787\n",
      "Epoch 657 training loss: 0.008519683964550495\n",
      "Epoch 658 training loss: 0.008554061874747276\n",
      "Epoch 659 training loss: 0.008631521835923195\n",
      "Epoch 660 training loss: 0.008430644869804382\n",
      "Epoch 661 training loss: 0.00824875570833683\n",
      "Epoch 662 training loss: 0.008599348366260529\n",
      "Epoch 663 training loss: 0.008515299297869205\n",
      "Epoch 664 training loss: 0.008835644461214542\n",
      "Epoch 665 training loss: 0.008942041546106339\n",
      "Epoch 666 training loss: 0.008754200302064419\n",
      "Epoch 667 training loss: 0.008416247554123402\n",
      "Epoch 668 training loss: 0.008281665854156017\n",
      "Epoch 669 training loss: 0.008340160362422466\n",
      "Epoch 670 training loss: 0.008376482874155045\n",
      "Epoch 671 training loss: 0.008862965740263462\n",
      "Epoch 672 training loss: 0.008259245194494724\n",
      "Epoch 673 training loss: 0.008938350714743137\n",
      "Epoch 674 training loss: 0.00822127889841795\n",
      "Epoch 675 training loss: 0.008435368537902832\n",
      "Epoch 676 training loss: 0.008347258903086185\n",
      "Epoch 677 training loss: 0.008762971498072147\n",
      "Epoch 678 training loss: 0.008370406925678253\n",
      "Epoch 679 training loss: 0.008297846652567387\n",
      "Epoch 680 training loss: 0.008381886407732964\n",
      "Epoch 681 training loss: 0.008609370328485966\n",
      "Epoch 682 training loss: 0.008612595498561859\n",
      "Epoch 683 training loss: 0.008434093557298183\n",
      "Epoch 684 training loss: 0.008489913307130337\n",
      "Epoch 685 training loss: 0.008396521210670471\n",
      "Epoch 686 training loss: 0.008247874677181244\n",
      "Epoch 687 training loss: 0.008861501701176167\n",
      "Epoch 688 training loss: 0.008430159650743008\n",
      "Epoch 689 training loss: 0.008324438706040382\n",
      "Epoch 690 training loss: 0.008738507516682148\n",
      "Epoch 691 training loss: 0.00825980119407177\n",
      "Epoch 692 training loss: 0.008512438274919987\n",
      "Epoch 693 training loss: 0.008478742092847824\n",
      "Epoch 694 training loss: 0.008423840627074242\n",
      "Epoch 695 training loss: 0.008339869789779186\n",
      "Epoch 696 training loss: 0.00839537288993597\n",
      "Epoch 697 training loss: 0.008555484935641289\n",
      "Epoch 698 training loss: 0.008531409315764904\n",
      "Epoch 699 training loss: 0.00833289884030819\n",
      "Epoch 700 training loss: 0.008344187401235104\n",
      "Epoch 701 training loss: 0.008707464672625065\n",
      "Epoch 702 training loss: 0.008305457420647144\n",
      "Epoch 703 training loss: 0.008581327274441719\n",
      "Epoch 704 training loss: 0.008271670900285244\n",
      "Epoch 705 training loss: 0.008621017448604107\n",
      "Epoch 706 training loss: 0.008348560892045498\n",
      "Epoch 707 training loss: 0.008338822983205318\n",
      "Epoch 708 training loss: 0.008211687207221985\n",
      "Epoch 709 training loss: 0.008209341205656528\n",
      "Epoch 710 training loss: 0.00874833669513464\n",
      "Epoch 711 training loss: 0.008205406367778778\n",
      "Epoch 712 training loss: 0.008615368977189064\n",
      "Epoch 713 training loss: 0.008283472619950771\n",
      "Epoch 714 training loss: 0.008651024661958218\n",
      "Epoch 715 training loss: 0.008192772045731544\n",
      "Epoch 716 training loss: 0.008076432161033154\n",
      "Epoch 717 training loss: 0.00862258579581976\n",
      "Epoch 718 training loss: 0.008246304467320442\n",
      "Epoch 719 training loss: 0.008493509143590927\n",
      "Epoch 720 training loss: 0.008234602399170399\n",
      "Epoch 721 training loss: 0.008709704503417015\n",
      "Epoch 722 training loss: 0.00837749894708395\n",
      "Epoch 723 training loss: 0.008326312527060509\n",
      "Epoch 724 training loss: 0.008404752239584923\n",
      "Epoch 725 training loss: 0.008679263293743134\n",
      "Epoch 726 training loss: 0.008234835229814053\n",
      "Epoch 727 training loss: 0.008328838273882866\n",
      "Epoch 728 training loss: 0.008505468256771564\n",
      "Epoch 729 training loss: 0.008255240507423878\n",
      "Epoch 730 training loss: 0.00834964495152235\n",
      "Epoch 731 training loss: 0.008417591452598572\n",
      "Epoch 732 training loss: 0.008446235209703445\n",
      "Epoch 733 training loss: 0.00825412105768919\n",
      "Epoch 734 training loss: 0.008568170480430126\n",
      "Epoch 735 training loss: 0.008329366333782673\n",
      "Epoch 736 training loss: 0.008233094587922096\n",
      "Epoch 737 training loss: 0.008374525234103203\n",
      "Epoch 738 training loss: 0.008557328954339027\n",
      "Epoch 739 training loss: 0.008348801173269749\n",
      "Epoch 740 training loss: 0.008513232693076134\n",
      "Epoch 741 training loss: 0.008199235424399376\n",
      "Epoch 742 training loss: 0.008383841253817081\n",
      "Epoch 743 training loss: 0.008428363129496574\n",
      "Epoch 744 training loss: 0.008375494740903378\n",
      "Epoch 745 training loss: 0.008556438609957695\n",
      "Epoch 746 training loss: 0.008249896578490734\n",
      "Epoch 747 training loss: 0.008369605988264084\n",
      "Epoch 748 training loss: 0.00842728465795517\n",
      "Epoch 749 training loss: 0.008390555158257484\n",
      "Epoch 750 training loss: 0.00822382140904665\n",
      "Epoch 751 training loss: 0.008525162935256958\n",
      "Epoch 752 training loss: 0.008210500702261925\n",
      "Epoch 753 training loss: 0.008203105069696903\n",
      "Epoch 754 training loss: 0.008248377591371536\n",
      "Epoch 755 training loss: 0.008400524035096169\n",
      "Epoch 756 training loss: 0.008361446671187878\n",
      "Epoch 757 training loss: 0.008429838344454765\n",
      "Epoch 758 training loss: 0.008207744918763638\n",
      "Epoch 759 training loss: 0.008533376269042492\n",
      "Epoch 760 training loss: 0.00832377839833498\n",
      "Epoch 761 training loss: 0.008343493565917015\n",
      "Epoch 762 training loss: 0.008451957255601883\n",
      "Epoch 763 training loss: 0.008307973854243755\n",
      "Epoch 764 training loss: 0.008073871023952961\n",
      "Epoch 765 training loss: 0.008419377729296684\n",
      "Epoch 766 training loss: 0.00831508170813322\n",
      "Epoch 767 training loss: 0.008404633030295372\n",
      "Epoch 768 training loss: 0.008276611566543579\n",
      "Epoch 769 training loss: 0.008289454504847527\n",
      "Epoch 770 training loss: 0.00835469551384449\n",
      "Epoch 771 training loss: 0.008271627128124237\n",
      "Epoch 772 training loss: 0.008123443461954594\n",
      "Epoch 773 training loss: 0.008311482146382332\n",
      "Epoch 774 training loss: 0.008332481607794762\n",
      "Epoch 775 training loss: 0.00850042887032032\n",
      "Epoch 776 training loss: 0.008141395635902882\n",
      "Epoch 777 training loss: 0.00825379230082035\n",
      "Epoch 778 training loss: 0.00854759942740202\n",
      "Epoch 779 training loss: 0.008038002997636795\n",
      "Epoch 780 training loss: 0.00849326141178608\n",
      "Epoch 781 training loss: 0.008361494168639183\n",
      "Epoch 782 training loss: 0.008202245458960533\n",
      "Epoch 783 training loss: 0.008329632692039013\n",
      "Epoch 784 training loss: 0.0082162544131279\n",
      "Epoch 785 training loss: 0.008536974899470806\n",
      "Epoch 786 training loss: 0.008297205902636051\n",
      "Epoch 787 training loss: 0.008304029703140259\n",
      "Epoch 788 training loss: 0.008089307695627213\n",
      "Epoch 789 training loss: 0.008323786780238152\n",
      "Epoch 790 training loss: 0.008280452340841293\n",
      "Epoch 791 training loss: 0.008332964964210987\n",
      "Epoch 792 training loss: 0.008190670982003212\n",
      "Epoch 793 training loss: 0.00806380808353424\n",
      "Epoch 794 training loss: 0.00832232553511858\n",
      "Epoch 795 training loss: 0.008089407347142696\n",
      "Epoch 796 training loss: 0.00834847241640091\n",
      "Epoch 797 training loss: 0.008181707002222538\n",
      "Epoch 798 training loss: 0.008219531737267971\n",
      "Epoch 799 training loss: 0.008350132964551449\n",
      "Epoch 800 training loss: 0.008372977375984192\n",
      "Epoch 801 training loss: 0.00838476326316595\n",
      "Epoch 802 training loss: 0.008106423541903496\n",
      "Epoch 803 training loss: 0.008327372372150421\n",
      "Epoch 804 training loss: 0.008255809545516968\n",
      "Epoch 805 training loss: 0.008268911391496658\n",
      "Epoch 806 training loss: 0.008530613034963608\n",
      "Epoch 807 training loss: 0.00815817341208458\n",
      "Epoch 808 training loss: 0.008048356510698795\n",
      "Epoch 809 training loss: 0.008414116688072681\n",
      "Epoch 810 training loss: 0.008171522058546543\n",
      "Epoch 811 training loss: 0.008117974735796452\n",
      "Epoch 812 training loss: 0.0083633316680789\n",
      "Epoch 813 training loss: 0.008549927733838558\n",
      "Epoch 814 training loss: 0.008096208795905113\n",
      "Epoch 815 training loss: 0.008069025352597237\n",
      "Epoch 816 training loss: 0.008234530687332153\n",
      "Epoch 817 training loss: 0.008362926542758942\n",
      "Epoch 818 training loss: 0.0082937590777874\n",
      "Epoch 819 training loss: 0.00838085263967514\n",
      "Epoch 820 training loss: 0.007946819998323917\n",
      "Epoch 821 training loss: 0.008629276417195797\n",
      "Epoch 822 training loss: 0.008136161603033543\n",
      "Epoch 823 training loss: 0.008451404981315136\n",
      "Epoch 824 training loss: 0.008016875945031643\n",
      "Epoch 825 training loss: 0.008135020732879639\n",
      "Epoch 826 training loss: 0.008340690284967422\n",
      "Epoch 827 training loss: 0.008241104893386364\n",
      "Epoch 828 training loss: 0.00804860983043909\n",
      "Epoch 829 training loss: 0.008165804669260979\n",
      "Epoch 830 training loss: 0.008503119461238384\n",
      "Epoch 831 training loss: 0.008041801862418652\n",
      "Epoch 832 training loss: 0.008150472305715084\n",
      "Epoch 833 training loss: 0.008238273672759533\n",
      "Epoch 834 training loss: 0.00842207483947277\n",
      "Epoch 835 training loss: 0.00794163066893816\n",
      "Epoch 836 training loss: 0.007966366596519947\n",
      "Epoch 837 training loss: 0.008485333994030952\n",
      "Epoch 838 training loss: 0.008179837837815285\n",
      "Epoch 839 training loss: 0.008123173378407955\n",
      "Epoch 840 training loss: 0.008738611824810505\n",
      "Epoch 841 training loss: 0.00811455026268959\n",
      "Epoch 842 training loss: 0.008233245462179184\n",
      "Epoch 843 training loss: 0.008221287280321121\n",
      "Epoch 844 training loss: 0.008015399798750877\n",
      "Epoch 845 training loss: 0.00825290847569704\n",
      "Epoch 846 training loss: 0.008078740909695625\n",
      "Epoch 847 training loss: 0.00810407567769289\n",
      "Epoch 848 training loss: 0.008353499695658684\n",
      "Epoch 849 training loss: 0.007969272322952747\n",
      "Epoch 850 training loss: 0.008077025413513184\n",
      "Epoch 851 training loss: 0.008188758045434952\n",
      "Epoch 852 training loss: 0.008319087326526642\n",
      "Epoch 853 training loss: 0.008403852581977844\n",
      "Epoch 854 training loss: 0.008066114038228989\n",
      "Epoch 855 training loss: 0.00811253022402525\n",
      "Epoch 856 training loss: 0.008299460634589195\n",
      "Epoch 857 training loss: 0.008071167394518852\n",
      "Epoch 858 training loss: 0.008334613405168056\n",
      "Epoch 859 training loss: 0.008091285824775696\n",
      "Epoch 860 training loss: 0.008197973482310772\n",
      "Epoch 861 training loss: 0.008229468017816544\n",
      "Epoch 862 training loss: 0.008094945922493935\n",
      "Epoch 863 training loss: 0.007987619377672672\n",
      "Epoch 864 training loss: 0.007987672463059425\n",
      "Epoch 865 training loss: 0.00832451693713665\n",
      "Epoch 866 training loss: 0.008015799336135387\n",
      "Epoch 867 training loss: 0.008071036078035831\n",
      "Epoch 868 training loss: 0.008461899124085903\n",
      "Epoch 869 training loss: 0.007908281870186329\n",
      "Epoch 870 training loss: 0.008162085898220539\n",
      "Epoch 871 training loss: 0.0081868851557374\n",
      "Epoch 872 training loss: 0.007957461290061474\n",
      "Epoch 873 training loss: 0.008269550278782845\n",
      "Epoch 874 training loss: 0.008311313576996326\n",
      "Epoch 875 training loss: 0.008181355893611908\n",
      "Epoch 876 training loss: 0.008286547847092152\n",
      "Epoch 877 training loss: 0.00796560663729906\n",
      "Epoch 878 training loss: 0.008442847989499569\n",
      "Epoch 879 training loss: 0.0078525859862566\n",
      "Epoch 880 training loss: 0.008054917678236961\n",
      "Epoch 881 training loss: 0.00786519143730402\n",
      "Epoch 882 training loss: 0.008084520697593689\n",
      "Epoch 883 training loss: 0.00812488328665495\n",
      "Epoch 884 training loss: 0.008223351091146469\n",
      "Epoch 885 training loss: 0.008452627807855606\n",
      "Epoch 886 training loss: 0.008224216289818287\n",
      "Epoch 887 training loss: 0.008170844055712223\n",
      "Epoch 888 training loss: 0.008170673623681068\n",
      "Epoch 889 training loss: 0.008139021694660187\n",
      "Epoch 890 training loss: 0.008048996329307556\n",
      "Epoch 891 training loss: 0.007927724160254002\n",
      "Epoch 892 training loss: 0.008257443085312843\n",
      "Epoch 893 training loss: 0.007967157289385796\n",
      "Epoch 894 training loss: 0.008310025557875633\n",
      "Epoch 895 training loss: 0.008067003451287746\n",
      "Epoch 896 training loss: 0.007879507727921009\n",
      "Epoch 897 training loss: 0.008204521611332893\n",
      "Epoch 898 training loss: 0.008250783197581768\n",
      "Epoch 899 training loss: 0.008380993269383907\n",
      "Epoch 900 training loss: 0.0081655103713274\n",
      "Epoch 901 training loss: 0.007930419407784939\n",
      "Epoch 902 training loss: 0.008173671551048756\n",
      "Epoch 903 training loss: 0.008150468580424786\n",
      "Epoch 904 training loss: 0.00801034364849329\n",
      "Epoch 905 training loss: 0.008282851427793503\n",
      "Epoch 906 training loss: 0.00824336800724268\n",
      "Epoch 907 training loss: 0.008055733516812325\n",
      "Epoch 908 training loss: 0.007957198657095432\n",
      "Epoch 909 training loss: 0.008485960774123669\n",
      "Epoch 910 training loss: 0.008157143369317055\n",
      "Epoch 911 training loss: 0.007952407002449036\n",
      "Epoch 912 training loss: 0.00816657766699791\n",
      "Epoch 913 training loss: 0.00822355691343546\n",
      "Epoch 914 training loss: 0.008089002221822739\n",
      "Epoch 915 training loss: 0.008222655393183231\n",
      "Epoch 916 training loss: 0.007993419654667377\n",
      "Epoch 917 training loss: 0.007968823425471783\n",
      "Epoch 918 training loss: 0.00786726363003254\n",
      "Epoch 919 training loss: 0.008179262280464172\n",
      "Epoch 920 training loss: 0.008089500479400158\n",
      "Epoch 921 training loss: 0.008252661675214767\n",
      "Epoch 922 training loss: 0.007902967743575573\n",
      "Epoch 923 training loss: 0.008088603615760803\n",
      "Epoch 924 training loss: 0.00795393530279398\n",
      "Epoch 925 training loss: 0.00843742024153471\n",
      "Epoch 926 training loss: 0.008203946053981781\n",
      "Epoch 927 training loss: 0.008110200054943562\n",
      "Epoch 928 training loss: 0.008017758838832378\n",
      "Epoch 929 training loss: 0.007927575148642063\n",
      "Epoch 930 training loss: 0.008204332552850246\n",
      "Epoch 931 training loss: 0.008019914850592613\n",
      "Epoch 932 training loss: 0.007878653705120087\n",
      "Epoch 933 training loss: 0.008142504841089249\n",
      "Epoch 934 training loss: 0.00791613943874836\n",
      "Epoch 935 training loss: 0.008241127245128155\n",
      "Epoch 936 training loss: 0.007954247295856476\n",
      "Epoch 937 training loss: 0.008061355911195278\n",
      "Epoch 938 training loss: 0.008000930771231651\n",
      "Epoch 939 training loss: 0.00848045852035284\n",
      "Epoch 940 training loss: 0.007844259962439537\n",
      "Epoch 941 training loss: 0.008175992406904697\n",
      "Epoch 942 training loss: 0.008373628370463848\n",
      "Epoch 943 training loss: 0.00788891687989235\n",
      "Epoch 944 training loss: 0.007885022088885307\n",
      "Epoch 945 training loss: 0.008033249527215958\n",
      "Epoch 946 training loss: 0.008127689361572266\n",
      "Epoch 947 training loss: 0.007993209175765514\n",
      "Epoch 948 training loss: 0.008069591596722603\n",
      "Epoch 949 training loss: 0.008113004267215729\n",
      "Epoch 950 training loss: 0.008223136886954308\n",
      "Epoch 951 training loss: 0.0080986637622118\n",
      "Epoch 952 training loss: 0.008567364886403084\n",
      "Epoch 953 training loss: 0.007925034500658512\n",
      "Epoch 954 training loss: 0.007787894457578659\n",
      "Epoch 955 training loss: 0.008008666336536407\n",
      "Epoch 956 training loss: 0.008018103428184986\n",
      "Epoch 957 training loss: 0.008013835176825523\n",
      "Epoch 958 training loss: 0.008552249521017075\n",
      "Epoch 959 training loss: 0.008167658932507038\n",
      "Epoch 960 training loss: 0.00791058037430048\n",
      "Epoch 961 training loss: 0.00810912624001503\n",
      "Epoch 962 training loss: 0.007888155989348888\n",
      "Epoch 963 training loss: 0.008090468123555183\n",
      "Epoch 964 training loss: 0.007987131364643574\n",
      "Epoch 965 training loss: 0.00822514109313488\n",
      "Epoch 966 training loss: 0.007906931452453136\n",
      "Epoch 967 training loss: 0.00786307081580162\n",
      "Epoch 968 training loss: 0.00808698683977127\n",
      "Epoch 969 training loss: 0.008557235822081566\n",
      "Epoch 970 training loss: 0.008153512142598629\n",
      "Epoch 971 training loss: 0.00804845616221428\n",
      "Epoch 972 training loss: 0.00796901248395443\n",
      "Epoch 973 training loss: 0.007923468016088009\n",
      "Epoch 974 training loss: 0.008047785609960556\n",
      "Epoch 975 training loss: 0.007918903604149818\n",
      "Epoch 976 training loss: 0.008550225757062435\n",
      "Epoch 977 training loss: 0.007977095432579517\n",
      "Epoch 978 training loss: 0.007885240018367767\n",
      "Epoch 979 training loss: 0.00797110516577959\n",
      "Epoch 980 training loss: 0.007930153049528599\n",
      "Epoch 981 training loss: 0.007779119536280632\n",
      "Epoch 982 training loss: 0.007876764051616192\n",
      "Epoch 983 training loss: 0.008316676132380962\n",
      "Epoch 984 training loss: 0.007886066101491451\n",
      "Epoch 985 training loss: 0.008085246197879314\n",
      "Epoch 986 training loss: 0.008012279868125916\n",
      "Epoch 987 training loss: 0.00818746816366911\n",
      "Epoch 988 training loss: 0.007836793549358845\n",
      "Epoch 989 training loss: 0.007961583323776722\n",
      "Epoch 990 training loss: 0.007888180203735828\n",
      "Epoch 991 training loss: 0.007990285754203796\n",
      "Epoch 992 training loss: 0.007929353043437004\n",
      "Epoch 993 training loss: 0.008198077790439129\n",
      "Epoch 994 training loss: 0.007890029810369015\n",
      "Epoch 995 training loss: 0.008147367276251316\n",
      "Epoch 996 training loss: 0.007968208752572536\n",
      "Epoch 997 training loss: 0.00807669572532177\n",
      "Epoch 998 training loss: 0.007986797019839287\n",
      "Epoch 999 training loss: 0.00827275775372982\n",
      "Epoch 1000 training loss: 0.008104848675429821\n",
      "Epoch 1001 training loss: 0.007799683604389429\n",
      "Epoch 1002 training loss: 0.008114656433463097\n",
      "Epoch 1003 training loss: 0.007744221016764641\n",
      "Epoch 1004 training loss: 0.008463077247142792\n",
      "Epoch 1005 training loss: 0.007965967990458012\n",
      "Epoch 1006 training loss: 0.00810154527425766\n",
      "Epoch 1007 training loss: 0.008131440728902817\n",
      "Epoch 1008 training loss: 0.007962842471897602\n",
      "Epoch 1009 training loss: 0.007958635687828064\n",
      "Epoch 1010 training loss: 0.007896624505519867\n",
      "Epoch 1011 training loss: 0.008353947661817074\n",
      "Epoch 1012 training loss: 0.007967730984091759\n",
      "Epoch 1013 training loss: 0.007838782854378223\n",
      "Epoch 1014 training loss: 0.008032127283513546\n",
      "Epoch 1015 training loss: 0.00811082310974598\n",
      "Epoch 1016 training loss: 0.007881744764745235\n",
      "Epoch 1017 training loss: 0.007972819730639458\n",
      "Epoch 1018 training loss: 0.008037892170250416\n",
      "Epoch 1019 training loss: 0.008198486641049385\n",
      "Epoch 1020 training loss: 0.00777027290314436\n",
      "Epoch 1021 training loss: 0.007727242540568113\n",
      "Epoch 1022 training loss: 0.007845277898013592\n",
      "Epoch 1023 training loss: 0.008046802133321762\n",
      "Epoch 1024 training loss: 0.008339457213878632\n",
      "0.6751824817518248\n",
      "[[6474 1545]\n",
      " [1926  741]]\n",
      "Accuracy: 0.6751824817518248\n",
      "Precision:  0.3241469816272966\n",
      "Recall:  0.2778402699662542\n",
      "F1:  0.2992125984251969\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=1024, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.001, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.06506173312664032\n",
      "Epoch 2 training loss: 0.07763776928186417\n",
      "Epoch 3 training loss: 0.07750710844993591\n",
      "Epoch 4 training loss: 0.07750604301691055\n",
      "Epoch 5 training loss: 0.0775478407740593\n",
      "Epoch 6 training loss: 0.07753785699605942\n",
      "Epoch 7 training loss: 0.07755934447050095\n",
      "Epoch 8 training loss: 0.077571801841259\n",
      "Epoch 9 training loss: 0.07756922394037247\n",
      "Epoch 10 training loss: 0.07756340503692627\n",
      "Epoch 11 training loss: 0.07754787057638168\n",
      "Epoch 12 training loss: 0.07752231508493423\n",
      "Epoch 13 training loss: 0.07755836099386215\n",
      "Epoch 14 training loss: 0.07758618146181107\n",
      "Epoch 15 training loss: 0.07756387442350388\n",
      "Epoch 16 training loss: 0.07757235318422318\n",
      "Epoch 17 training loss: 0.07750573009252548\n",
      "Epoch 18 training loss: 0.07753414660692215\n",
      "Epoch 19 training loss: 0.07756569981575012\n",
      "Epoch 20 training loss: 0.07760342955589294\n",
      "Epoch 21 training loss: 0.07752853631973267\n",
      "Epoch 22 training loss: 0.07759113609790802\n",
      "Epoch 23 training loss: 0.0775730237364769\n",
      "Epoch 24 training loss: 0.07748489081859589\n",
      "Epoch 25 training loss: 0.07759560644626617\n",
      "Epoch 26 training loss: 0.07760005444288254\n",
      "Epoch 27 training loss: 0.07755468040704727\n",
      "Epoch 28 training loss: 0.07757961750030518\n",
      "Epoch 29 training loss: 0.07765664905309677\n",
      "Epoch 30 training loss: 0.07753808051347733\n",
      "Epoch 31 training loss: 0.07755281776189804\n",
      "Epoch 32 training loss: 0.07756466418504715\n",
      "Epoch 33 training loss: 0.07744160294532776\n",
      "Epoch 34 training loss: 0.07760224491357803\n",
      "Epoch 35 training loss: 0.07759104669094086\n",
      "Epoch 36 training loss: 0.0775335431098938\n",
      "Epoch 37 training loss: 0.07760794460773468\n",
      "Epoch 38 training loss: 0.07758035510778427\n",
      "Epoch 39 training loss: 0.07764425128698349\n",
      "Epoch 40 training loss: 0.07746368646621704\n",
      "Epoch 41 training loss: 0.07755619287490845\n",
      "Epoch 42 training loss: 0.0775802955031395\n",
      "Epoch 43 training loss: 0.07760118693113327\n",
      "Epoch 44 training loss: 0.07758918404579163\n",
      "Epoch 45 training loss: 0.07755822688341141\n",
      "Epoch 46 training loss: 0.07765565812587738\n",
      "Epoch 47 training loss: 0.07766027003526688\n",
      "Epoch 48 training loss: 0.07756874710321426\n",
      "Epoch 49 training loss: 0.07753007113933563\n",
      "Epoch 50 training loss: 0.07750634849071503\n",
      "Epoch 51 training loss: 0.07748124748468399\n",
      "Epoch 52 training loss: 0.07757296413183212\n",
      "Epoch 53 training loss: 0.07763794809579849\n",
      "Epoch 54 training loss: 0.07752776890993118\n",
      "Epoch 55 training loss: 0.0775943249464035\n",
      "Epoch 56 training loss: 0.07758201658725739\n",
      "Epoch 57 training loss: 0.07751638442277908\n",
      "Epoch 58 training loss: 0.07758913934230804\n",
      "Epoch 59 training loss: 0.07758311927318573\n",
      "Epoch 60 training loss: 0.07748141139745712\n",
      "Epoch 61 training loss: 0.0774914026260376\n",
      "Epoch 62 training loss: 0.07765224575996399\n",
      "Epoch 63 training loss: 0.07758308947086334\n",
      "Epoch 64 training loss: 0.0774996429681778\n",
      "0.7880404267265582\n",
      "[[7219  800]\n",
      " [1465 1202]]\n",
      "Accuracy: 0.7880404267265582\n",
      "Precision:  0.6003996003996004\n",
      "Recall:  0.4506936632920885\n",
      "F1:  0.5148854144356393\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=64, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.66,0.60,0.72 ,0.61 ,0.80]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['blue', 'darkred', 'forestgreen', 'darkgoldenrod', 'darkgreen'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for algorithms with Recall\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "algorithm_names = [\"Local Outlier Factor\", \"Isolation Forest\", \"One-Class SVM\", \"LSTM Autoencoder\", \"Autoencoder\"]\n",
    "\n",
    "metric_values = [0.95, 0.19, 0.84, 0.92, 0.44]\n",
    "\n",
    "plt.bar(algorithm_names, metric_values, color=['blue', 'darkred', 'forestgreen', 'darkgoldenrod', 'darkgreen'])\n",
    "plt.xlabel('Anomaly Detection Models')\n",
    "plt.ylabel('Recall Metric')\n",
    "plt.title('Performance Comparison of Different Anomaly Detection Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07836276292800903\n",
      "Epoch 2 training loss: 0.07754524797201157\n",
      "Epoch 3 training loss: 0.07745422422885895\n",
      "Epoch 4 training loss: 0.07746409624814987\n",
      "Epoch 5 training loss: 0.07742595672607422\n",
      "Epoch 6 training loss: 0.07742536813020706\n",
      "Epoch 7 training loss: 0.07751432806253433\n",
      "Epoch 8 training loss: 0.0775236040353775\n",
      "Epoch 9 training loss: 0.07753631472587585\n",
      "Epoch 10 training loss: 0.0775417909026146\n",
      "Epoch 11 training loss: 0.07748011499643326\n",
      "Epoch 12 training loss: 0.0774666965007782\n",
      "Epoch 13 training loss: 0.07744150608778\n",
      "Epoch 14 training loss: 0.07743609696626663\n",
      "Epoch 15 training loss: 0.07757815718650818\n",
      "Epoch 16 training loss: 0.07755087316036224\n",
      "Epoch 17 training loss: 0.07752357423305511\n",
      "Epoch 18 training loss: 0.07744619250297546\n",
      "Epoch 19 training loss: 0.07757273316383362\n",
      "Epoch 20 training loss: 0.07744909822940826\n",
      "Epoch 21 training loss: 0.07758618891239166\n",
      "Epoch 22 training loss: 0.07743437588214874\n",
      "Epoch 23 training loss: 0.07756263762712479\n",
      "Epoch 24 training loss: 0.07752298563718796\n",
      "Epoch 25 training loss: 0.07752542942762375\n",
      "Epoch 26 training loss: 0.07756495475769043\n",
      "Epoch 27 training loss: 0.07750868797302246\n",
      "Epoch 28 training loss: 0.07753882557153702\n",
      "Epoch 29 training loss: 0.07749243080615997\n",
      "Epoch 30 training loss: 0.07750589400529861\n",
      "Epoch 31 training loss: 0.0774526298046112\n",
      "Epoch 32 training loss: 0.07752778381109238\n",
      "Epoch 33 training loss: 0.07742828130722046\n",
      "Epoch 34 training loss: 0.07756347954273224\n",
      "Epoch 35 training loss: 0.07752636820077896\n",
      "Epoch 36 training loss: 0.07749959826469421\n",
      "Epoch 37 training loss: 0.07742299139499664\n",
      "Epoch 38 training loss: 0.07745842635631561\n",
      "Epoch 39 training loss: 0.0775073692202568\n",
      "Epoch 40 training loss: 0.07751455157995224\n",
      "Epoch 41 training loss: 0.07746180891990662\n",
      "Epoch 42 training loss: 0.07754518836736679\n",
      "Epoch 43 training loss: 0.07750449329614639\n",
      "Epoch 44 training loss: 0.07747507840394974\n",
      "Epoch 45 training loss: 0.0774766057729721\n",
      "Epoch 46 training loss: 0.07754531502723694\n",
      "Epoch 47 training loss: 0.07754786312580109\n",
      "Epoch 48 training loss: 0.07752633094787598\n",
      "Epoch 49 training loss: 0.07751888781785965\n",
      "Epoch 50 training loss: 0.07747573405504227\n",
      "Epoch 51 training loss: 0.0775204673409462\n",
      "Epoch 52 training loss: 0.07747606933116913\n",
      "Epoch 53 training loss: 0.0774846151471138\n",
      "Epoch 54 training loss: 0.07765131443738937\n",
      "Epoch 55 training loss: 0.07753846049308777\n",
      "Epoch 56 training loss: 0.07741639763116837\n",
      "Epoch 57 training loss: 0.07751066237688065\n",
      "Epoch 58 training loss: 0.07752508670091629\n",
      "Epoch 59 training loss: 0.07754455506801605\n",
      "Epoch 60 training loss: 0.07749159634113312\n",
      "Epoch 61 training loss: 0.07746665179729462\n",
      "Epoch 62 training loss: 0.07758487015962601\n",
      "Epoch 63 training loss: 0.07753114402294159\n",
      "Epoch 64 training loss: 0.07749012112617493\n",
      "Epoch 65 training loss: 0.07751606404781342\n",
      "Epoch 66 training loss: 0.07754688709974289\n",
      "Epoch 67 training loss: 0.07752802968025208\n",
      "Epoch 68 training loss: 0.07753319293260574\n",
      "Epoch 69 training loss: 0.07754796743392944\n",
      "Epoch 70 training loss: 0.07742898911237717\n",
      "Epoch 71 training loss: 0.07756330072879791\n",
      "Epoch 72 training loss: 0.07753963768482208\n",
      "Epoch 73 training loss: 0.0775553435087204\n",
      "Epoch 74 training loss: 0.07745508849620819\n",
      "Epoch 75 training loss: 0.07741343230009079\n",
      "Epoch 76 training loss: 0.07757149636745453\n",
      "Epoch 77 training loss: 0.07751405984163284\n",
      "Epoch 78 training loss: 0.07746954262256622\n",
      "Epoch 79 training loss: 0.07744664698839188\n",
      "Epoch 80 training loss: 0.07746455073356628\n",
      "Epoch 81 training loss: 0.07753638178110123\n",
      "Epoch 82 training loss: 0.07749690115451813\n",
      "Epoch 83 training loss: 0.077464260160923\n",
      "Epoch 84 training loss: 0.07753986865282059\n",
      "Epoch 85 training loss: 0.07742704451084137\n",
      "Epoch 86 training loss: 0.07747697085142136\n",
      "Epoch 87 training loss: 0.0775236040353775\n",
      "Epoch 88 training loss: 0.07745207101106644\n",
      "Epoch 89 training loss: 0.0774829089641571\n",
      "Epoch 90 training loss: 0.07758104056119919\n",
      "Epoch 91 training loss: 0.0774734690785408\n",
      "Epoch 92 training loss: 0.07754381000995636\n",
      "Epoch 93 training loss: 0.07755278795957565\n",
      "Epoch 94 training loss: 0.07750848680734634\n",
      "Epoch 95 training loss: 0.07762368768453598\n",
      "Epoch 96 training loss: 0.07752431929111481\n",
      "Epoch 97 training loss: 0.0774645060300827\n",
      "Epoch 98 training loss: 0.07745029032230377\n",
      "Epoch 99 training loss: 0.07766041904687881\n",
      "Epoch 100 training loss: 0.07756766676902771\n",
      "Epoch 101 training loss: 0.07752783596515656\n",
      "Epoch 102 training loss: 0.07756087183952332\n",
      "Epoch 103 training loss: 0.0775577574968338\n",
      "Epoch 104 training loss: 0.0775250792503357\n",
      "Epoch 105 training loss: 0.0774771049618721\n",
      "Epoch 106 training loss: 0.0775931254029274\n",
      "Epoch 107 training loss: 0.07745718955993652\n",
      "Epoch 108 training loss: 0.07746520638465881\n",
      "Epoch 109 training loss: 0.07755742967128754\n",
      "Epoch 110 training loss: 0.07747315615415573\n",
      "Epoch 111 training loss: 0.07748185843229294\n",
      "Epoch 112 training loss: 0.07748344540596008\n",
      "Epoch 113 training loss: 0.07749829441308975\n",
      "Epoch 114 training loss: 0.07748587429523468\n",
      "Epoch 115 training loss: 0.07752258330583572\n",
      "Epoch 116 training loss: 0.07752960175275803\n",
      "Epoch 117 training loss: 0.07753312587738037\n",
      "Epoch 118 training loss: 0.07755168527364731\n",
      "Epoch 119 training loss: 0.077485591173172\n",
      "Epoch 120 training loss: 0.07754621654748917\n",
      "Epoch 121 training loss: 0.07759495079517365\n",
      "Epoch 122 training loss: 0.07758749276399612\n",
      "Epoch 123 training loss: 0.07742922008037567\n",
      "Epoch 124 training loss: 0.07758192718029022\n",
      "Epoch 125 training loss: 0.07751648873090744\n",
      "Epoch 126 training loss: 0.07739662379026413\n",
      "Epoch 127 training loss: 0.07750488817691803\n",
      "Epoch 128 training loss: 0.07753628492355347\n",
      "0.7885083286543141\n",
      "[[7254  765]\n",
      " [1495 1172]]\n",
      "Accuracy: 0.7885083286543141\n",
      "Precision:  0.6050593701600413\n",
      "Recall:  0.4394450693663292\n",
      "F1:  0.5091225021720244\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=128, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.07849182188510895\n",
      "Epoch 2 training loss: 0.0776526927947998\n",
      "Epoch 3 training loss: 0.07778098434209824\n",
      "Epoch 4 training loss: 0.07774022221565247\n",
      "Epoch 5 training loss: 0.0776030570268631\n",
      "Epoch 6 training loss: 0.07779345661401749\n",
      "Epoch 7 training loss: 0.0777362510561943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m autoenc \u001B[38;5;241m=\u001B[39m Autoencoder([\u001B[38;5;241m96\u001B[39m, \u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m24\u001B[39m, \u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m8\u001B[39m], [\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m24\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m96\u001B[39m], nn\u001B[38;5;241m.\u001B[39mReLU(), nn\u001B[38;5;241m.\u001B[39mSigmoid())\n\u001B[0;32m      5\u001B[0m hp \u001B[38;5;241m=\u001B[39m Hparams(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, anomaly_generation_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, clean_test_data_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, window_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, window_slide\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, one_hot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 6\u001B[0m (avg, cm) \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_train_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoenc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(avg)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(cm)\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\multi_modal_edge_ai\\models\\anomaly_detection\\train_and_eval\\model_validator.py:65\u001B[0m, in \u001B[0;36mmodel_train_eval\u001B[1;34m(model, data, hparams)\u001B[0m\n\u001B[0;32m     61\u001B[0m normalized_testing_df \u001B[38;5;241m=\u001B[39m dataframe_standard_scaling(numeric_testing_df, n_features_adl)\n\u001B[0;32m     63\u001B[0m normalized_training_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(ADLDataset(normalized_training_df), hparams\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 65\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(normalized_training_dataloader, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mvars\u001B[39m(hparams))\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mset_reconstruction_error_threshold\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     68\u001B[0m     model\u001B[38;5;241m.\u001B[39mset_reconstruction_error_threshold(hparams\u001B[38;5;241m.\u001B[39mreconstruction_error_quantile)\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\multi_modal_edge_ai\\models\\anomaly_detection\\ml_models\\autoencoder.py:61\u001B[0m, in \u001B[0;36mAutoencoder.train\u001B[1;34m(self, data, **hyperparams)\u001B[0m\n\u001B[0;32m     59\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     60\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 61\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m     epoch_training_loss\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m     64\u001B[0m curr_reconstruction_errors \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m epoch_training_loss\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\venv\\lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    133\u001B[0m         group,\n\u001B[0;32m    134\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    139\u001B[0m         state_steps)\n\u001B[1;32m--> 141\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\venv\\lib\\site-packages\\torch\\optim\\adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 281\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\TUD\\Y2\\Quarter 4- SP\\Multi-Modal Edge-AI\\venv\\lib\\site-packages\\torch\\optim\\adam.py:335\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    332\u001B[0m step_t \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight_decay \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 335\u001B[0m     grad \u001B[38;5;241m=\u001B[39m \u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(param):\n\u001B[0;32m    338\u001B[0m     grad \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(grad)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=16, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=True, reconstruction_error_quantile= 0.99)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T11:27:29.314553200Z",
     "start_time": "2023-06-15T11:22:36.463901600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "autoenc.save('anomaly_detection_model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T10:07:56.204615300Z",
     "start_time": "2023-06-15T10:07:56.165615300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "5\n",
      "Epoch 1 training loss: 0.07032772153615952\n",
      "Epoch 2 training loss: 0.07013164460659027\n",
      "Epoch 3 training loss: 0.06991345435380936\n",
      "Epoch 4 training loss: 0.07010791450738907\n",
      "Epoch 5 training loss: 0.07016096264123917\n",
      "Epoch 6 training loss: 0.07004042714834213\n",
      "Epoch 7 training loss: 0.07026807963848114\n",
      "Epoch 8 training loss: 0.07008042931556702\n",
      "Epoch 9 training loss: 0.07007605582475662\n",
      "Epoch 10 training loss: 0.07015755027532578\n",
      "Epoch 11 training loss: 0.07003292441368103\n",
      "Epoch 12 training loss: 0.0701369196176529\n",
      "Epoch 13 training loss: 0.07015486061573029\n",
      "Epoch 14 training loss: 0.07008297741413116\n",
      "Epoch 15 training loss: 0.0702085793018341\n",
      "Epoch 16 training loss: 0.07004798948764801\n",
      "Epoch 17 training loss: 0.07002263516187668\n",
      "Epoch 18 training loss: 0.07004742324352264\n",
      "Epoch 19 training loss: 0.07024405896663666\n",
      "Epoch 20 training loss: 0.07009511440992355\n",
      "Epoch 21 training loss: 0.07013522833585739\n",
      "Epoch 22 training loss: 0.07002323120832443\n",
      "Epoch 23 training loss: 0.07006151229143143\n",
      "Epoch 24 training loss: 0.0702165812253952\n",
      "Epoch 25 training loss: 0.07007678598165512\n",
      "Epoch 26 training loss: 0.07005652040243149\n",
      "Epoch 27 training loss: 0.07005822658538818\n",
      "Epoch 28 training loss: 0.07014302164316177\n",
      "Epoch 29 training loss: 0.07009068131446838\n",
      "Epoch 30 training loss: 0.07013352960348129\n",
      "Epoch 31 training loss: 0.070038802921772\n",
      "Epoch 32 training loss: 0.07014951854944229\n",
      "Epoch 33 training loss: 0.07018934935331345\n",
      "Epoch 34 training loss: 0.0701412707567215\n",
      "Epoch 35 training loss: 0.07001311331987381\n",
      "Epoch 36 training loss: 0.07018197327852249\n",
      "Epoch 37 training loss: 0.07011280953884125\n",
      "Epoch 38 training loss: 0.07002485543489456\n",
      "Epoch 39 training loss: 0.06997409462928772\n",
      "Epoch 40 training loss: 0.07003915309906006\n",
      "Epoch 41 training loss: 0.07016520947217941\n",
      "Epoch 42 training loss: 0.070044606924057\n",
      "Epoch 43 training loss: 0.07006324827671051\n",
      "Epoch 44 training loss: 0.07006672769784927\n",
      "Epoch 45 training loss: 0.07016246765851974\n",
      "Epoch 46 training loss: 0.07017835974693298\n",
      "Epoch 47 training loss: 0.0700092762708664\n",
      "Epoch 48 training loss: 0.07006949931383133\n",
      "Epoch 49 training loss: 0.07001554220914841\n",
      "Epoch 50 training loss: 0.070020891726017\n",
      "Epoch 51 training loss: 0.07002243399620056\n",
      "Epoch 52 training loss: 0.07005273550748825\n",
      "Epoch 53 training loss: 0.07006140053272247\n",
      "Epoch 54 training loss: 0.07006537169218063\n",
      "Epoch 55 training loss: 0.06995023787021637\n",
      "Epoch 56 training loss: 0.06996390223503113\n",
      "Epoch 57 training loss: 0.07003174722194672\n",
      "Epoch 58 training loss: 0.07006654143333435\n",
      "Epoch 59 training loss: 0.06999827176332474\n",
      "Epoch 60 training loss: 0.07009550929069519\n",
      "Epoch 61 training loss: 0.06996416300535202\n",
      "Epoch 62 training loss: 0.07025374472141266\n",
      "Epoch 63 training loss: 0.07017910480499268\n",
      "Epoch 64 training loss: 0.07017335295677185\n",
      "Epoch 65 training loss: 0.07019491493701935\n",
      "Epoch 66 training loss: 0.07011090964078903\n",
      "Epoch 67 training loss: 0.07006177306175232\n",
      "Epoch 68 training loss: 0.07005617767572403\n",
      "Epoch 69 training loss: 0.07005568593740463\n",
      "Epoch 70 training loss: 0.07001142203807831\n",
      "Epoch 71 training loss: 0.07001015543937683\n",
      "Epoch 72 training loss: 0.07013397663831711\n",
      "Epoch 73 training loss: 0.0701632872223854\n",
      "Epoch 74 training loss: 0.06998487561941147\n",
      "Epoch 75 training loss: 0.07013306021690369\n",
      "Epoch 76 training loss: 0.07019234448671341\n",
      "Epoch 77 training loss: 0.07012191414833069\n",
      "Epoch 78 training loss: 0.07012765109539032\n",
      "Epoch 79 training loss: 0.07005806267261505\n",
      "Epoch 80 training loss: 0.07000092417001724\n",
      "Epoch 81 training loss: 0.07001638412475586\n",
      "Epoch 82 training loss: 0.07009565830230713\n",
      "Epoch 83 training loss: 0.06999500095844269\n",
      "Epoch 84 training loss: 0.07008449733257294\n",
      "Epoch 85 training loss: 0.06995122879743576\n",
      "Epoch 86 training loss: 0.07005976140499115\n",
      "Epoch 87 training loss: 0.07007959485054016\n",
      "Epoch 88 training loss: 0.07007542252540588\n",
      "Epoch 89 training loss: 0.07002770155668259\n",
      "Epoch 90 training loss: 0.07012353837490082\n",
      "Epoch 91 training loss: 0.07008837163448334\n",
      "Epoch 92 training loss: 0.07004393637180328\n",
      "Epoch 93 training loss: 0.07022172957658768\n",
      "Epoch 94 training loss: 0.07002388685941696\n",
      "Epoch 95 training loss: 0.07004546374082565\n",
      "Epoch 96 training loss: 0.06999761611223221\n",
      "Epoch 97 training loss: 0.07017096877098083\n",
      "Epoch 98 training loss: 0.07014700770378113\n",
      "Epoch 99 training loss: 0.0701674148440361\n",
      "Epoch 100 training loss: 0.07007214426994324\n",
      "Epoch 101 training loss: 0.07001900672912598\n",
      "Epoch 102 training loss: 0.07003632932901382\n",
      "Epoch 103 training loss: 0.07007000595331192\n",
      "Epoch 104 training loss: 0.07006864249706268\n",
      "Epoch 105 training loss: 0.07010043412446976\n",
      "Epoch 106 training loss: 0.07004006206989288\n",
      "Epoch 107 training loss: 0.07001563161611557\n",
      "Epoch 108 training loss: 0.07004563510417938\n",
      "Epoch 109 training loss: 0.07022657990455627\n",
      "Epoch 110 training loss: 0.07007276266813278\n",
      "Epoch 111 training loss: 0.07013190537691116\n",
      "Epoch 112 training loss: 0.07008086144924164\n",
      "Epoch 113 training loss: 0.07014445960521698\n",
      "Epoch 114 training loss: 0.07003936916589737\n",
      "Epoch 115 training loss: 0.07003054767847061\n",
      "Epoch 116 training loss: 0.07018356770277023\n",
      "Epoch 117 training loss: 0.06997079402208328\n",
      "Epoch 118 training loss: 0.07005728781223297\n",
      "Epoch 119 training loss: 0.07004450261592865\n",
      "Epoch 120 training loss: 0.07006915658712387\n",
      "Epoch 121 training loss: 0.07014858722686768\n",
      "Epoch 122 training loss: 0.06993570178747177\n",
      "Epoch 123 training loss: 0.07016632705926895\n",
      "Epoch 124 training loss: 0.0700722187757492\n",
      "Epoch 125 training loss: 0.0700228363275528\n",
      "Epoch 126 training loss: 0.07020872086286545\n",
      "Epoch 127 training loss: 0.0702405571937561\n",
      "Epoch 128 training loss: 0.07002212107181549\n",
      "Epoch 129 training loss: 0.07011671364307404\n",
      "Epoch 130 training loss: 0.07012967765331268\n",
      "Epoch 131 training loss: 0.07003208994865417\n",
      "Epoch 132 training loss: 0.07009629905223846\n",
      "Epoch 133 training loss: 0.07006313651800156\n",
      "Epoch 134 training loss: 0.0699814036488533\n",
      "Epoch 135 training loss: 0.07020781934261322\n",
      "Epoch 136 training loss: 0.07011153548955917\n",
      "Epoch 137 training loss: 0.07003903388977051\n",
      "Epoch 138 training loss: 0.07007252424955368\n",
      "Epoch 139 training loss: 0.0701291561126709\n",
      "Epoch 140 training loss: 0.07015199214220047\n",
      "Epoch 141 training loss: 0.07018566131591797\n",
      "Epoch 142 training loss: 0.07003425806760788\n",
      "Epoch 143 training loss: 0.07000088691711426\n",
      "Epoch 144 training loss: 0.07013106346130371\n",
      "Epoch 145 training loss: 0.07008533179759979\n",
      "Epoch 146 training loss: 0.07004563510417938\n",
      "Epoch 147 training loss: 0.07007628679275513\n",
      "Epoch 148 training loss: 0.0700380951166153\n",
      "Epoch 149 training loss: 0.07006755471229553\n",
      "Epoch 150 training loss: 0.07020555436611176\n",
      "Epoch 151 training loss: 0.07014203816652298\n",
      "Epoch 152 training loss: 0.07002650201320648\n",
      "Epoch 153 training loss: 0.07005098462104797\n",
      "Epoch 154 training loss: 0.07008646428585052\n",
      "Epoch 155 training loss: 0.07013063132762909\n",
      "Epoch 156 training loss: 0.07014114409685135\n",
      "Epoch 157 training loss: 0.0700058788061142\n",
      "Epoch 158 training loss: 0.0700007900595665\n",
      "Epoch 159 training loss: 0.07015125453472137\n",
      "Epoch 160 training loss: 0.06996535509824753\n",
      "Epoch 161 training loss: 0.07001829147338867\n",
      "Epoch 162 training loss: 0.07020443677902222\n",
      "Epoch 163 training loss: 0.07003679871559143\n",
      "Epoch 164 training loss: 0.0699693113565445\n",
      "Epoch 165 training loss: 0.06999295204877853\n",
      "Epoch 166 training loss: 0.07010062783956528\n",
      "Epoch 167 training loss: 0.0701887309551239\n",
      "Epoch 168 training loss: 0.07006553560495377\n",
      "Epoch 169 training loss: 0.07019074261188507\n",
      "Epoch 170 training loss: 0.07005967944860458\n",
      "Epoch 171 training loss: 0.07015010714530945\n",
      "Epoch 172 training loss: 0.07021986693143845\n",
      "Epoch 173 training loss: 0.07012948393821716\n",
      "Epoch 174 training loss: 0.07003277540206909\n",
      "Epoch 175 training loss: 0.07014679908752441\n",
      "Epoch 176 training loss: 0.06998889893293381\n",
      "Epoch 177 training loss: 0.0700836256146431\n",
      "Epoch 178 training loss: 0.07014229148626328\n",
      "Epoch 179 training loss: 0.07000496238470078\n",
      "Epoch 180 training loss: 0.07008130103349686\n",
      "Epoch 181 training loss: 0.06997469067573547\n",
      "Epoch 182 training loss: 0.07001160830259323\n",
      "Epoch 183 training loss: 0.07002659142017365\n",
      "Epoch 184 training loss: 0.07007187604904175\n",
      "Epoch 185 training loss: 0.07005686312913895\n",
      "Epoch 186 training loss: 0.07002206891775131\n",
      "Epoch 187 training loss: 0.06999912858009338\n",
      "Epoch 188 training loss: 0.07019496709108353\n",
      "Epoch 189 training loss: 0.07001138478517532\n",
      "Epoch 190 training loss: 0.07009170204401016\n",
      "Epoch 191 training loss: 0.06999038904905319\n",
      "Epoch 192 training loss: 0.07000315189361572\n",
      "Epoch 193 training loss: 0.07003144174814224\n",
      "Epoch 194 training loss: 0.07009423524141312\n",
      "Epoch 195 training loss: 0.07001618295907974\n",
      "Epoch 196 training loss: 0.07014770805835724\n",
      "Epoch 197 training loss: 0.07011939585208893\n",
      "Epoch 198 training loss: 0.07003352046012878\n",
      "Epoch 199 training loss: 0.07004771381616592\n",
      "Epoch 200 training loss: 0.07009992748498917\n",
      "Epoch 201 training loss: 0.07004817575216293\n",
      "Epoch 202 training loss: 0.06994438916444778\n",
      "Epoch 203 training loss: 0.07018636167049408\n",
      "Epoch 204 training loss: 0.06996544450521469\n",
      "Epoch 205 training loss: 0.07006680220365524\n",
      "Epoch 206 training loss: 0.07007493078708649\n",
      "Epoch 207 training loss: 0.06989386677742004\n",
      "Epoch 208 training loss: 0.07018058747053146\n",
      "Epoch 209 training loss: 0.07006572932004929\n",
      "Epoch 210 training loss: 0.07010643184185028\n",
      "Epoch 211 training loss: 0.07017384469509125\n",
      "Epoch 212 training loss: 0.07010799646377563\n",
      "Epoch 213 training loss: 0.06995140016078949\n",
      "Epoch 214 training loss: 0.0701102763414383\n",
      "Epoch 215 training loss: 0.07007811963558197\n",
      "Epoch 216 training loss: 0.07003269344568253\n",
      "Epoch 217 training loss: 0.06999655812978745\n",
      "Epoch 218 training loss: 0.07013523578643799\n",
      "Epoch 219 training loss: 0.07003766298294067\n",
      "Epoch 220 training loss: 0.07019294798374176\n",
      "Epoch 221 training loss: 0.07018455862998962\n",
      "Epoch 222 training loss: 0.07000668346881866\n",
      "Epoch 223 training loss: 0.07019682228565216\n",
      "Epoch 224 training loss: 0.07004459202289581\n",
      "Epoch 225 training loss: 0.07007469236850739\n",
      "Epoch 226 training loss: 0.0701286643743515\n",
      "Epoch 227 training loss: 0.0700114369392395\n",
      "Epoch 228 training loss: 0.06999912858009338\n",
      "Epoch 229 training loss: 0.0700727179646492\n",
      "Epoch 230 training loss: 0.07018620520830154\n",
      "Epoch 231 training loss: 0.07015638798475266\n",
      "Epoch 232 training loss: 0.06994199007749557\n",
      "Epoch 233 training loss: 0.07000899314880371\n",
      "Epoch 234 training loss: 0.07010889053344727\n",
      "Epoch 235 training loss: 0.07008278369903564\n",
      "Epoch 236 training loss: 0.07006776332855225\n",
      "Epoch 237 training loss: 0.0700656846165657\n",
      "Epoch 238 training loss: 0.07016697525978088\n",
      "Epoch 239 training loss: 0.07003984600305557\n",
      "Epoch 240 training loss: 0.07009788602590561\n",
      "Epoch 241 training loss: 0.07022017985582352\n",
      "Epoch 242 training loss: 0.07007844746112823\n",
      "Epoch 243 training loss: 0.0700104609131813\n",
      "Epoch 244 training loss: 0.07003575563430786\n",
      "Epoch 245 training loss: 0.07017925381660461\n",
      "Epoch 246 training loss: 0.06999248266220093\n",
      "Epoch 247 training loss: 0.07006275653839111\n",
      "Epoch 248 training loss: 0.0700029730796814\n",
      "Epoch 249 training loss: 0.06994545459747314\n",
      "Epoch 250 training loss: 0.07008551061153412\n",
      "Epoch 251 training loss: 0.07017181068658829\n",
      "Epoch 252 training loss: 0.07008426636457443\n",
      "Epoch 253 training loss: 0.07000238448381424\n",
      "Epoch 254 training loss: 0.07011322677135468\n",
      "Epoch 255 training loss: 0.07004731893539429\n",
      "Epoch 256 training loss: 0.07000732421875\n",
      "0.751263335204941\n",
      "[[6985 1034]\n",
      " [1624 1043]]\n",
      "Accuracy: 0.751263335204941\n",
      "Precision:  0.502166586422725\n",
      "Recall:  0.3910761154855643\n",
      "F1:  0.4397133220910624\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=False\n",
    "\n",
    "autoenc = Autoencoder([40, 32, 24, 16, 8], [8, 16, 24, 32, 40], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=8, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.1, window_size=8, window_slide=1, one_hot=False)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07821372896432877\n",
      "Epoch 2 training loss: 0.0774301290512085\n",
      "Epoch 3 training loss: 0.07747204601764679\n",
      "Epoch 4 training loss: 0.07747378945350647\n",
      "Epoch 5 training loss: 0.0775231420993805\n",
      "Epoch 6 training loss: 0.07740684598684311\n",
      "Epoch 7 training loss: 0.07742051780223846\n",
      "Epoch 8 training loss: 0.07739140093326569\n",
      "Epoch 9 training loss: 0.07745695859193802\n",
      "Epoch 10 training loss: 0.07746768742799759\n",
      "Epoch 11 training loss: 0.07740789651870728\n",
      "Epoch 12 training loss: 0.077417753636837\n",
      "Epoch 13 training loss: 0.07751183211803436\n",
      "Epoch 14 training loss: 0.07744431495666504\n",
      "Epoch 15 training loss: 0.07744622230529785\n",
      "Epoch 16 training loss: 0.07750263810157776\n",
      "Epoch 17 training loss: 0.07744259387254715\n",
      "Epoch 18 training loss: 0.07745462656021118\n",
      "Epoch 19 training loss: 0.07743870466947556\n",
      "Epoch 20 training loss: 0.07745484262704849\n",
      "Epoch 21 training loss: 0.07742375880479813\n",
      "Epoch 22 training loss: 0.07747796922922134\n",
      "Epoch 23 training loss: 0.07749824970960617\n",
      "Epoch 24 training loss: 0.07748693227767944\n",
      "Epoch 25 training loss: 0.0774930864572525\n",
      "Epoch 26 training loss: 0.07741627097129822\n",
      "Epoch 27 training loss: 0.07749664783477783\n",
      "Epoch 28 training loss: 0.07742944359779358\n",
      "Epoch 29 training loss: 0.077445849776268\n",
      "Epoch 30 training loss: 0.07746104151010513\n",
      "Epoch 31 training loss: 0.07749255746603012\n",
      "Epoch 32 training loss: 0.0774720087647438\n",
      "Epoch 33 training loss: 0.0775042474269867\n",
      "Epoch 34 training loss: 0.07745180279016495\n",
      "Epoch 35 training loss: 0.07747648656368256\n",
      "Epoch 36 training loss: 0.07744362950325012\n",
      "Epoch 37 training loss: 0.07748481631278992\n",
      "Epoch 38 training loss: 0.07741475850343704\n",
      "Epoch 39 training loss: 0.07748692482709885\n",
      "Epoch 40 training loss: 0.07747168838977814\n",
      "Epoch 41 training loss: 0.07745791971683502\n",
      "Epoch 42 training loss: 0.07745706290006638\n",
      "Epoch 43 training loss: 0.07747597992420197\n",
      "Epoch 44 training loss: 0.07746551930904388\n",
      "Epoch 45 training loss: 0.07745984196662903\n",
      "Epoch 46 training loss: 0.07743500173091888\n",
      "Epoch 47 training loss: 0.07747801393270493\n",
      "Epoch 48 training loss: 0.07740675657987595\n",
      "Epoch 49 training loss: 0.07746816426515579\n",
      "Epoch 50 training loss: 0.07742462307214737\n",
      "Epoch 51 training loss: 0.07744874805212021\n",
      "Epoch 52 training loss: 0.07746195048093796\n",
      "Epoch 53 training loss: 0.0774989202618599\n",
      "Epoch 54 training loss: 0.07747622579336166\n",
      "Epoch 55 training loss: 0.077483631670475\n",
      "Epoch 56 training loss: 0.07751216739416122\n",
      "Epoch 57 training loss: 0.07748410850763321\n",
      "Epoch 58 training loss: 0.07746347784996033\n",
      "Epoch 59 training loss: 0.0774628147482872\n",
      "Epoch 60 training loss: 0.0774625614285469\n",
      "Epoch 61 training loss: 0.0775102823972702\n",
      "Epoch 62 training loss: 0.07749015837907791\n",
      "Epoch 63 training loss: 0.07745887339115143\n",
      "Epoch 64 training loss: 0.07746994495391846\n",
      "Epoch 65 training loss: 0.07750430703163147\n",
      "Epoch 66 training loss: 0.0774727314710617\n",
      "Epoch 67 training loss: 0.07748156785964966\n",
      "Epoch 68 training loss: 0.07744712382555008\n",
      "Epoch 69 training loss: 0.0774344727396965\n",
      "Epoch 70 training loss: 0.07744915038347244\n",
      "Epoch 71 training loss: 0.07742217183113098\n",
      "Epoch 72 training loss: 0.07739919424057007\n",
      "Epoch 73 training loss: 0.07746726274490356\n",
      "Epoch 74 training loss: 0.07747264951467514\n",
      "Epoch 75 training loss: 0.07750871777534485\n",
      "Epoch 76 training loss: 0.07747753709554672\n",
      "Epoch 77 training loss: 0.07746852934360504\n",
      "Epoch 78 training loss: 0.0774313360452652\n",
      "Epoch 79 training loss: 0.07749033719301224\n",
      "Epoch 80 training loss: 0.0773961991071701\n",
      "Epoch 81 training loss: 0.0774637758731842\n",
      "Epoch 82 training loss: 0.07737694680690765\n",
      "Epoch 83 training loss: 0.07749482244253159\n",
      "Epoch 84 training loss: 0.07744594663381577\n",
      "Epoch 85 training loss: 0.07748038321733475\n",
      "Epoch 86 training loss: 0.0774850994348526\n",
      "Epoch 87 training loss: 0.07746998965740204\n",
      "Epoch 88 training loss: 0.07747902721166611\n",
      "Epoch 89 training loss: 0.07742265611886978\n",
      "Epoch 90 training loss: 0.0774666890501976\n",
      "Epoch 91 training loss: 0.07746469229459763\n",
      "Epoch 92 training loss: 0.07741707563400269\n",
      "Epoch 93 training loss: 0.07745788246393204\n",
      "Epoch 94 training loss: 0.0774354487657547\n",
      "Epoch 95 training loss: 0.07748907804489136\n",
      "Epoch 96 training loss: 0.07747332751750946\n",
      "Epoch 97 training loss: 0.07735586911439896\n",
      "Epoch 98 training loss: 0.07749322801828384\n",
      "Epoch 99 training loss: 0.07746251672506332\n",
      "Epoch 100 training loss: 0.07745691388845444\n",
      "Epoch 101 training loss: 0.07745112478733063\n",
      "Epoch 102 training loss: 0.07746928185224533\n",
      "Epoch 103 training loss: 0.07747086137533188\n",
      "Epoch 104 training loss: 0.07748518139123917\n",
      "Epoch 105 training loss: 0.07754199951887131\n",
      "Epoch 106 training loss: 0.07755640149116516\n",
      "Epoch 107 training loss: 0.0774640366435051\n",
      "Epoch 108 training loss: 0.07746428996324539\n",
      "Epoch 109 training loss: 0.07750626653432846\n",
      "Epoch 110 training loss: 0.07746750861406326\n",
      "Epoch 111 training loss: 0.0774291381239891\n",
      "Epoch 112 training loss: 0.07747825980186462\n",
      "Epoch 113 training loss: 0.07743310928344727\n",
      "Epoch 114 training loss: 0.07740695774555206\n",
      "Epoch 115 training loss: 0.07746350020170212\n",
      "Epoch 116 training loss: 0.07749108970165253\n",
      "Epoch 117 training loss: 0.07750780135393143\n",
      "Epoch 118 training loss: 0.07743425667285919\n",
      "Epoch 119 training loss: 0.07744568586349487\n",
      "Epoch 120 training loss: 0.07744721323251724\n",
      "Epoch 121 training loss: 0.07746068388223648\n",
      "Epoch 122 training loss: 0.07747896015644073\n",
      "Epoch 123 training loss: 0.07740408927202225\n",
      "Epoch 124 training loss: 0.07749899476766586\n",
      "Epoch 125 training loss: 0.077507883310318\n",
      "Epoch 126 training loss: 0.07739807665348053\n",
      "Epoch 127 training loss: 0.07744429260492325\n",
      "Epoch 128 training loss: 0.07748968154191971\n",
      "Epoch 129 training loss: 0.07748547196388245\n",
      "Epoch 130 training loss: 0.07749123871326447\n",
      "Epoch 131 training loss: 0.07743554562330246\n",
      "Epoch 132 training loss: 0.07741477340459824\n",
      "Epoch 133 training loss: 0.07744339108467102\n",
      "Epoch 134 training loss: 0.07749395817518234\n",
      "Epoch 135 training loss: 0.07748646289110184\n",
      "Epoch 136 training loss: 0.0775308683514595\n",
      "Epoch 137 training loss: 0.07746300101280212\n",
      "Epoch 138 training loss: 0.07744716107845306\n",
      "Epoch 139 training loss: 0.07745543867349625\n",
      "Epoch 140 training loss: 0.07747106999158859\n",
      "Epoch 141 training loss: 0.07745051383972168\n",
      "Epoch 142 training loss: 0.07746978849172592\n",
      "Epoch 143 training loss: 0.077456034719944\n",
      "Epoch 144 training loss: 0.07744794338941574\n",
      "Epoch 145 training loss: 0.0774807408452034\n",
      "Epoch 146 training loss: 0.07742209732532501\n",
      "Epoch 147 training loss: 0.07745733112096786\n",
      "Epoch 148 training loss: 0.07743379473686218\n",
      "Epoch 149 training loss: 0.07741392403841019\n",
      "Epoch 150 training loss: 0.0774640142917633\n",
      "Epoch 151 training loss: 0.07747244834899902\n",
      "Epoch 152 training loss: 0.07745373249053955\n",
      "Epoch 153 training loss: 0.07748451083898544\n",
      "Epoch 154 training loss: 0.0774102658033371\n",
      "Epoch 155 training loss: 0.07750198990106583\n",
      "Epoch 156 training loss: 0.07746578752994537\n",
      "Epoch 157 training loss: 0.07745657861232758\n",
      "Epoch 158 training loss: 0.07747791707515717\n",
      "Epoch 159 training loss: 0.07744302600622177\n",
      "Epoch 160 training loss: 0.07743103802204132\n",
      "Epoch 161 training loss: 0.07743555307388306\n",
      "Epoch 162 training loss: 0.07745924592018127\n",
      "Epoch 163 training loss: 0.07748781144618988\n",
      "Epoch 164 training loss: 0.07747867703437805\n",
      "Epoch 165 training loss: 0.07753469049930573\n",
      "Epoch 166 training loss: 0.07745355367660522\n",
      "Epoch 167 training loss: 0.07746531814336777\n",
      "Epoch 168 training loss: 0.07744434475898743\n",
      "Epoch 169 training loss: 0.07748933136463165\n",
      "Epoch 170 training loss: 0.07748561352491379\n",
      "Epoch 171 training loss: 0.07745755463838577\n",
      "Epoch 172 training loss: 0.07743830233812332\n",
      "Epoch 173 training loss: 0.0774468258023262\n",
      "Epoch 174 training loss: 0.07749338448047638\n",
      "Epoch 175 training loss: 0.0773722380399704\n",
      "Epoch 176 training loss: 0.07745666801929474\n",
      "Epoch 177 training loss: 0.07743725925683975\n",
      "Epoch 178 training loss: 0.07743974030017853\n",
      "Epoch 179 training loss: 0.0774901807308197\n",
      "Epoch 180 training loss: 0.07750094681978226\n",
      "Epoch 181 training loss: 0.07740659266710281\n",
      "Epoch 182 training loss: 0.07747036218643188\n",
      "Epoch 183 training loss: 0.07746116816997528\n",
      "Epoch 184 training loss: 0.07747870683670044\n",
      "Epoch 185 training loss: 0.07743918895721436\n",
      "Epoch 186 training loss: 0.07744904607534409\n",
      "Epoch 187 training loss: 0.0774957686662674\n",
      "Epoch 188 training loss: 0.07743287831544876\n",
      "Epoch 189 training loss: 0.07750769704580307\n",
      "Epoch 190 training loss: 0.07741899788379669\n",
      "Epoch 191 training loss: 0.07744442671537399\n",
      "Epoch 192 training loss: 0.07748144119977951\n",
      "Epoch 193 training loss: 0.07739192247390747\n",
      "Epoch 194 training loss: 0.07749181985855103\n",
      "Epoch 195 training loss: 0.07746051996946335\n",
      "Epoch 196 training loss: 0.07749651372432709\n",
      "Epoch 197 training loss: 0.07749483734369278\n",
      "Epoch 198 training loss: 0.07747551798820496\n",
      "Epoch 199 training loss: 0.07748185843229294\n",
      "Epoch 200 training loss: 0.0774434432387352\n",
      "Epoch 201 training loss: 0.07755545526742935\n",
      "Epoch 202 training loss: 0.07741467654705048\n",
      "Epoch 203 training loss: 0.0774427056312561\n",
      "Epoch 204 training loss: 0.07745103538036346\n",
      "Epoch 205 training loss: 0.07738429307937622\n",
      "Epoch 206 training loss: 0.07745025306940079\n",
      "Epoch 207 training loss: 0.07749398052692413\n",
      "Epoch 208 training loss: 0.07742059230804443\n",
      "Epoch 209 training loss: 0.0774666965007782\n",
      "Epoch 210 training loss: 0.07747874408960342\n",
      "Epoch 211 training loss: 0.07745484262704849\n",
      "Epoch 212 training loss: 0.0774792730808258\n",
      "Epoch 213 training loss: 0.07748302072286606\n",
      "Epoch 214 training loss: 0.07746487855911255\n",
      "Epoch 215 training loss: 0.07742641866207123\n",
      "Epoch 216 training loss: 0.07749319076538086\n",
      "Epoch 217 training loss: 0.07744765281677246\n",
      "Epoch 218 training loss: 0.07742835581302643\n",
      "Epoch 219 training loss: 0.07747877389192581\n",
      "Epoch 220 training loss: 0.07748446613550186\n",
      "Epoch 221 training loss: 0.07743504643440247\n",
      "Epoch 222 training loss: 0.07745642960071564\n",
      "Epoch 223 training loss: 0.07742395251989365\n",
      "Epoch 224 training loss: 0.07746625691652298\n",
      "Epoch 225 training loss: 0.0774972140789032\n",
      "Epoch 226 training loss: 0.07750188559293747\n",
      "Epoch 227 training loss: 0.07748524844646454\n",
      "Epoch 228 training loss: 0.07740265876054764\n",
      "Epoch 229 training loss: 0.07743129134178162\n",
      "Epoch 230 training loss: 0.07745473086833954\n",
      "Epoch 231 training loss: 0.07744167745113373\n",
      "Epoch 232 training loss: 0.07739246636629105\n",
      "Epoch 233 training loss: 0.07748404890298843\n",
      "Epoch 234 training loss: 0.07746339589357376\n",
      "Epoch 235 training loss: 0.07747901976108551\n",
      "Epoch 236 training loss: 0.07743935286998749\n",
      "Epoch 237 training loss: 0.07743876427412033\n",
      "Epoch 238 training loss: 0.07751055061817169\n",
      "Epoch 239 training loss: 0.07753288000822067\n",
      "Epoch 240 training loss: 0.07750222831964493\n",
      "Epoch 241 training loss: 0.07745762169361115\n",
      "Epoch 242 training loss: 0.07749946415424347\n",
      "Epoch 243 training loss: 0.0774020105600357\n",
      "Epoch 244 training loss: 0.07749827951192856\n",
      "Epoch 245 training loss: 0.07745644450187683\n",
      "Epoch 246 training loss: 0.07756714522838593\n",
      "Epoch 247 training loss: 0.07754246890544891\n",
      "Epoch 248 training loss: 0.07744432985782623\n",
      "Epoch 249 training loss: 0.07746729999780655\n",
      "Epoch 250 training loss: 0.07744508981704712\n",
      "Epoch 251 training loss: 0.0774877741932869\n",
      "Epoch 252 training loss: 0.07747697085142136\n",
      "Epoch 253 training loss: 0.07743818312883377\n",
      "Epoch 254 training loss: 0.0775001272559166\n",
      "Epoch 255 training loss: 0.07746410369873047\n",
      "Epoch 256 training loss: 0.07744985073804855\n",
      "0.7330151600224593\n",
      "[[6183 1836]\n",
      " [1017 1650]]\n",
      "Accuracy: 0.7330151600224593\n",
      "Precision:  0.47332185886402756\n",
      "Recall:  0.6186726659167604\n",
      "F1:  0.5363237445148707\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8], [8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07812798023223877\n",
      "Epoch 2 training loss: 0.07743661850690842\n",
      "Epoch 3 training loss: 0.07740376144647598\n",
      "Epoch 4 training loss: 0.07743975520133972\n",
      "Epoch 5 training loss: 0.07742688804864883\n",
      "Epoch 6 training loss: 0.07745155692100525\n",
      "Epoch 7 training loss: 0.0774465948343277\n",
      "Epoch 8 training loss: 0.07741744071245193\n",
      "Epoch 9 training loss: 0.07745532691478729\n",
      "Epoch 10 training loss: 0.07742851227521896\n",
      "Epoch 11 training loss: 0.07746095210313797\n",
      "Epoch 12 training loss: 0.07742488384246826\n",
      "Epoch 13 training loss: 0.07747579365968704\n",
      "Epoch 14 training loss: 0.07744032144546509\n",
      "Epoch 15 training loss: 0.07742897421121597\n",
      "Epoch 16 training loss: 0.07751155644655228\n",
      "Epoch 17 training loss: 0.07739351689815521\n",
      "Epoch 18 training loss: 0.07740240544080734\n",
      "Epoch 19 training loss: 0.07748274505138397\n",
      "Epoch 20 training loss: 0.07744436711072922\n",
      "Epoch 21 training loss: 0.0774078294634819\n",
      "Epoch 22 training loss: 0.07739578932523727\n",
      "Epoch 23 training loss: 0.07742733508348465\n",
      "Epoch 24 training loss: 0.07743795961141586\n",
      "Epoch 25 training loss: 0.07744920253753662\n",
      "Epoch 26 training loss: 0.0773923397064209\n",
      "Epoch 27 training loss: 0.07746272534132004\n",
      "Epoch 28 training loss: 0.07750817388296127\n",
      "Epoch 29 training loss: 0.07742610573768616\n",
      "Epoch 30 training loss: 0.0774325430393219\n",
      "Epoch 31 training loss: 0.07742555439472198\n",
      "Epoch 32 training loss: 0.07745441049337387\n",
      "Epoch 33 training loss: 0.07742538303136826\n",
      "Epoch 34 training loss: 0.07744311541318893\n",
      "Epoch 35 training loss: 0.07742466777563095\n",
      "Epoch 36 training loss: 0.07745049148797989\n",
      "Epoch 37 training loss: 0.07742957770824432\n",
      "Epoch 38 training loss: 0.07740917801856995\n",
      "Epoch 39 training loss: 0.07737935334444046\n",
      "Epoch 40 training loss: 0.07746833562850952\n",
      "Epoch 41 training loss: 0.07742499560117722\n",
      "Epoch 42 training loss: 0.07744681090116501\n",
      "Epoch 43 training loss: 0.07743407040834427\n",
      "Epoch 44 training loss: 0.07742394506931305\n",
      "Epoch 45 training loss: 0.07745876163244247\n",
      "Epoch 46 training loss: 0.07740865647792816\n",
      "Epoch 47 training loss: 0.07742428779602051\n",
      "Epoch 48 training loss: 0.07749349623918533\n",
      "Epoch 49 training loss: 0.07746322453022003\n",
      "Epoch 50 training loss: 0.07737366855144501\n",
      "Epoch 51 training loss: 0.07741657644510269\n",
      "Epoch 52 training loss: 0.07747690379619598\n",
      "Epoch 53 training loss: 0.07742226868867874\n",
      "Epoch 54 training loss: 0.07741229236125946\n",
      "Epoch 55 training loss: 0.07743892073631287\n",
      "Epoch 56 training loss: 0.07744564861059189\n",
      "Epoch 57 training loss: 0.07745062559843063\n",
      "Epoch 58 training loss: 0.07746175676584244\n",
      "Epoch 59 training loss: 0.0773690789937973\n",
      "Epoch 60 training loss: 0.07740002870559692\n",
      "Epoch 61 training loss: 0.07745984196662903\n",
      "Epoch 62 training loss: 0.07751363515853882\n",
      "Epoch 63 training loss: 0.07748191058635712\n",
      "Epoch 64 training loss: 0.07743840664625168\n",
      "Epoch 65 training loss: 0.07747188955545425\n",
      "Epoch 66 training loss: 0.07738712430000305\n",
      "Epoch 67 training loss: 0.07740531116724014\n",
      "Epoch 68 training loss: 0.07740769535303116\n",
      "Epoch 69 training loss: 0.07745185494422913\n",
      "Epoch 70 training loss: 0.07747133076190948\n",
      "Epoch 71 training loss: 0.07744130492210388\n",
      "Epoch 72 training loss: 0.07746735960245132\n",
      "Epoch 73 training loss: 0.07742126286029816\n",
      "Epoch 74 training loss: 0.0774296224117279\n",
      "Epoch 75 training loss: 0.07747523486614227\n",
      "Epoch 76 training loss: 0.07737012952566147\n",
      "Epoch 77 training loss: 0.07746380567550659\n",
      "Epoch 78 training loss: 0.07742446660995483\n",
      "Epoch 79 training loss: 0.0774160772562027\n",
      "Epoch 80 training loss: 0.07743265479803085\n",
      "Epoch 81 training loss: 0.07742413133382797\n",
      "Epoch 82 training loss: 0.07740895450115204\n",
      "Epoch 83 training loss: 0.07746709883213043\n",
      "Epoch 84 training loss: 0.07745310664176941\n",
      "Epoch 85 training loss: 0.07742098718881607\n",
      "Epoch 86 training loss: 0.07739657908678055\n",
      "Epoch 87 training loss: 0.0774468183517456\n",
      "Epoch 88 training loss: 0.07743831723928452\n",
      "Epoch 89 training loss: 0.07748374342918396\n",
      "Epoch 90 training loss: 0.07743804901838303\n",
      "Epoch 91 training loss: 0.0774640440940857\n",
      "Epoch 92 training loss: 0.07744201272726059\n",
      "Epoch 93 training loss: 0.07743050903081894\n",
      "Epoch 94 training loss: 0.07742185145616531\n",
      "Epoch 95 training loss: 0.07744797319173813\n",
      "Epoch 96 training loss: 0.07747352868318558\n",
      "Epoch 97 training loss: 0.07741480320692062\n",
      "Epoch 98 training loss: 0.07743187993764877\n",
      "Epoch 99 training loss: 0.0774860605597496\n",
      "Epoch 100 training loss: 0.0774078294634819\n",
      "Epoch 101 training loss: 0.07745438814163208\n",
      "Epoch 102 training loss: 0.07744579762220383\n",
      "Epoch 103 training loss: 0.07746630162000656\n",
      "Epoch 104 training loss: 0.07745155692100525\n",
      "Epoch 105 training loss: 0.0774223580956459\n",
      "Epoch 106 training loss: 0.07742784917354584\n",
      "Epoch 107 training loss: 0.07745251804590225\n",
      "Epoch 108 training loss: 0.07742860913276672\n",
      "Epoch 109 training loss: 0.07747869938611984\n",
      "Epoch 110 training loss: 0.0774160772562027\n",
      "Epoch 111 training loss: 0.0774698406457901\n",
      "Epoch 112 training loss: 0.07744206488132477\n",
      "Epoch 113 training loss: 0.07739043235778809\n",
      "Epoch 114 training loss: 0.0774000734090805\n",
      "Epoch 115 training loss: 0.07745474576950073\n",
      "Epoch 116 training loss: 0.0774509385228157\n",
      "Epoch 117 training loss: 0.07743088155984879\n",
      "Epoch 118 training loss: 0.07746271044015884\n",
      "Epoch 119 training loss: 0.07748843729496002\n",
      "Epoch 120 training loss: 0.07750355452299118\n",
      "Epoch 121 training loss: 0.07747828215360641\n",
      "Epoch 122 training loss: 0.07738235592842102\n",
      "Epoch 123 training loss: 0.07739520072937012\n",
      "Epoch 124 training loss: 0.07751726359128952\n",
      "Epoch 125 training loss: 0.07738599926233292\n",
      "Epoch 126 training loss: 0.07745524495840073\n",
      "Epoch 127 training loss: 0.07741500437259674\n",
      "Epoch 128 training loss: 0.07742521911859512\n",
      "Epoch 129 training loss: 0.0774087905883789\n",
      "Epoch 130 training loss: 0.07746502012014389\n",
      "Epoch 131 training loss: 0.07741180062294006\n",
      "Epoch 132 training loss: 0.07736284285783768\n",
      "Epoch 133 training loss: 0.07748546451330185\n",
      "Epoch 134 training loss: 0.0774245485663414\n",
      "Epoch 135 training loss: 0.07741770893335342\n",
      "Epoch 136 training loss: 0.07736793160438538\n",
      "Epoch 137 training loss: 0.07739600539207458\n",
      "Epoch 138 training loss: 0.07742960005998611\n",
      "Epoch 139 training loss: 0.07744214683771133\n",
      "Epoch 140 training loss: 0.07741190493106842\n",
      "Epoch 141 training loss: 0.07742875069379807\n",
      "Epoch 142 training loss: 0.07742192596197128\n",
      "Epoch 143 training loss: 0.07738184928894043\n",
      "Epoch 144 training loss: 0.07744675874710083\n",
      "Epoch 145 training loss: 0.07743664085865021\n",
      "Epoch 146 training loss: 0.07738455384969711\n",
      "Epoch 147 training loss: 0.07745568454265594\n",
      "Epoch 148 training loss: 0.07746700942516327\n",
      "Epoch 149 training loss: 0.07742408663034439\n",
      "Epoch 150 training loss: 0.07741950452327728\n",
      "Epoch 151 training loss: 0.07746175676584244\n",
      "Epoch 152 training loss: 0.0774260088801384\n",
      "Epoch 153 training loss: 0.0774802416563034\n",
      "Epoch 154 training loss: 0.07741723209619522\n",
      "Epoch 155 training loss: 0.07743414491415024\n",
      "Epoch 156 training loss: 0.077408067882061\n",
      "Epoch 157 training loss: 0.07744409888982773\n",
      "Epoch 158 training loss: 0.07742739468812943\n",
      "Epoch 159 training loss: 0.07744953781366348\n",
      "Epoch 160 training loss: 0.07747656852006912\n",
      "Epoch 161 training loss: 0.0774165466427803\n",
      "Epoch 162 training loss: 0.07741856575012207\n",
      "Epoch 163 training loss: 0.07741240411996841\n",
      "Epoch 164 training loss: 0.07741659134626389\n",
      "Epoch 165 training loss: 0.0774424821138382\n",
      "Epoch 166 training loss: 0.07746914029121399\n",
      "Epoch 167 training loss: 0.07744292914867401\n",
      "Epoch 168 training loss: 0.07748151570558548\n",
      "Epoch 169 training loss: 0.07744846493005753\n",
      "Epoch 170 training loss: 0.07738830894231796\n",
      "Epoch 171 training loss: 0.0774289220571518\n",
      "Epoch 172 training loss: 0.07743427157402039\n",
      "Epoch 173 training loss: 0.07743165642023087\n",
      "Epoch 174 training loss: 0.07747069746255875\n",
      "Epoch 175 training loss: 0.07738874107599258\n",
      "Epoch 176 training loss: 0.07741918414831161\n",
      "Epoch 177 training loss: 0.07743562012910843\n",
      "Epoch 178 training loss: 0.07735966891050339\n",
      "Epoch 179 training loss: 0.07743476331233978\n",
      "Epoch 180 training loss: 0.07739116251468658\n",
      "Epoch 181 training loss: 0.07742628455162048\n",
      "Epoch 182 training loss: 0.07741645723581314\n",
      "Epoch 183 training loss: 0.07747811824083328\n",
      "Epoch 184 training loss: 0.07749971002340317\n",
      "Epoch 185 training loss: 0.07743722945451736\n",
      "Epoch 186 training loss: 0.07746146619319916\n",
      "Epoch 187 training loss: 0.07741804420948029\n",
      "Epoch 188 training loss: 0.07744847238063812\n",
      "Epoch 189 training loss: 0.07746303826570511\n",
      "Epoch 190 training loss: 0.07743881642818451\n",
      "Epoch 191 training loss: 0.07744438946247101\n",
      "Epoch 192 training loss: 0.07748193293809891\n",
      "Epoch 193 training loss: 0.07738836854696274\n",
      "Epoch 194 training loss: 0.07742340117692947\n",
      "Epoch 195 training loss: 0.0774988904595375\n",
      "Epoch 196 training loss: 0.07740726321935654\n",
      "Epoch 197 training loss: 0.07745065540075302\n",
      "Epoch 198 training loss: 0.07747900485992432\n",
      "Epoch 199 training loss: 0.07747728377580643\n",
      "Epoch 200 training loss: 0.07743824273347855\n",
      "Epoch 201 training loss: 0.07746019959449768\n",
      "Epoch 202 training loss: 0.07745882123708725\n",
      "Epoch 203 training loss: 0.07747272402048111\n",
      "Epoch 204 training loss: 0.07742422074079514\n",
      "Epoch 205 training loss: 0.07743415981531143\n",
      "Epoch 206 training loss: 0.07745102047920227\n",
      "Epoch 207 training loss: 0.0774073451757431\n",
      "Epoch 208 training loss: 0.07741142809391022\n",
      "Epoch 209 training loss: 0.07740122824907303\n",
      "Epoch 210 training loss: 0.07748263329267502\n",
      "Epoch 211 training loss: 0.07749155163764954\n",
      "Epoch 212 training loss: 0.07741154730319977\n",
      "Epoch 213 training loss: 0.07739461958408356\n",
      "Epoch 214 training loss: 0.07747567445039749\n",
      "Epoch 215 training loss: 0.07741361856460571\n",
      "Epoch 216 training loss: 0.07742414623498917\n",
      "Epoch 217 training loss: 0.07741661369800568\n",
      "Epoch 218 training loss: 0.0774569883942604\n",
      "Epoch 219 training loss: 0.07744848728179932\n",
      "Epoch 220 training loss: 0.07744541019201279\n",
      "Epoch 221 training loss: 0.07745078206062317\n",
      "Epoch 222 training loss: 0.07744016498327255\n",
      "Epoch 223 training loss: 0.0774693489074707\n",
      "Epoch 224 training loss: 0.07743754237890244\n",
      "Epoch 225 training loss: 0.07743097841739655\n",
      "Epoch 226 training loss: 0.0774221420288086\n",
      "Epoch 227 training loss: 0.07746686041355133\n",
      "Epoch 228 training loss: 0.07739643007516861\n",
      "Epoch 229 training loss: 0.0774589329957962\n",
      "Epoch 230 training loss: 0.07740103453397751\n",
      "Epoch 231 training loss: 0.07740858942270279\n",
      "Epoch 232 training loss: 0.07741384953260422\n",
      "Epoch 233 training loss: 0.07745321840047836\n",
      "Epoch 234 training loss: 0.07743988186120987\n",
      "Epoch 235 training loss: 0.07742397487163544\n",
      "Epoch 236 training loss: 0.07745464146137238\n",
      "Epoch 237 training loss: 0.07745802402496338\n",
      "Epoch 238 training loss: 0.07734030485153198\n",
      "Epoch 239 training loss: 0.07746613770723343\n",
      "Epoch 240 training loss: 0.0773654356598854\n",
      "Epoch 241 training loss: 0.07748863846063614\n",
      "Epoch 242 training loss: 0.07746215164661407\n",
      "Epoch 243 training loss: 0.07744883000850677\n",
      "Epoch 244 training loss: 0.07743442058563232\n",
      "Epoch 245 training loss: 0.07742837816476822\n",
      "Epoch 246 training loss: 0.07740101963281631\n",
      "Epoch 247 training loss: 0.07742860168218613\n",
      "Epoch 248 training loss: 0.07742811739444733\n",
      "Epoch 249 training loss: 0.07739914953708649\n",
      "Epoch 250 training loss: 0.07744497060775757\n",
      "Epoch 251 training loss: 0.07748188078403473\n",
      "Epoch 252 training loss: 0.07741960138082504\n",
      "Epoch 253 training loss: 0.07741804420948029\n",
      "Epoch 254 training loss: 0.07745210081338882\n",
      "Epoch 255 training loss: 0.07737227529287338\n",
      "Epoch 256 training loss: 0.07741500437259674\n",
      "0.749766049036122\n",
      "[[6371 1648]\n",
      " [1026 1641]]\n",
      "Accuracy: 0.749766049036122\n",
      "Precision:  0.4989358467619337\n",
      "Recall:  0.6152980877390326\n",
      "F1:  0.551040967092008\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 50% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=10, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07820510119199753\n",
      "Epoch 2 training loss: 0.07742731273174286\n",
      "Epoch 3 training loss: 0.07747434079647064\n",
      "Epoch 4 training loss: 0.07741634547710419\n",
      "Epoch 5 training loss: 0.0774001032114029\n",
      "Epoch 6 training loss: 0.07741684466600418\n",
      "Epoch 7 training loss: 0.07742767035961151\n",
      "Epoch 8 training loss: 0.07743506133556366\n",
      "Epoch 9 training loss: 0.07750232517719269\n",
      "Epoch 10 training loss: 0.0774184986948967\n",
      "Epoch 11 training loss: 0.07745100557804108\n",
      "Epoch 12 training loss: 0.0774000734090805\n",
      "Epoch 13 training loss: 0.07746321707963943\n",
      "Epoch 14 training loss: 0.07743411511182785\n",
      "Epoch 15 training loss: 0.0774422287940979\n",
      "Epoch 16 training loss: 0.07743213325738907\n",
      "Epoch 17 training loss: 0.07738926261663437\n",
      "Epoch 18 training loss: 0.07744530588388443\n",
      "Epoch 19 training loss: 0.07743768393993378\n",
      "Epoch 20 training loss: 0.07742731273174286\n",
      "Epoch 21 training loss: 0.0773899033665657\n",
      "Epoch 22 training loss: 0.07744169235229492\n",
      "Epoch 23 training loss: 0.07743233442306519\n",
      "Epoch 24 training loss: 0.07745523750782013\n",
      "Epoch 25 training loss: 0.07745399326086044\n",
      "Epoch 26 training loss: 0.07738808542490005\n",
      "Epoch 27 training loss: 0.0773855522274971\n",
      "Epoch 28 training loss: 0.07735876739025116\n",
      "Epoch 29 training loss: 0.077479787170887\n",
      "Epoch 30 training loss: 0.07748821377754211\n",
      "Epoch 31 training loss: 0.07742900401353836\n",
      "Epoch 32 training loss: 0.07741423696279526\n",
      "Epoch 33 training loss: 0.07743994891643524\n",
      "Epoch 34 training loss: 0.07737038284540176\n",
      "Epoch 35 training loss: 0.07741237431764603\n",
      "Epoch 36 training loss: 0.07751559466123581\n",
      "Epoch 37 training loss: 0.07740432024002075\n",
      "Epoch 38 training loss: 0.07741332054138184\n",
      "Epoch 39 training loss: 0.07745438069105148\n",
      "Epoch 40 training loss: 0.07742320001125336\n",
      "Epoch 41 training loss: 0.07748078554868698\n",
      "Epoch 42 training loss: 0.07743661850690842\n",
      "Epoch 43 training loss: 0.07749215513467789\n",
      "Epoch 44 training loss: 0.0774574875831604\n",
      "Epoch 45 training loss: 0.07744728028774261\n",
      "Epoch 46 training loss: 0.07741936296224594\n",
      "Epoch 47 training loss: 0.07745704799890518\n",
      "Epoch 48 training loss: 0.07742255181074142\n",
      "Epoch 49 training loss: 0.077397920191288\n",
      "Epoch 50 training loss: 0.07735235244035721\n",
      "Epoch 51 training loss: 0.07738921791315079\n",
      "Epoch 52 training loss: 0.07738762348890305\n",
      "Epoch 53 training loss: 0.07744771242141724\n",
      "Epoch 54 training loss: 0.07736868411302567\n",
      "Epoch 55 training loss: 0.07743068784475327\n",
      "Epoch 56 training loss: 0.07742238789796829\n",
      "Epoch 57 training loss: 0.07743077725172043\n",
      "Epoch 58 training loss: 0.07743866741657257\n",
      "Epoch 59 training loss: 0.07746447622776031\n",
      "Epoch 60 training loss: 0.0774567723274231\n",
      "Epoch 61 training loss: 0.07738073915243149\n",
      "Epoch 62 training loss: 0.07738539576530457\n",
      "Epoch 63 training loss: 0.07743057608604431\n",
      "Epoch 64 training loss: 0.0774732157588005\n",
      "Epoch 65 training loss: 0.07742919772863388\n",
      "Epoch 66 training loss: 0.07744184136390686\n",
      "Epoch 67 training loss: 0.07742877304553986\n",
      "Epoch 68 training loss: 0.07750348746776581\n",
      "Epoch 69 training loss: 0.07738280296325684\n",
      "Epoch 70 training loss: 0.07744532823562622\n",
      "Epoch 71 training loss: 0.07740303128957748\n",
      "Epoch 72 training loss: 0.07741285115480423\n",
      "Epoch 73 training loss: 0.07748894393444061\n",
      "Epoch 74 training loss: 0.0774330347776413\n",
      "Epoch 75 training loss: 0.07743067294359207\n",
      "Epoch 76 training loss: 0.07745052129030228\n",
      "Epoch 77 training loss: 0.07744957506656647\n",
      "Epoch 78 training loss: 0.07744444906711578\n",
      "Epoch 79 training loss: 0.0774051770567894\n",
      "Epoch 80 training loss: 0.07744510471820831\n",
      "Epoch 81 training loss: 0.07740612328052521\n",
      "Epoch 82 training loss: 0.07742924243211746\n",
      "Epoch 83 training loss: 0.07757336646318436\n",
      "Epoch 84 training loss: 0.07736792415380478\n",
      "Epoch 85 training loss: 0.07742153853178024\n",
      "Epoch 86 training loss: 0.07742853462696075\n",
      "Epoch 87 training loss: 0.07741580903530121\n",
      "Epoch 88 training loss: 0.07741668075323105\n",
      "Epoch 89 training loss: 0.07750406116247177\n",
      "Epoch 90 training loss: 0.07747916877269745\n",
      "Epoch 91 training loss: 0.07740850746631622\n",
      "Epoch 92 training loss: 0.07739674299955368\n",
      "Epoch 93 training loss: 0.07744048535823822\n",
      "Epoch 94 training loss: 0.07740170508623123\n",
      "Epoch 95 training loss: 0.07742632925510406\n",
      "Epoch 96 training loss: 0.07744962722063065\n",
      "Epoch 97 training loss: 0.07742469757795334\n",
      "Epoch 98 training loss: 0.07745000720024109\n",
      "Epoch 99 training loss: 0.07739900797605515\n",
      "Epoch 100 training loss: 0.07741492241621017\n",
      "Epoch 101 training loss: 0.0774027556180954\n",
      "Epoch 102 training loss: 0.07741694897413254\n",
      "Epoch 103 training loss: 0.07737328112125397\n",
      "Epoch 104 training loss: 0.07747209817171097\n",
      "Epoch 105 training loss: 0.07746019214391708\n",
      "Epoch 106 training loss: 0.07741567492485046\n",
      "Epoch 107 training loss: 0.0774003341794014\n",
      "Epoch 108 training loss: 0.07742796093225479\n",
      "Epoch 109 training loss: 0.07739478349685669\n",
      "Epoch 110 training loss: 0.0774015560746193\n",
      "Epoch 111 training loss: 0.07742643356323242\n",
      "Epoch 112 training loss: 0.07742012292146683\n",
      "Epoch 113 training loss: 0.07739701867103577\n",
      "Epoch 114 training loss: 0.07742179185152054\n",
      "Epoch 115 training loss: 0.07743221521377563\n",
      "Epoch 116 training loss: 0.07748748362064362\n",
      "Epoch 117 training loss: 0.07745298743247986\n",
      "Epoch 118 training loss: 0.07744850218296051\n",
      "Epoch 119 training loss: 0.07749152183532715\n",
      "Epoch 120 training loss: 0.07742016017436981\n",
      "Epoch 121 training loss: 0.07739897072315216\n",
      "Epoch 122 training loss: 0.07745049148797989\n",
      "Epoch 123 training loss: 0.07745064795017242\n",
      "Epoch 124 training loss: 0.07739677280187607\n",
      "Epoch 125 training loss: 0.077447310090065\n",
      "Epoch 126 training loss: 0.07743377238512039\n",
      "Epoch 127 training loss: 0.07736174017190933\n",
      "Epoch 128 training loss: 0.07745746523141861\n",
      "Epoch 129 training loss: 0.07744526118040085\n",
      "Epoch 130 training loss: 0.07740183919668198\n",
      "Epoch 131 training loss: 0.07741434872150421\n",
      "Epoch 132 training loss: 0.0774325281381607\n",
      "Epoch 133 training loss: 0.07740558683872223\n",
      "Epoch 134 training loss: 0.07746411859989166\n",
      "Epoch 135 training loss: 0.07739145308732986\n",
      "Epoch 136 training loss: 0.07738038152456284\n",
      "Epoch 137 training loss: 0.07738006860017776\n",
      "Epoch 138 training loss: 0.07742536813020706\n",
      "Epoch 139 training loss: 0.07739362865686417\n",
      "Epoch 140 training loss: 0.07741432636976242\n",
      "Epoch 141 training loss: 0.07743701338768005\n",
      "Epoch 142 training loss: 0.07740746438503265\n",
      "Epoch 143 training loss: 0.07744122296571732\n",
      "Epoch 144 training loss: 0.07740173488855362\n",
      "Epoch 145 training loss: 0.07742122560739517\n",
      "Epoch 146 training loss: 0.07743996381759644\n",
      "Epoch 147 training loss: 0.07749509811401367\n",
      "Epoch 148 training loss: 0.07745082676410675\n",
      "Epoch 149 training loss: 0.07737855613231659\n",
      "Epoch 150 training loss: 0.07746215164661407\n",
      "Epoch 151 training loss: 0.0774436891078949\n",
      "Epoch 152 training loss: 0.07745885848999023\n",
      "Epoch 153 training loss: 0.07743802666664124\n",
      "Epoch 154 training loss: 0.07745373249053955\n",
      "Epoch 155 training loss: 0.07747890055179596\n",
      "Epoch 156 training loss: 0.07744718343019485\n",
      "Epoch 157 training loss: 0.07741285115480423\n",
      "Epoch 158 training loss: 0.0774376392364502\n",
      "Epoch 159 training loss: 0.07737496495246887\n",
      "Epoch 160 training loss: 0.07740356028079987\n",
      "Epoch 161 training loss: 0.07748392224311829\n",
      "Epoch 162 training loss: 0.0774281769990921\n",
      "Epoch 163 training loss: 0.07742192596197128\n",
      "Epoch 164 training loss: 0.07741227000951767\n",
      "Epoch 165 training loss: 0.07740402966737747\n",
      "Epoch 166 training loss: 0.07747163623571396\n",
      "Epoch 167 training loss: 0.07737860083580017\n",
      "Epoch 168 training loss: 0.07744190096855164\n",
      "Epoch 169 training loss: 0.07742905616760254\n",
      "Epoch 170 training loss: 0.0774131491780281\n",
      "Epoch 171 training loss: 0.07739119231700897\n",
      "Epoch 172 training loss: 0.07745599001646042\n",
      "Epoch 173 training loss: 0.07742186635732651\n",
      "Epoch 174 training loss: 0.07746647298336029\n",
      "Epoch 175 training loss: 0.07737014442682266\n",
      "Epoch 176 training loss: 0.07744915783405304\n",
      "Epoch 177 training loss: 0.07743239402770996\n",
      "Epoch 178 training loss: 0.07748205959796906\n",
      "Epoch 179 training loss: 0.07746542990207672\n",
      "Epoch 180 training loss: 0.0774821788072586\n",
      "Epoch 181 training loss: 0.07745592296123505\n",
      "Epoch 182 training loss: 0.07748066633939743\n",
      "Epoch 183 training loss: 0.07745208591222763\n",
      "Epoch 184 training loss: 0.07744574546813965\n",
      "Epoch 185 training loss: 0.07736950367689133\n",
      "Epoch 186 training loss: 0.0773940160870552\n",
      "Epoch 187 training loss: 0.07738503068685532\n",
      "Epoch 188 training loss: 0.0774136632680893\n",
      "Epoch 189 training loss: 0.0774226039648056\n",
      "Epoch 190 training loss: 0.07741347700357437\n",
      "Epoch 191 training loss: 0.07740890979766846\n",
      "Epoch 192 training loss: 0.0774514451622963\n",
      "Epoch 193 training loss: 0.07738316059112549\n",
      "Epoch 194 training loss: 0.07743290811777115\n",
      "Epoch 195 training loss: 0.07745641469955444\n",
      "Epoch 196 training loss: 0.07746319472789764\n",
      "Epoch 197 training loss: 0.07745571434497833\n",
      "Epoch 198 training loss: 0.07743602246046066\n",
      "Epoch 199 training loss: 0.07743863761425018\n",
      "Epoch 200 training loss: 0.07747361063957214\n",
      "Epoch 201 training loss: 0.07745525240898132\n",
      "Epoch 202 training loss: 0.0774555504322052\n",
      "Epoch 203 training loss: 0.07744631916284561\n",
      "Epoch 204 training loss: 0.07739078998565674\n",
      "Epoch 205 training loss: 0.077388696372509\n",
      "Epoch 206 training loss: 0.07741390913724899\n",
      "Epoch 207 training loss: 0.07736082375049591\n",
      "Epoch 208 training loss: 0.07742109149694443\n",
      "Epoch 209 training loss: 0.07743155211210251\n",
      "Epoch 210 training loss: 0.0774386078119278\n",
      "Epoch 211 training loss: 0.07742296159267426\n",
      "Epoch 212 training loss: 0.07745782285928726\n",
      "Epoch 213 training loss: 0.07743490487337112\n",
      "Epoch 214 training loss: 0.07743872702121735\n",
      "Epoch 215 training loss: 0.07742881774902344\n",
      "Epoch 216 training loss: 0.07736515253782272\n",
      "Epoch 217 training loss: 0.0773802325129509\n",
      "Epoch 218 training loss: 0.07746198773384094\n",
      "Epoch 219 training loss: 0.07748524844646454\n",
      "Epoch 220 training loss: 0.07749640196561813\n",
      "Epoch 221 training loss: 0.07744390517473221\n",
      "Epoch 222 training loss: 0.07744646072387695\n",
      "Epoch 223 training loss: 0.07745266705751419\n",
      "Epoch 224 training loss: 0.0774136632680893\n",
      "Epoch 225 training loss: 0.07741881161928177\n",
      "Epoch 226 training loss: 0.07740063965320587\n",
      "Epoch 227 training loss: 0.07743268460035324\n",
      "Epoch 228 training loss: 0.07746909558773041\n",
      "Epoch 229 training loss: 0.07741882652044296\n",
      "Epoch 230 training loss: 0.07745423913002014\n",
      "Epoch 231 training loss: 0.07748814672231674\n",
      "Epoch 232 training loss: 0.0774727612733841\n",
      "Epoch 233 training loss: 0.07740207761526108\n",
      "Epoch 234 training loss: 0.07741750776767731\n",
      "Epoch 235 training loss: 0.07740811258554459\n",
      "Epoch 236 training loss: 0.07747641950845718\n",
      "Epoch 237 training loss: 0.07739314436912537\n",
      "Epoch 238 training loss: 0.07741294801235199\n",
      "Epoch 239 training loss: 0.07740698009729385\n",
      "Epoch 240 training loss: 0.07741361111402512\n",
      "Epoch 241 training loss: 0.07743487507104874\n",
      "Epoch 242 training loss: 0.07740864902734756\n",
      "Epoch 243 training loss: 0.07737050205469131\n",
      "Epoch 244 training loss: 0.07739847153425217\n",
      "Epoch 245 training loss: 0.07743341475725174\n",
      "Epoch 246 training loss: 0.07743142545223236\n",
      "Epoch 247 training loss: 0.07743512094020844\n",
      "Epoch 248 training loss: 0.07745401561260223\n",
      "Epoch 249 training loss: 0.07744383811950684\n",
      "Epoch 250 training loss: 0.07737212628126144\n",
      "Epoch 251 training loss: 0.07740776985883713\n",
      "Epoch 252 training loss: 0.07743430137634277\n",
      "Epoch 253 training loss: 0.07736478000879288\n",
      "Epoch 254 training loss: 0.07745715975761414\n",
      "Epoch 255 training loss: 0.07744615525007248\n",
      "Epoch 256 training loss: 0.07740207016468048\n",
      "0.6766060606060607\n",
      "[[1142  316]\n",
      " [1018 1649]]\n",
      "Accuracy: 0.6766060606060607\n",
      "Precision:  0.8391857506361323\n",
      "Recall:  0.6182977127859017\n",
      "F1:  0.7120034542314335\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=256, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.05,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_df: 8890 anomalous_df: 729\n",
      "12\n",
      "Epoch 1 training loss: 0.07864207029342651\n",
      "Epoch 2 training loss: 0.07804663479328156\n",
      "0.6744242424242424\n",
      "[[1124  334]\n",
      " [1009 1658]]\n",
      "Accuracy: 0.6744242424242424\n",
      "Precision:  0.8323293172690763\n",
      "Recall:  0.6216722909636295\n",
      "F1:  0.7117407168920369\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder with 10% anomalies and one_hot=True\n",
    "\n",
    "autoenc = Autoencoder([96, 64, 32, 24, 16, 8, 4], [4, 8, 16, 24, 32, 64, 96], nn.ReLU(), nn.Sigmoid())\n",
    "\n",
    "hp = Hparams(batch_size=4, n_epochs=2, anomaly_generation_ratio=0.01, clean_test_data_ratio=0.3, learning_rate=0.1,\n",
    "             window_size=8, window_slide=1, one_hot=True)\n",
    "(avg, cm) = model_train_eval(autoenc, data, hp)\n",
    "print(avg)\n",
    "print(cm)\n",
    "(tp, tn, fp, fn) = (cm[1][1], cm[0][0], cm[0][1], cm[1][0])\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))\n",
    "print(\"Precision: \", tp / (tp + fp))\n",
    "print(\"Recall: \", tp / (tp + fn))\n",
    "print(\"F1: \", 2 * tp / (2 * tp + fp + fn))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
